---
title: "Natural Language Processing - Chapter 7"
author: "Tim Roessling"
date: "`r Sys.Date()`"
format: html
jupyter: myvenv
---
# Topic Modeling

## Topic Modeling

As the volume of information grows, it becomes increasingly difficult to locate relevant content. Topic modeling is a technique used to uncover the hidden thematic structure in a collection of documents.

**When might you need to find hidden topics in text?**
- Organizing large collections of documents
- Summarizing unstructured text data
- Improving information retrieval and search
- Feature selection for downstream machine learning tasks

### What is Topic Modeling?

- Topic modeling refers to a set of unsupervised machine learning techniques (such as clustering) for discovering the abstract "topics" that occur in a collection of documents.
- It provides methods for automatically organizing, understanding, searching, and summarizing large electronic archives.
- Common applications include document clustering, organizing large blocks of textual data, information retrieval from unstructured text, and feature selection.

---

## Latent Dirichlet Allocation (LDA)

Latent Dirichlet Allocation (LDA) is a foundational topic modeling technique. Let's break down the name:

- **Latent:** Hidden or underlying (the topics are not directly observed).
- **Dirichlet:** Refers to the Dirichlet distribution, which models the distribution of topics in documents and words in topics.
- **Allocation:** Assigning topics to words in documents.

---

### LDA Intuition

LDA is a **generative model**: it assumes that documents are created by mixing topics, and each topic is a collection of words.

- Each **topic** is a distribution over words.
- Each **document** is a mixture of topics.
- Each **word** in a document is drawn from one of the document's topics.

**In short:**  
Documents are treated as bags of words. Each document is generated by a mixture of topics, and each topic is a mixture of words. The goal of LDA is to uncover these hidden topics from the observed words.

---

### How LDA Works

1. For each document, choose a distribution over topics.
2. For each word in the document:
    - Pick a topic from the document's topic distribution.
    - Pick a word from the topic's word distribution.

LDA tries to reverse-engineer this process: given the words, it infers the topics.

---

### Key Components

- **Word distribution per topic** (`β_k`): Which words are likely in each topic.
- **Topic distribution per document** (`θ_d`): Which topics are likely in each document.
- **Topic assignment for each word** (`z_{d,n}`): Which topic generated each word.

![LDA as a graphical model](images/chapter7_1.png){#fig-tree width=80%}

---

### The Trade-off: Document vs. Topic Sparsity

LDA balances two goals:

- **Document sparsity:** Each document should use as few topics as possible.
- **Topic sparsity:** Each topic should use as few words as possible.

These are in tension:  
- If all words in a document come from one topic, that topic must cover many words (less topic sparsity).
- If each topic uses only a few words, documents must use more topics (less document sparsity).

**LDA finds a balance, resulting in topics that best explain the documents.**

---

### LDA Parameters

- **Document density factor (`α`):** Controls how many topics are expected per document.
- **Topic word density factor (`β`):** Controls how many words are expected per topic.
- **Number of topics (`K`):** How many topics to extract.

---

### Parameter Estimation in LDA

LDA uses **Bayes' Theorem** to estimate the hidden variables (topics):

\[
P(\text{hypothesis} \mid \text{observations}) = \frac{P(\text{observations} \mid \text{hypothesis}) \cdot P(\text{hypothesis})}{P(\text{observations})}
\]

- **Prior:** What we believe before seeing the data.
- **Posterior:** What we believe after seeing the data.

**Key challenge:**  
Computing the exact posterior is intractable.

**Common solutions:**
- **Direct methods:** Expectation Maximization, Variational Inference, Expectation Propagation.
- **Indirect methods:** Estimate the posterior using sampling (e.g., Gibbs sampling, a type of Markov Chain Monte Carlo).

```python
# Python Example: LDA with scikit-learn and gensim
import numpy as np
import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation
from sklearn.datasets import fetch_20newsgroups
import gensim
from gensim import corpora
import pyLDAvis.gensim_models
import pyLDAvis

# Example 1: LDA with scikit-learn
# Load sample data
newsgroups = fetch_20newsgroups(subset='train', 
                               categories=['alt.atheism', 'sci.space', 'rec.sport.hockey'],
                               remove=('headers', 'footers', 'quotes'))

# Preprocess and vectorize
vectorizer = CountVectorizer(max_features=1000, stop_words='english', 
                           min_df=2, max_df=0.95)
doc_term_matrix = vectorizer.fit_transform(newsgroups.data)

# Fit LDA model
lda = LatentDirichletAllocation(n_components=3, random_state=42, 
                              max_iter=10, learning_method='online')
lda.fit(doc_term_matrix)

# Display top words per topic
feature_names = vectorizer.get_feature_names_out()
for topic_idx, topic in enumerate(lda.components_):
    top_words = [feature_names[i] for i in topic.argsort()[-10:]]
    print(f"Topic {topic_idx}: {', '.join(top_words)}")

# Example 2: LDA with gensim (more advanced)
# Prepare documents
texts = [doc.split() for doc in newsgroups.data[:100]]  # Simple tokenization
dictionary = corpora.Dictionary(texts)
corpus = [dictionary.doc2bow(text) for text in texts]

# Train LDA model
lda_model = gensim.models.LdaModel(corpus=corpus, id2word=dictionary, 
                                 num_topics=3, random_state=42,
                                 alpha='auto', per_word_topics=True)

# Print topics
for idx, topic in lda_model.print_topics(-1):
    print(f'Topic {idx}: {topic}')

# Get document-topic probabilities
doc_topics = lda_model[corpus[0]]
print(f"Document 0 topic distribution: {doc_topics[0]}")
# Visualize (requires pyLDAvis)
vis = pyLDAvis.gensim_models.prepare(lda_model, corpus, dictionary)
pyLDAvis.display(vis)

```


### Gibbs Sampling in LDA

Gibbs sampling is a Markov Chain Monte Carlo (MCMC) method used to estimate the posterior distribution of the hidden variables in LDA, specifically the topic assignments for each word.

#### What does Gibbs sampling do in LDA?

- It generates samples from the joint probability distribution of the topic assignments for all words in all documents.
- For each word, it samples a new topic assignment based on the current state of all other assignments.
- This iterative process gradually approximates the true posterior distribution.


#### The Gibbs Sampling Algorithm

1. **Initialization:**  
    Randomly assign each word in each document to one of the K topics.

2. **Iterative Sampling:**  
    For each word \( w_{d,n} \) in document \( d \):
    - Remove the current topic assignment for \( w_{d,n} \).
    - Compute the probability of assigning topic \( k \) to \( w_{d,n} \) as:
      \[
      P(z_{d,n} = k \mid z_{-}, w) \propto 
      \frac{n_{d,k}^{-n} + \alpha}{\sum_{k'} n_{d,k'}^{-n} + K\alpha} \cdot
      \frac{n_{k,w}^{-n} + \beta}{\sum_{w'} n_{k,w'}^{-n} + V\beta}
      \]
      where:
      - \( n_{d,k}^{-n} \): Number of words in document \( d \) assigned to topic \( k \), excluding the current word.
      - \( n_{k,w}^{-n} \): Number of times word \( w \) is assigned to topic \( k \), excluding the current word.
      - \( \alpha \), \( \beta \): Dirichlet priors.
      - \( K \): Number of topics.
      - \( V \): Vocabulary size.
    - Sample a new topic for \( w_{d,n} \) from this distribution.
    - Update the counts accordingly.

3. **Repeat:**  
    Iterate over all words multiple times (epochs) until the topic assignments stabilize.

#### Intuition

- **Document-topic counts** encourage each document to use as few topics as possible (document sparsity).
- **Topic-word counts** encourage each topic to use as few words as possible (topic sparsity).
- Over many iterations, the assignments converge to a good approximation of the true topic structure.

#### Summary

Gibbs sampling provides an efficient way to estimate the hidden topic structure in LDA by iteratively updating topic assignments for each word based on the current state of the model.

---

### Practical Considerations for LDA

- **Similar to k-means clustering:** LDA is unsupervised and requires the number of topics to be specified in advance.
- **Bag-of-words assumption:** Ignores word order and semantic context.
- **Preprocessing is crucial:** Good results require cleaning (stop words, stemming, lemmatization, etc.).
- **Not ideal for short texts:** LDA struggles with very short documents (e.g., tweets, headlines).

---

### Visualizing and Using LDA Results

- **Document-topic probabilities:** Each document can be represented by its topic probability vector.
- **Word-topic assignments:** Each word in a document can be colored or labeled by its assigned topic.
- **Document similarity:** Documents are similar if they have similar topic distributions (e.g., using Euclidean distance).

---
## Top2Vec and BERTopic

### Top2Vec: Distributed Topic Representations

Top2Vec is a modern topic modeling algorithm (arXiv 2020, [GitHub: ddangelov/Top2Vec](https://github.com/ddangelov/Top2Vec)) that automatically discovers topics in text data **without requiring you to specify the number of topics in advance**. It also works **without extensive preprocessing** (e.g., stop word removal, stemming, or lemmatization).

#### Key Ideas

- **Vector Space Representation:**  
    - Each document and word is embedded into a high-dimensional vector space.
    - Terms are axes; documents and words are points or vectors.
- **Dimensionality Reduction:**  
    - UMAP is used to reduce the dimensionality of the embeddings, preserving semantic relationships (using cosine similarity).
- **Clustering:**  
    - HDBSCAN clusters the reduced vectors to identify topic groups.
- **Topic Vectors:**  
    - For each cluster, a topic vector is calculated.
    - The closest words to each topic vector define the topic.

#### Top2Vec Workflow

1. **Embed documents and words** (e.g., using doc2vec or word2vec).
2. **Reduce dimensionality** (UMAP).
3. **Cluster embeddings** (HDBSCAN).
4. **Calculate topic vectors** for each cluster.
5. **Find closest words** to each topic vector to define topics.

![Top2Vec](images/chapter7_2.png){#fig-tree width=80%}

#### Hyperparameters

- `min_count`: Minimum word frequency to include in the model.
- `embedding_model`: Embedding method (e.g., 'doc2vec', 'word2vec').
- `umap_args`: UMAP settings for dimensionality reduction.
- `hdbscan_args`: HDBSCAN clustering settings.
- `vector_size`: Size of embedding vectors.

#### Pros and Cons

**Pros:**
- Automatically determines the number of topics.
- Preserves semantic meaning better than traditional models.
- Minimal preprocessing required.

**Cons:**
- Computationally intensive—use small datasets for initial testing.
- Many hyperparameters to tune; results can vary.
- Requires experimentation to achieve optimal results.

```python
# Python Example: Top2Vec

from top2vec import Top2Vec
from sklearn.datasets import fetch_20newsgroups
import numpy as np

# Load sample data
newsgroups = fetch_20newsgroups(subset='train', 
                               categories=['alt.atheism', 'sci.space', 'rec.sport.hockey'],
                               remove=('headers', 'footers', 'quotes'))

# Take a smaller subset for faster processing
documents = newsgroups.data[:200]  # Use only 200 documents for demo

# Create Top2Vec model
# Note: This may take several minutes to run
model = Top2Vec(documents, 
                embedding_model='universal-sentence-encoder',  # Easier to use, no C compiler needed
                speed='fast',  # Options: 'fast', 'deep', 'learn'
                workers=1)

# Get number of topics found
num_topics = model.get_num_topics()
print(f"Number of topics found: {num_topics}")

# Get topic words for each topic
topic_words, word_scores, topic_nums = model.get_topics()

# Display topics
for i, topic_num in enumerate(topic_nums):
    print(f"\nTopic {topic_num}:")
    print(f"Words: {', '.join(topic_words[i][:10])}")  # Show top 10 words

# Get topic for a specific document
doc_topics, doc_scores, doc_ids = model.search_documents_by_topic(topic_num=0, num_docs=3)
print(f"\nSample documents for Topic 0:")
for i, doc_id in enumerate(doc_ids):
    print(f"Doc {doc_id}: {documents[doc_id][:100]}...")

# Search for similar documents using keywords
try:
    doc_topics, doc_scores, doc_ids = model.search_documents_by_keywords(keywords=["space", "NASA"], num_docs=3)
    print(f"\nDocuments similar to 'space, NASA':")
    for i, doc_id in enumerate(doc_ids):
        print(f"Score: {doc_scores[i]:.3f} - {documents[doc_id][:100]}...")
except:
    print("No documents found for these keywords")
```

---

### BERTopic

**BERTopic** is another modern topic modeling technique inspired by Top2Vec, but leverages transformer-based embeddings (e.g., BERT).

#### Key Features

- **No centroid calculation:**  
    Each document in a cluster is treated as unique.
- **Topic extraction:**  
    Uses class-based TF-IDF to extract topic words from clusters.
- **Transformer-based:**  
    Utilizes contextual embeddings for improved topic quality, especially in nuanced or short texts.
- **Computational cost:**  
    More resource-intensive due to transformer models.

---

### K-means Clustering with Word2Vec

- **Embed documents** using Word2Vec.
- **Cluster** the embeddings using k-means.
- **Best for short texts** (e.g., tweets, headlines).

---

**Summary Table**

| Method      | Needs # Topics? | Preprocessing | Handles Short Texts | Computational Cost | Notes                      |
|-------------|-----------------|--------------|---------------------|-------------------|----------------------------|
| LDA         | Yes             | Yes          | No                  | Moderate          | Classic, interpretable     |
| Top2Vec     | No              | No           | Yes                 | High              | Finds topics automatically |
| BERTopic    | No              | No           | Yes                 | Very High         | Transformer-based          |
| K-means+W2V | Yes             | Yes          | Yes                 | Low/Moderate      | Simple, fast               |

---

**Practical Tips:**
- Start with small datasets to experiment with Top2Vec or BERTopic.
- Tune hyperparameters for your specific data and task.
- Consider computational resources—transformer-based models can be slow.
- Use visualizations to interpret and validate discovered topics.


## The Revolution in NLP

Natural Language Processing (NLP) has undergone a revolution in recent years, primarily due to the introduction of the **transformer model**. Transformers are trained on massive datasets using objectives like *masked language modeling* (predicting missing words in a sentence). Further improvements are achieved through **Reinforcement Learning from Human Feedback (RLHF)**, allowing models to better align with human preferences.

> **Example:**\
> Given the sentence: "The cat sat on the \_\_\_," a transformer model predicts the missing word, such as "mat".

## Language is Hard

Despite impressive progress, language remains a challenging domain for AI.

### Why is Language Hard?

-   **Infinite Possibilities:**\
    Most sentences you hear are unique—you've never heard them before and may never hear them again.
-   **Ambiguity:**
    -   *Lexical Ambiguity:* Words can have multiple meanings.\
        \> *Example:* "bank" (river bank vs. financial bank)
    -   *Structural Ambiguity:* Sentences can be interpreted in different ways.\
        \> *Example:* "I saw the man with the telescope." (Who has the telescope?)

Many thinkers have argued that true human-level language understanding may be impossible for machines. Current LLMs (Large Language Models) appear to process and reason with language at a high level, but is this really human-like understanding?

------------------------------------------------------------------------

## Brief History

-   **Descartes:**\
    Argued that a machine could never truly imitate a human; there would always be a way to tell the difference.
-   **Turing Test:**\
    Proposed by Alan Turing as a test of a machine’s ability to exhibit intelligent behavior indistinguishable from a human.
    -   A human judge engages in a conversation with both a human and a machine.
    -   If the judge cannot reliably tell which is which, the machine is said to have passed the test.

## NLP: The Basic Approach

### Background

-   **Numerical Data:** Numbers, measurements, etc.
-   **Categorical Data:** Discrete categories or labels.
-   **Text Data:** Unstructured, variable-length, and context-dependent (e.g., email content, headlines, political speeches).

Text data is fundamentally different from structured data. Can we treat language as structured data for machine learning?

#### Making Language into Structured Data

-   **Bag-of-Words Representation:**
    -   Assign a feature (column) for each word in the vocabulary.
    -   For a given text, the value is 1 if the word occurs, 0 otherwise.
    -   Alternative values: word counts or TF-IDF scores.
    -   Most features are 0 for any given text (sparse representation).

#### Supervised ML for Text Processing

-   **Labeled Text Data:** Enables building classifiers for:
    -   Spam Detection
    -   Sentiment Analysis
    -   Topic Detection
    -   ...and more
-   Raises questions: Does this approach capture real understanding? How does it relate to the Turing Test?

#### Bag-of-Words Limitation

-   Ignores word order and context ("bag" of words).
-   Cannot distinguish between "dog bites man" and "man bites dog".

------------------------------------------------------------------------

### Language Modeling with N-grams

-   **Goal:** Assign a probability to a sequence of words.
-   **Markov Assumption:** The probability of a word depends only on the previous *n-1* words.

#### Example: "He went to the store"

-   **Unigrams (1-grams):** He, went, to, the, store
-   **Bigrams (2-grams):** He went, went to, to the, the store
-   **Trigrams (3-grams):** He went to, went to the, to the store
-   **4-grams:** He went to the, went to the store
-   **5-gram:** He went to the store

#### N-gram Approximations

-   **Bigram Model:**\
    ( p(\text{He went to the store}) = p(\text{He}) \times p(\text{went}\|\text{He}) \times p(\text{to}\|\text{went}) \times p(\text{the}\|\text{to}) \times p(\text{store}\|\text{the}) )

-   **Trigram Model:**\
    ( p(\text{He went to the store}) = p(\text{He}) \times p(\text{went}\|\text{He}) \times p(\text{to}\|\text{He went}) \times p(\text{the}\|\text{went to}) \times p(\text{store}\|\text{to the}) )

-   **Key Idea:**\
    N-gram models capture some local word order, but struggle with long-range dependencies and rare phrases.

------------------------------------------------------------------------

### Training an LLM

> “A general language model (LM) should be able to compute the probability of (and also generate) any string.”\
> *(Radford et al., 2019)*

### What Does Training an LLM Look Like?

Consider how humans complete sentences: - *As Descartes said, "I think, therefore I **." -*** **For all intents and**  - \*I learned how to drive a \_\_\_\*

Or in dialogue: \> **Monica:** Okay, everybody relax. This is not even a \_\_\_\
\> **Rachel:** Oh God... well, it started about a half hour before the \_\_\_\
\> **Ross:** (squatting and reading the instructions) I’m supposed to attach a brackety \_\_\_

The core training objective for LLMs is **next word prediction**:\
Given a sequence of words, predict the most likely next word.

#### Neural Network for Next Word Prediction

-   The model is a neural network that outputs a score for every word in the vocabulary.
-   These scores are converted into probabilities using the **softmax** function.
-   For example, with a vocabulary of 50,000 words, the output might look like: `[fish: 0.00002, help: 0.00002, ..., the: 0.00002, ..., aardvarks: 0.00002]`

#### Training Process

1.  **Compute Loss:**
    -   The true next word is masked (hidden).
    -   The model predicts probabilities for all words.
    -   The loss function measures how well the model predicts the correct word (e.g., *Loss = 1 - prob(correct word)*).
    -   If the model assigns a probability of 1 to the correct word, loss is 0 (best). If 0, loss is 1 (worst).
2.  **Update Weights:**
    -   The model adjusts its internal weights to increase the probability of the correct word.
    -   This also slightly decreases the probability for all other words.
    -   Each training example provides a small update—repeated millions or billions of times.

#### Example: Weight Updates

Suppose the correct next word is "fish": - The model increases the weights leading to "fish". - The probabilities for other words (e.g., "help", "the", "aardvarks") are slightly reduced.

> Each example nudges the model to make the correct word more likely in context, and less likely for others.\
> Over time, the model learns to predict words in a wide variety of contexts.

------------------------------------------------------------------------

*This is the fundamental process behind training large language models: predict the next word, compute the loss, update the weights, and repeat—at massive scale.*

### Probing GPT

Large Language Models (LLMs) today generate highly coherent, grammatical text that can be indistinguishable from human output. They demonstrate some understanding of hierarchical structure and abstract linguistic categories (Mahowald et al., 2024).

While these models are not perfect learners of abstract linguistic rules, neither are humans. LLMs are progressing toward acquiring formal linguistic competence and have already challenged claims about the impossibility of learning certain linguistic knowledge—such as hierarchical structure and abstract categories—from input alone (Mahowald et al., 2024).

------------------------------------------------------------------------

## AI: Where Are We Heading?

**Artificial General Intelligence (AGI):**\
AGI is defined as AI that matches or surpasses human cognitive capabilities across a wide range of tasks.

**AGI Benchmarks:**

-   **The Robot College Student Test (Goertzel):**\
    A machine enrolls in a university, takes and passes the same classes as humans, and obtains a degree. LLMs can now pass university-level exams without attending classes.

-   **The Employment Test (Nilsson):**\
    A machine performs an economically important job at least as well as humans. AI is already replacing humans in roles ranging from fast food to marketing.

-   **The Ikea Test (Marcus):**\
    An AI views the parts and instructions of an Ikea flat-pack product, then controls a robot to assemble the furniture correctly.

-   **The Coffee Test (Wozniak):**\
    A machine enters an average home and figures out how to make coffee: find the machine, coffee, water, mug, and brew coffee by pushing the right buttons. This remains unsolved.

-   **The Modern Turing Test (Suleyman):**\
    An AI is given \$100,000 and must turn it into \$1 million.

------------------------------------------------------------------------

## Applications of LLMs

-   AI interfaces for customer support and onboarding
-   Research portals with Retrieval-Augmented Generation (RAG)
-   Automated customer support (e.g., Zendesk)
-   Accessibility tools (e.g., BeMyAI)

------------------------------------------------------------------------

## Takeaways

-   **Language is Hard:**
    -   Language is infinite and ambiguous (both lexically and structurally).
-   **The Revolution in NLP:**
    -   LLMs now approach human-level language ability.
-   **Exciting Research Directions:**
    -   Building applications with LLMs
    -   Probing their abilities
-   **Powerful AI is Coming:**
    -   The field is rapidly advancing, with significant societal impact on the horizon.