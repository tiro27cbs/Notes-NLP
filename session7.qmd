---
title: "Topic Modeling"
author: "Tim Roessling"
date: "2025-06-12"
format: html
jupyter: myvenv
---


## Topic Modeling

As the volume of information grows, it becomes increasingly difficult to locate relevant content. Topic modeling is a technique used to uncover the hidden thematic structure in a collection of documents.

**When might you need to find hidden topics in text?**

- Organizing large collections of documents
- Summarizing unstructured text data
- Improving information retrieval and search
- Feature selection for downstream machine learning tasks

### What is Topic Modeling?

- Topic modeling refers to a set of unsupervised machine learning techniques (such as clustering) for discovering the abstract "topics" that occur in a collection of documents.
- It provides methods for automatically organizing, understanding, searching, and summarizing large electronic archives.
- Common applications include document clustering, organizing large blocks of textual data, information retrieval from unstructured text, and feature selection.

---

## Latent Dirichlet Allocation (LDA)

Latent Dirichlet Allocation (LDA) is a foundational topic modeling technique. Let's break down the name:

- **Latent:** Hidden or underlying (the topics are not directly observed).
- **Dirichlet:** Refers to the Dirichlet distribution, which models the distribution of topics in documents and words in topics.
- **Allocation:** Assigning topics to words in documents.

---

### LDA Intuition

LDA is a **generative model**: it assumes that documents are created by mixing topics, and each topic is a collection of words.

- Each **topic** is a distribution over words.
- Each **document** is a mixture of topics.
- Each **word** in a document is drawn from one of the document's topics.

**In short:**  
Documents are treated as bags of words. Each document is generated by a mixture of topics, and each topic is a mixture of words. The goal of LDA is to uncover these hidden topics from the observed words.

---

### How LDA Works

1. For each document, choose a distribution over topics.
2. For each word in the document:
    - Pick a topic from the document's topic distribution.
    - Pick a word from the topic's word distribution.

LDA tries to reverse-engineer this process: given the words, it infers the topics.

---

### Key Components

- **Word distribution per topic** (`β_k`): Which words are likely in each topic.
- **Topic distribution per document** (`θ_d`): Which topics are likely in each document.
- **Topic assignment for each word** (`z_{d,n}`): Which topic generated each word.

![LDA as a graphical model](images\chapter7_1.png){#fig-tree width=80%}

---

### The Trade-off: Document vs. Topic Sparsity

LDA balances two goals:

- **Document sparsity:** Each document should use as few topics as possible.
- **Topic sparsity:** Each topic should use as few words as possible.

These are in tension:  
- If all words in a document come from one topic, that topic must cover many words (less topic sparsity).
- If each topic uses only a few words, documents must use more topics (less document sparsity).

**LDA finds a balance, resulting in topics that best explain the documents.**

---

### LDA Parameters

- **Document density factor (`α`):** Controls how many topics are expected per document.
- **Topic word density factor (`β`):** Controls how many words are expected per topic.
- **Number of topics (`K`):** How many topics to extract.

---

### Parameter Estimation in LDA

LDA uses **Bayes' Theorem** to estimate the hidden variables (topics):

$$
P(\text{hypothesis} \mid \text{observations}) = \frac{P(\text{observations} \mid \text{hypothesis}) \cdot P(\text{hypothesis})}{P(\text{observations})}
$$

- **Prior:** What we believe before seeing the data.
- **Posterior:** What we believe after seeing the data.

**Key challenge:**  
Computing the exact posterior is intractable.

**Common solutions:**
- **Direct methods:** Expectation Maximization, Variational Inference, Expectation Propagation.
- **Indirect methods:** Estimate the posterior using sampling (e.g., Gibbs sampling, a type of Markov Chain Monte Carlo).

```python
# Python Example: LDA with scikit-learn and gensim
import numpy as np
import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation
from sklearn.datasets import fetch_20newsgroups
import gensim
from gensim import corpora
import pyLDAvis.gensim_models
import pyLDAvis

# Example 1: LDA with scikit-learn
# Load sample data
newsgroups = fetch_20newsgroups(subset='train', 
                               categories=['alt.atheism', 'sci.space', 'rec.sport.hockey'],
                               remove=('headers', 'footers', 'quotes'))

# Preprocess and vectorize
vectorizer = CountVectorizer(max_features=1000, stop_words='english', 
                           min_df=2, max_df=0.95)
doc_term_matrix = vectorizer.fit_transform(newsgroups.data)

# Fit LDA model
lda = LatentDirichletAllocation(n_components=3, random_state=42, 
                              max_iter=10, learning_method='online')
lda.fit(doc_term_matrix)

# Display top words per topic
feature_names = vectorizer.get_feature_names_out()
for topic_idx, topic in enumerate(lda.components_):
    top_words = [feature_names[i] for i in topic.argsort()[-10:]]
    print(f"Topic {topic_idx}: {', '.join(top_words)}")

# Example 2: LDA with gensim (more advanced)
# Prepare documents
texts = [doc.split() for doc in newsgroups.data[:100]]  # Simple tokenization
dictionary = corpora.Dictionary(texts)
corpus = [dictionary.doc2bow(text) for text in texts]

# Train LDA model
lda_model = gensim.models.LdaModel(corpus=corpus, id2word=dictionary, 
                                 num_topics=3, random_state=42,
                                 alpha='auto', per_word_topics=True)

# Print topics
for idx, topic in lda_model.print_topics(-1):
    print(f'Topic {idx}: {topic}')

# Get document-topic probabilities
doc_topics = lda_model[corpus[0]]
print(f"Document 0 topic distribution: {doc_topics[0]}")
# Visualize (requires pyLDAvis)
vis = pyLDAvis.gensim_models.prepare(lda_model, corpus, dictionary)
pyLDAvis.display(vis)
```

---

### Gibbs Sampling in LDA

Gibbs sampling is a Markov Chain Monte Carlo (MCMC) method used to estimate the posterior distribution of the hidden variables in LDA, specifically the topic assignments for each word.

#### What does Gibbs sampling do in LDA?

- It generates samples from the joint probability distribution of the topic assignments for all words in all documents.
- For each word, it samples a new topic assignment based on the current state of all other assignments.
- This iterative process gradually approximates the true posterior distribution.

#### The Gibbs Sampling Algorithm

1. **Initialization:**  
   Randomly assign each word in each document to one of the K topics.

2. **Iterative Sampling:**  
    For each word \( w_{d,n} \) in document \( d \):

    - Remove the current topic assignment for \( w_{d,n} \).
    - Compute the probability of assigning topic \( k \) to \( w_{d,n} \) as:

      $$
      P(z_{d,n} = k \mid z_{-}, w) \propto 
      \frac{n_{d,k}^{-n} + \alpha}{\sum_{k'} n_{d,k'}^{-n} + K\alpha} \cdot
      \frac{n_{k,w}^{-n} + \beta}{\sum_{w'} n_{k,w'}^{-n} + V\beta}
      $$

      where:  
      - \( n_{d,k}^{-n} \): Number of words in document \( d \) assigned to topic \( k \), excluding the current word.  
      - \( n_{k,w}^{-n} \): Number of times word \( w \) is assigned to topic \( k \), excluding the current word.  
      - \( \alpha \), \( \beta \): Dirichlet priors.  
      - \( K \): Number of topics.  
      - \( V \): Vocabulary size.

    - Sample a new topic for \( w_{d,n} \) from this distribution.
    - Update the counts accordingly.

3. **Repeat:**  
   Iterate over all words multiple times (epochs) until the topic assignments stabilize.

#### Intuition

- **Document-topic counts** encourage each document to use as few topics as possible (document sparsity).
- **Topic-word counts** encourage each topic to use as few words as possible (topic sparsity).
- Over many iterations, the assignments converge to a good approximation of the true topic structure.

#### Summary

Gibbs sampling provides an efficient way to estimate the hidden topic structure in LDA by iteratively updating topic assignments for each word based on the current state of the model.

---

### Practical Considerations for LDA

- **Similar to k-means clustering:** LDA is unsupervised and requires the number of topics to be specified in advance.
- **Bag-of-words assumption:** Ignores word order and semantic context.
- **Preprocessing is crucial:** Good results require cleaning (stop words, stemming, lemmatization, etc.).
- **Not ideal for short texts:** LDA struggles with very short documents (e.g., tweets, headlines).

---

### Visualizing and Using LDA Results

- **Document-topic probabilities:** Each document can be represented by its topic probability vector.
- **Word-topic assignments:** Each word in a document can be colored or labeled by its assigned topic.
- **Document similarity:** Documents are similar if they have similar topic distributions (e.g., using Euclidean distance).

---

## Top2Vec and BERTopic

### Top2Vec: Distributed Topic Representations

Top2Vec is a modern topic modeling algorithm (arXiv 2020, [GitHub: ddangelov/Top2Vec](https://github.com/ddangelov/Top2Vec)) that automatically discovers topics in text data **without requiring you to specify the number of topics in advance**. It also works **without extensive preprocessing** (e.g., stop word removal, stemming, or lemmatization).

#### Key Ideas

- **Vector Space Representation:**  
  - Each document and word is embedded into a high-dimensional vector space.
  - Terms are axes; documents and words are points or vectors.
- **Dimensionality Reduction:**  
  - UMAP is used to reduce the dimensionality of the embeddings, preserving semantic relationships (using cosine similarity).
- **Clustering:**  
  - HDBSCAN clusters the reduced vectors to identify topic groups.
- **Topic Vectors:**  
  - For each cluster, a topic vector is calculated.
  - The closest words to each topic vector define the topic.

#### Top2Vec Workflow

1. **Embed documents and words** (e.g., using doc2vec or word2vec).
2. **Reduce dimensionality** (UMAP).
3. **Cluster embeddings** (HDBSCAN).
4. **Calculate topic vectors** for each cluster.
5. **Find closest words** to each topic vector to define topics.

![Top2Vec](images\chapter7_2.png){#fig-tree width=80%}

#### Hyperparameters

- `min_count`: Minimum word frequency to include in the model.
- `embedding_model`: Embedding method (e.g., 'doc2vec', 'word2vec').
- `umap_args`: UMAP settings for dimensionality reduction.
- `hdbscan_args`: HDBSCAN clustering settings.
- `vector_size`: Size of embedding vectors.

#### Pros and Cons

**Pros:**
- Automatically determines the number of topics.
- Preserves semantic meaning better than traditional models.
- Minimal preprocessing required.

**Cons:**
- Computationally intensive—use small datasets for initial testing.
- Many hyperparameters to tune; results can vary.
- Requires experimentation to achieve optimal results.

```python
# Python Example: Top2Vec

from top2vec import Top2Vec
from sklearn.datasets import fetch_20newsgroups
import numpy as np

# Load sample data
newsgroups = fetch_20newsgroups(subset='train', 
                               categories=['alt.atheism', 'sci.space', 'rec.sport.hockey'],
                               remove=('headers', 'footers', 'quotes'))

# Take a smaller subset for faster processing
documents = newsgroups.data[:200]  # Use only 200 documents for demo

# Create Top2Vec model
# Note: This may take several minutes to run
model = Top2Vec(documents, 
                embedding_model='universal-sentence-encoder',  # Easier to use, no C compiler needed
                speed='fast',  # Options: 'fast', 'deep', 'learn'
                workers=1)

# Get number of topics found
num_topics = model.get_num_topics()
print(f"Number of topics found: {num_topics}")

# Get topic words for each topic
topic_words, word_scores, topic_nums = model.get_topics()

# Display topics
for i, topic_num in enumerate(topic_nums):
    print(f"\nTopic {topic_num}:")
    print(f"Words: {', '.join(topic_words[i][:10])}")  # Show top 10 words

# Get topic for a specific document
doc_topics, doc_scores, doc_ids = model.search_documents_by_topic(topic_num=0, num_docs=3)
print(f"\nSample documents for Topic 0:")
for i, doc_id in enumerate(doc_ids):
    print(f"Doc {doc_id}: {documents[doc_id][:100]}...")

# Search for similar documents using keywords
try:
    doc_topics, doc_scores, doc_ids = model.search_documents_by_keywords(keywords=["space", "NASA"], num_docs=3)
    print(f"\nDocuments similar to 'space, NASA':")
    for i, doc_id in enumerate(doc_ids):
        print(f"Score: {doc_scores[i]:.3f} - {documents[doc_id][:100]}...")
except:
    print("No documents found for these keywords")
```

---

### BERTopic

**BERTopic** is another modern topic modeling technique inspired by Top2Vec, but leverages transformer-based embeddings (e.g., BERT).

#### Key Features

- **No centroid calculation:**  
  Each document in a cluster is treated as unique.
- **Topic extraction:**  
  Uses class-based TF-IDF to extract topic words from clusters.
- **Transformer-based:**  
  Utilizes contextual embeddings for improved topic quality, especially in nuanced or short texts.
- **Computational cost:**  
  More resource-intensive due to transformer models.

---

### K-means Clustering with Word2Vec

- **Embed documents** using Word2Vec.
- **Cluster** the embeddings using k-means.
- **Best for short texts** (e.g., tweets, headlines).

---

**Summary Table**

| Method      | Needs # Topics? | Preprocessing | Handles Short Texts | Computational Cost | Notes                      |
|-------------|-----------------|--------------|---------------------|-------------------|----------------------------|
| LDA         | Yes             | Yes          | No                  | Moderate          | Classic, interpretable     |
| Top2Vec     | No              | No           | Yes                 | High              | Finds topics automatically |
| BERTopic    | No              | No           | Yes                 | Very High         | Transformer-based          |
| K-means+W2V | Yes             | Yes          | Yes                 | Low/Moderate      | Simple, fast               |

---

**Practical Tips:**
- Start with small datasets to experiment with Top2Vec or BERTopic.
- Tune hyperparameters for your specific data and task.
- Consider computational resources—transformer-based models can be slow.
- Use visualizations to interpret and validate discovered topics.

---

