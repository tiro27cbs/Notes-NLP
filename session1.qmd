---
title: "Chapter 1: Introduction to Natural Language Processing"
author: "Tim Roessling"
date: "`r Sys.Date()`"
format: html
jupyter: myvenv
---

# Chapter 1: Introduction to Natural Language Processing

## The Revolution in NLP

Natural Language Processing (NLP) has undergone a revolution in recent years, primarily due to the introduction of the **transformer model**. Transformers are trained on massive datasets using objectives like *masked language modeling* (predicting missing words in a sentence). Further improvements are achieved through **Reinforcement Learning from Human Feedback (RLHF)**, allowing models to better align with human preferences.

> **Example:**\
> Given the sentence: "The cat sat on the \_\_\_," a transformer model predicts the missing word, such as "mat".

## Language is Hard

Despite impressive progress, language remains a challenging domain for AI.

### Why is Language Hard?

-   **Infinite Possibilities:**\
    Most sentences you hear are unique—you've never heard them before and may never hear them again.
-   **Ambiguity:**
    -   *Lexical Ambiguity:* Words can have multiple meanings.\
        \> *Example:* "bank" (river bank vs. financial bank)
    -   *Structural Ambiguity:* Sentences can be interpreted in different ways.\
        \> *Example:* "I saw the man with the telescope." (Who has the telescope?)

Many thinkers have argued that true human-level language understanding may be impossible for machines. Current LLMs (Large Language Models) appear to process and reason with language at a high level, but is this really human-like understanding?

------------------------------------------------------------------------

## Brief History

-   **Descartes:**\
    Argued that a machine could never truly imitate a human; there would always be a way to tell the difference.
-   **Turing Test:**\
    Proposed by Alan Turing as a test of a machine’s ability to exhibit intelligent behavior indistinguishable from a human.
    -   A human judge engages in a conversation with both a human and a machine.
    -   If the judge cannot reliably tell which is which, the machine is said to have passed the test.

## NLP: The Basic Approach

### Background

-   **Numerical Data:** Numbers, measurements, etc.
-   **Categorical Data:** Discrete categories or labels.
-   **Text Data:** Unstructured, variable-length, and context-dependent (e.g., email content, headlines, political speeches).

Text data is fundamentally different from structured data. Can we treat language as structured data for machine learning?

#### Making Language into Structured Data

-   **Bag-of-Words Representation:**
    -   Assign a feature (column) for each word in the vocabulary.
    -   For a given text, the value is 1 if the word occurs, 0 otherwise.
    -   Alternative values: word counts or TF-IDF scores.
    -   Most features are 0 for any given text (sparse representation).

#### Supervised ML for Text Processing

-   **Labeled Text Data:** Enables building classifiers for:
    -   Spam Detection
    -   Sentiment Analysis
    -   Topic Detection
    -   ...and more
-   Raises questions: Does this approach capture real understanding? How does it relate to the Turing Test?

#### Bag-of-Words Limitation

-   Ignores word order and context ("bag" of words).
-   Cannot distinguish between "dog bites man" and "man bites dog".

------------------------------------------------------------------------

### Language Modeling with N-grams

-   **Goal:** Assign a probability to a sequence of words.
-   **Markov Assumption:** The probability of a word depends only on the previous *n-1* words.

#### Example: "He went to the store"

-   **Unigrams (1-grams):** He, went, to, the, store
-   **Bigrams (2-grams):** He went, went to, to the, the store
-   **Trigrams (3-grams):** He went to, went to the, to the store
-   **4-grams:** He went to the, went to the store
-   **5-gram:** He went to the store

#### N-gram Approximations

-   **Bigram Model:**\
    ( p(\text{He went to the store}) = p(\text{He}) \times p(\text{went}\|\text{He}) \times p(\text{to}\|\text{went}) \times p(\text{the}\|\text{to}) \times p(\text{store}\|\text{the}) )

-   **Trigram Model:**\
    ( p(\text{He went to the store}) = p(\text{He}) \times p(\text{went}\|\text{He}) \times p(\text{to}\|\text{He went}) \times p(\text{the}\|\text{went to}) \times p(\text{store}\|\text{to the}) )

-   **Key Idea:**\
    N-gram models capture some local word order, but struggle with long-range dependencies and rare phrases.

------------------------------------------------------------------------

### Training an LLM

> “A general language model (LM) should be able to compute the probability of (and also generate) any string.”\
> *(Radford et al., 2019)*

### What Does Training an LLM Look Like?

Consider how humans complete sentences: - *As Descartes said, "I think, therefore I **." -*** **For all intents and**  - \*I learned how to drive a \_\_\_\*

Or in dialogue: \> **Monica:** Okay, everybody relax. This is not even a \_\_\_\
\> **Rachel:** Oh God... well, it started about a half hour before the \_\_\_\
\> **Ross:** (squatting and reading the instructions) I’m supposed to attach a brackety \_\_\_

The core training objective for LLMs is **next word prediction**:\
Given a sequence of words, predict the most likely next word.

#### Neural Network for Next Word Prediction

-   The model is a neural network that outputs a score for every word in the vocabulary.
-   These scores are converted into probabilities using the **softmax** function.
-   For example, with a vocabulary of 50,000 words, the output might look like: `[fish: 0.00002, help: 0.00002, ..., the: 0.00002, ..., aardvarks: 0.00002]`

#### Training Process

1.  **Compute Loss:**
    -   The true next word is masked (hidden).
    -   The model predicts probabilities for all words.
    -   The loss function measures how well the model predicts the correct word (e.g., *Loss = 1 - prob(correct word)*).
    -   If the model assigns a probability of 1 to the correct word, loss is 0 (best). If 0, loss is 1 (worst).
2.  **Update Weights:**
    -   The model adjusts its internal weights to increase the probability of the correct word.
    -   This also slightly decreases the probability for all other words.
    -   Each training example provides a small update—repeated millions or billions of times.

#### Example: Weight Updates

Suppose the correct next word is "fish": - The model increases the weights leading to "fish". - The probabilities for other words (e.g., "help", "the", "aardvarks") are slightly reduced.

> Each example nudges the model to make the correct word more likely in context, and less likely for others.\
> Over time, the model learns to predict words in a wide variety of contexts.

------------------------------------------------------------------------

*This is the fundamental process behind training large language models: predict the next word, compute the loss, update the weights, and repeat—at massive scale.*

### Probing GPT

Large Language Models (LLMs) today generate highly coherent, grammatical text that can be indistinguishable from human output. They demonstrate some understanding of hierarchical structure and abstract linguistic categories (Mahowald et al., 2024).

While these models are not perfect learners of abstract linguistic rules, neither are humans. LLMs are progressing toward acquiring formal linguistic competence and have already challenged claims about the impossibility of learning certain linguistic knowledge—such as hierarchical structure and abstract categories—from input alone (Mahowald et al., 2024).

------------------------------------------------------------------------

## AI: Where Are We Heading?

**Artificial General Intelligence (AGI):**\
AGI is defined as AI that matches or surpasses human cognitive capabilities across a wide range of tasks.

**AGI Benchmarks:**

-   **The Robot College Student Test (Goertzel):**\
    A machine enrolls in a university, takes and passes the same classes as humans, and obtains a degree. LLMs can now pass university-level exams without attending classes.

-   **The Employment Test (Nilsson):**\
    A machine performs an economically important job at least as well as humans. AI is already replacing humans in roles ranging from fast food to marketing.

-   **The Ikea Test (Marcus):**\
    An AI views the parts and instructions of an Ikea flat-pack product, then controls a robot to assemble the furniture correctly.

-   **The Coffee Test (Wozniak):**\
    A machine enters an average home and figures out how to make coffee: find the machine, coffee, water, mug, and brew coffee by pushing the right buttons. This remains unsolved.

-   **The Modern Turing Test (Suleyman):**\
    An AI is given \$100,000 and must turn it into \$1 million.

------------------------------------------------------------------------

## Applications of LLMs

-   AI interfaces for customer support and onboarding
-   Research portals with Retrieval-Augmented Generation (RAG)
-   Automated customer support (e.g., Zendesk)
-   Accessibility tools (e.g., BeMyAI)

------------------------------------------------------------------------

## Takeaways

-   **Language is Hard:**
    -   Language is infinite and ambiguous (both lexically and structurally).
-   **The Revolution in NLP:**
    -   LLMs now approach human-level language ability.
-   **Exciting Research Directions:**
    -   Building applications with LLMs
    -   Probing their abilities
-   **Powerful AI is Coming:**
    -   The field is rapidly advancing, with significant societal impact on the horizon.