<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.42">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Tim Roessling">

<title>14&nbsp; Concepts and Explanations Long – NLP Notes</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./summary_concise.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-2f5df379a58b258e96c21c0638c20c03.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-8da5b4427184b79ecddefad3d342027e.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./summary.html"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Concepts and Explanations Long</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">NLP Notes</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">NLP Notes</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./session1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Introduction to Natural Language Processing</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./session2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">NLP Libraries and Text Normalization</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./session3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">N-Gram Language Models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./session4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Text Classification and Naive Bayes</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./session5.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Logistic Regression and Text Representation</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./session6.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Sentiment Analysis</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./session7.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Topic Modeling</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./session8.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Lexical Semantics and Vector Embeddings</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./session9.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Simple Neural Networks</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./session10.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">The Transformer and Pre-trained Language Models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./session11.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Question Answering and Information Retrieval</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./summary_concise.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Concepts and Explanations Short</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./summary.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Concepts and Explanations Long</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#concepts-and-explanations---chapter-1" id="toc-concepts-and-explanations---chapter-1" class="nav-link active" data-scroll-target="#concepts-and-explanations---chapter-1"><span class="header-section-number">14.1</span> Concepts and Explanations - Chapter 1</a>
  <ul class="collapse">
  <li><a href="#the-revolution-in-nlp" id="toc-the-revolution-in-nlp" class="nav-link" data-scroll-target="#the-revolution-in-nlp"><span class="header-section-number">14.1.1</span> The Revolution in NLP</a></li>
  <li><a href="#language-is-hard" id="toc-language-is-hard" class="nav-link" data-scroll-target="#language-is-hard"><span class="header-section-number">14.1.2</span> Language is Hard</a></li>
  <li><a href="#brief-history" id="toc-brief-history" class="nav-link" data-scroll-target="#brief-history"><span class="header-section-number">14.1.3</span> Brief History</a></li>
  <li><a href="#nlp-the-basic-approach" id="toc-nlp-the-basic-approach" class="nav-link" data-scroll-target="#nlp-the-basic-approach"><span class="header-section-number">14.1.4</span> NLP: The Basic Approach</a></li>
  <li><a href="#supervised-ml-for-text-processing" id="toc-supervised-ml-for-text-processing" class="nav-link" data-scroll-target="#supervised-ml-for-text-processing"><span class="header-section-number">14.1.5</span> Supervised ML for Text Processing</a></li>
  <li><a href="#bag-of-words-limitation" id="toc-bag-of-words-limitation" class="nav-link" data-scroll-target="#bag-of-words-limitation"><span class="header-section-number">14.1.6</span> Bag-of-Words Limitation</a></li>
  <li><a href="#language-modeling-with-n-grams" id="toc-language-modeling-with-n-grams" class="nav-link" data-scroll-target="#language-modeling-with-n-grams"><span class="header-section-number">14.1.7</span> Language Modeling with N-grams</a></li>
  <li><a href="#training-an-llm" id="toc-training-an-llm" class="nav-link" data-scroll-target="#training-an-llm"><span class="header-section-number">14.1.8</span> Training an LLM</a></li>
  <li><a href="#probing-gpt" id="toc-probing-gpt" class="nav-link" data-scroll-target="#probing-gpt"><span class="header-section-number">14.1.9</span> Probing GPT</a></li>
  <li><a href="#ai-where-are-we-heading" id="toc-ai-where-are-we-heading" class="nav-link" data-scroll-target="#ai-where-are-we-heading"><span class="header-section-number">14.1.10</span> AI: Where Are We Heading?</a></li>
  <li><a href="#applications-of-llms" id="toc-applications-of-llms" class="nav-link" data-scroll-target="#applications-of-llms"><span class="header-section-number">14.1.11</span> Applications of LLMs</a></li>
  <li><a href="#takeaways" id="toc-takeaways" class="nav-link" data-scroll-target="#takeaways"><span class="header-section-number">14.1.12</span> Takeaways</a></li>
  </ul></li>
  <li><a href="#concepts-and-explanations---chapter-2" id="toc-concepts-and-explanations---chapter-2" class="nav-link" data-scroll-target="#concepts-and-explanations---chapter-2"><span class="header-section-number">14.2</span> Concepts and Explanations - Chapter 2</a>
  <ul class="collapse">
  <li><a href="#nlp-libraries" id="toc-nlp-libraries" class="nav-link" data-scroll-target="#nlp-libraries"><span class="header-section-number">14.2.1</span> NLP Libraries</a></li>
  <li><a href="#normalization" id="toc-normalization" class="nav-link" data-scroll-target="#normalization"><span class="header-section-number">14.2.2</span> Normalization</a></li>
  <li><a href="#morphological-normalization" id="toc-morphological-normalization" class="nav-link" data-scroll-target="#morphological-normalization"><span class="header-section-number">14.2.3</span> Morphological Normalization</a></li>
  <li><a href="#tokenization" id="toc-tokenization" class="nav-link" data-scroll-target="#tokenization"><span class="header-section-number">14.2.4</span> Tokenization</a></li>
  <li><a href="#stemming" id="toc-stemming" class="nav-link" data-scroll-target="#stemming"><span class="header-section-number">14.2.5</span> Stemming</a></li>
  <li><a href="#lemmatization" id="toc-lemmatization" class="nav-link" data-scroll-target="#lemmatization"><span class="header-section-number">14.2.6</span> Lemmatization</a></li>
  <li><a href="#regular-expressions" id="toc-regular-expressions" class="nav-link" data-scroll-target="#regular-expressions"><span class="header-section-number">14.2.7</span> Regular Expressions</a></li>
  <li><a href="#pos-tagging" id="toc-pos-tagging" class="nav-link" data-scroll-target="#pos-tagging"><span class="header-section-number">14.2.8</span> POS Tagging</a></li>
  <li><a href="#named-entity-recognition-ner" id="toc-named-entity-recognition-ner" class="nav-link" data-scroll-target="#named-entity-recognition-ner"><span class="header-section-number">14.2.9</span> Named Entity Recognition (NER)</a></li>
  </ul></li>
  <li><a href="#concepts-and-explanations---chapter-3" id="toc-concepts-and-explanations---chapter-3" class="nav-link" data-scroll-target="#concepts-and-explanations---chapter-3"><span class="header-section-number">14.3</span> Concepts and Explanations - Chapter 3</a>
  <ul class="collapse">
  <li><a href="#n-gram-language-models" id="toc-n-gram-language-models" class="nav-link" data-scroll-target="#n-gram-language-models"><span class="header-section-number">14.3.1</span> N-Gram Language Models</a></li>
  <li><a href="#conditional-probability-and-chain-rule" id="toc-conditional-probability-and-chain-rule" class="nav-link" data-scroll-target="#conditional-probability-and-chain-rule"><span class="header-section-number">14.3.2</span> Conditional Probability and Chain Rule</a></li>
  <li><a href="#markov-assumption" id="toc-markov-assumption" class="nav-link" data-scroll-target="#markov-assumption"><span class="header-section-number">14.3.3</span> Markov Assumption</a></li>
  <li><a href="#n-gram-probability-calculation-mle" id="toc-n-gram-probability-calculation-mle" class="nav-link" data-scroll-target="#n-gram-probability-calculation-mle"><span class="header-section-number">14.3.4</span> N-Gram Probability Calculation &amp; MLE</a></li>
  <li><a href="#padding" id="toc-padding" class="nav-link" data-scroll-target="#padding"><span class="header-section-number">14.3.5</span> Padding</a></li>
  <li><a href="#underflow" id="toc-underflow" class="nav-link" data-scroll-target="#underflow"><span class="header-section-number">14.3.6</span> Underflow</a></li>
  <li><a href="#smoothing-techniques" id="toc-smoothing-techniques" class="nav-link" data-scroll-target="#smoothing-techniques"><span class="header-section-number">14.3.7</span> Smoothing Techniques</a></li>
  <li><a href="#perplexity" id="toc-perplexity" class="nav-link" data-scroll-target="#perplexity"><span class="header-section-number">14.3.8</span> Perplexity</a></li>
  <li><a href="#practical-implementation" id="toc-practical-implementation" class="nav-link" data-scroll-target="#practical-implementation"><span class="header-section-number">14.3.9</span> Practical Implementation</a></li>
  </ul></li>
  <li><a href="#concepts-and-explanations---chapter-4" id="toc-concepts-and-explanations---chapter-4" class="nav-link" data-scroll-target="#concepts-and-explanations---chapter-4"><span class="header-section-number">14.4</span> Concepts and Explanations - Chapter 4</a>
  <ul class="collapse">
  <li><a href="#text-classification" id="toc-text-classification" class="nav-link" data-scroll-target="#text-classification"><span class="header-section-number">14.4.1</span> Text Classification</a></li>
  <li><a href="#types-of-text-classification-techniques" id="toc-types-of-text-classification-techniques" class="nav-link" data-scroll-target="#types-of-text-classification-techniques"><span class="header-section-number">14.4.2</span> Types of Text Classification Techniques</a></li>
  <li><a href="#bayes-theorem" id="toc-bayes-theorem" class="nav-link" data-scroll-target="#bayes-theorem"><span class="header-section-number">14.4.3</span> Bayes Theorem</a></li>
  <li><a href="#types-of-naive-bayes-classifiers" id="toc-types-of-naive-bayes-classifiers" class="nav-link" data-scroll-target="#types-of-naive-bayes-classifiers"><span class="header-section-number">14.4.4</span> Types of Naive Bayes Classifiers</a></li>
  <li><a href="#implementing-naive-bayes" id="toc-implementing-naive-bayes" class="nav-link" data-scroll-target="#implementing-naive-bayes"><span class="header-section-number">14.4.5</span> Implementing Naive Bayes</a></li>
  <li><a href="#evaluation-metrics" id="toc-evaluation-metrics" class="nav-link" data-scroll-target="#evaluation-metrics"><span class="header-section-number">14.4.6</span> Evaluation Metrics</a></li>
  <li><a href="#disadvantages-of-naive-bayes" id="toc-disadvantages-of-naive-bayes" class="nav-link" data-scroll-target="#disadvantages-of-naive-bayes"><span class="header-section-number">14.4.7</span> Disadvantages of Naive Bayes</a></li>
  <li><a href="#handling-class-imbalance" id="toc-handling-class-imbalance" class="nav-link" data-scroll-target="#handling-class-imbalance"><span class="header-section-number">14.4.8</span> Handling Class Imbalance</a></li>
  </ul></li>
  <li><a href="#concepts-and-explanations---chapter-5" id="toc-concepts-and-explanations---chapter-5" class="nav-link" data-scroll-target="#concepts-and-explanations---chapter-5"><span class="header-section-number">14.5</span> Concepts and Explanations - Chapter 5</a>
  <ul class="collapse">
  <li><a href="#logistic-regression" id="toc-logistic-regression" class="nav-link" data-scroll-target="#logistic-regression"><span class="header-section-number">14.5.1</span> Logistic Regression</a></li>
  <li><a href="#log-odds-logit" id="toc-log-odds-logit" class="nav-link" data-scroll-target="#log-odds-logit"><span class="header-section-number">14.5.2</span> Log-Odds (Logit)</a></li>
  <li><a href="#training-logistic-regression" id="toc-training-logistic-regression" class="nav-link" data-scroll-target="#training-logistic-regression"><span class="header-section-number">14.5.3</span> Training Logistic Regression</a></li>
  <li><a href="#bag-of-words-vs.-tf-idf" id="toc-bag-of-words-vs.-tf-idf" class="nav-link" data-scroll-target="#bag-of-words-vs.-tf-idf"><span class="header-section-number">14.5.4</span> Bag of Words vs.&nbsp;TF-IDF</a></li>
  <li><a href="#lr-model-assumptions" id="toc-lr-model-assumptions" class="nav-link" data-scroll-target="#lr-model-assumptions"><span class="header-section-number">14.5.5</span> LR Model Assumptions</a></li>
  <li><a href="#lr-limitations" id="toc-lr-limitations" class="nav-link" data-scroll-target="#lr-limitations"><span class="header-section-number">14.5.6</span> LR Limitations</a></li>
  </ul></li>
  <li><a href="#concepts-and-explanations---chapter-6" id="toc-concepts-and-explanations---chapter-6" class="nav-link" data-scroll-target="#concepts-and-explanations---chapter-6"><span class="header-section-number">14.6</span> Concepts and Explanations - Chapter 6</a>
  <ul class="collapse">
  <li><a href="#sentiment-analysis-sa" id="toc-sentiment-analysis-sa" class="nav-link" data-scroll-target="#sentiment-analysis-sa"><span class="header-section-number">14.6.1</span> Sentiment Analysis (SA)</a></li>
  <li><a href="#resources-for-sentiment-lexicons" id="toc-resources-for-sentiment-lexicons" class="nav-link" data-scroll-target="#resources-for-sentiment-lexicons"><span class="header-section-number">14.6.2</span> Resources for Sentiment Lexicons</a></li>
  <li><a href="#sentiment-analysis-approaches" id="toc-sentiment-analysis-approaches" class="nav-link" data-scroll-target="#sentiment-analysis-approaches"><span class="header-section-number">14.6.3</span> Sentiment Analysis Approaches</a></li>
  <li><a href="#model-comparison" id="toc-model-comparison" class="nav-link" data-scroll-target="#model-comparison"><span class="header-section-number">14.6.4</span> Model Comparison</a></li>
  <li><a href="#python-libraries-and-tools-for-sa" id="toc-python-libraries-and-tools-for-sa" class="nav-link" data-scroll-target="#python-libraries-and-tools-for-sa"><span class="header-section-number">14.6.5</span> Python Libraries and Tools for SA</a></li>
  <li><a href="#sa-challenges" id="toc-sa-challenges" class="nav-link" data-scroll-target="#sa-challenges"><span class="header-section-number">14.6.6</span> SA Challenges</a></li>
  </ul></li>
  <li><a href="#concepts-and-explanations---chapter-7" id="toc-concepts-and-explanations---chapter-7" class="nav-link" data-scroll-target="#concepts-and-explanations---chapter-7"><span class="header-section-number">14.7</span> Concepts and Explanations - Chapter 7</a>
  <ul class="collapse">
  <li><a href="#topic-modeling" id="toc-topic-modeling" class="nav-link" data-scroll-target="#topic-modeling"><span class="header-section-number">14.7.1</span> Topic Modeling</a></li>
  <li><a href="#latent-dirichlet-allocation-lda" id="toc-latent-dirichlet-allocation-lda" class="nav-link" data-scroll-target="#latent-dirichlet-allocation-lda"><span class="header-section-number">14.7.2</span> Latent Dirichlet Allocation (LDA)</a></li>
  <li><a href="#lda-intuition-and-components" id="toc-lda-intuition-and-components" class="nav-link" data-scroll-target="#lda-intuition-and-components"><span class="header-section-number">14.7.3</span> LDA Intuition and Components</a></li>
  <li><a href="#lda-parameters-and-trade-offs" id="toc-lda-parameters-and-trade-offs" class="nav-link" data-scroll-target="#lda-parameters-and-trade-offs"><span class="header-section-number">14.7.4</span> LDA Parameters and Trade-offs</a></li>
  <li><a href="#parameter-estimation-in-lda" id="toc-parameter-estimation-in-lda" class="nav-link" data-scroll-target="#parameter-estimation-in-lda"><span class="header-section-number">14.7.5</span> Parameter Estimation in LDA</a></li>
  <li><a href="#gibbs-sampling-in-lda" id="toc-gibbs-sampling-in-lda" class="nav-link" data-scroll-target="#gibbs-sampling-in-lda"><span class="header-section-number">14.7.6</span> Gibbs Sampling in LDA</a></li>
  <li><a href="#practical-considerations-for-lda" id="toc-practical-considerations-for-lda" class="nav-link" data-scroll-target="#practical-considerations-for-lda"><span class="header-section-number">14.7.7</span> Practical Considerations for LDA</a></li>
  <li><a href="#visualizing-and-using-lda-results" id="toc-visualizing-and-using-lda-results" class="nav-link" data-scroll-target="#visualizing-and-using-lda-results"><span class="header-section-number">14.7.8</span> Visualizing and Using LDA Results</a></li>
  <li><a href="#top2vec" id="toc-top2vec" class="nav-link" data-scroll-target="#top2vec"><span class="header-section-number">14.7.9</span> Top2Vec</a></li>
  <li><a href="#bertopic" id="toc-bertopic" class="nav-link" data-scroll-target="#bertopic"><span class="header-section-number">14.7.10</span> BERTopic</a></li>
  <li><a href="#k-means-clustering-with-word2vec" id="toc-k-means-clustering-with-word2vec" class="nav-link" data-scroll-target="#k-means-clustering-with-word2vec"><span class="header-section-number">14.7.11</span> K-means Clustering with Word2Vec</a></li>
  <li><a href="#the-revolution-in-nlp-transformers-and-llms" id="toc-the-revolution-in-nlp-transformers-and-llms" class="nav-link" data-scroll-target="#the-revolution-in-nlp-transformers-and-llms"><span class="header-section-number">14.7.12</span> The Revolution in NLP: Transformers and LLMs</a></li>
  <li><a href="#language-is-hard-1" id="toc-language-is-hard-1" class="nav-link" data-scroll-target="#language-is-hard-1"><span class="header-section-number">14.7.13</span> Language is Hard</a></li>
  <li><a href="#brief-history-turing-test-and-agi-benchmarks" id="toc-brief-history-turing-test-and-agi-benchmarks" class="nav-link" data-scroll-target="#brief-history-turing-test-and-agi-benchmarks"><span class="header-section-number">14.7.14</span> Brief History: Turing Test and AGI Benchmarks</a></li>
  <li><a href="#nlp-the-basic-approach-1" id="toc-nlp-the-basic-approach-1" class="nav-link" data-scroll-target="#nlp-the-basic-approach-1"><span class="header-section-number">14.7.15</span> NLP: The Basic Approach</a></li>
  <li><a href="#language-modeling-with-n-grams-1" id="toc-language-modeling-with-n-grams-1" class="nav-link" data-scroll-target="#language-modeling-with-n-grams-1"><span class="header-section-number">14.7.16</span> Language Modeling with N-grams</a></li>
  <li><a href="#training-an-llm-1" id="toc-training-an-llm-1" class="nav-link" data-scroll-target="#training-an-llm-1"><span class="header-section-number">14.7.17</span> Training an LLM</a></li>
  <li><a href="#probing-gpt-and-llm-capabilities" id="toc-probing-gpt-and-llm-capabilities" class="nav-link" data-scroll-target="#probing-gpt-and-llm-capabilities"><span class="header-section-number">14.7.18</span> Probing GPT and LLM Capabilities</a></li>
  <li><a href="#applications-of-llms-1" id="toc-applications-of-llms-1" class="nav-link" data-scroll-target="#applications-of-llms-1"><span class="header-section-number">14.7.19</span> Applications of LLMs</a></li>
  <li><a href="#takeaways-1" id="toc-takeaways-1" class="nav-link" data-scroll-target="#takeaways-1"><span class="header-section-number">14.7.20</span> Takeaways</a></li>
  </ul></li>
  <li><a href="#concepts-and-explanations---chapter-8" id="toc-concepts-and-explanations---chapter-8" class="nav-link" data-scroll-target="#concepts-and-explanations---chapter-8"><span class="header-section-number">14.8</span> Concepts and Explanations - Chapter 8</a>
  <ul class="collapse">
  <li><a href="#word-meanings" id="toc-word-meanings" class="nav-link" data-scroll-target="#word-meanings"><span class="header-section-number">14.8.1</span> Word Meanings</a></li>
  <li><a href="#word-relations" id="toc-word-relations" class="nav-link" data-scroll-target="#word-relations"><span class="header-section-number">14.8.2</span> Word Relations</a></li>
  <li><a href="#vector-semantics" id="toc-vector-semantics" class="nav-link" data-scroll-target="#vector-semantics"><span class="header-section-number">14.8.3</span> Vector Semantics</a></li>
  <li><a href="#word2vec" id="toc-word2vec" class="nav-link" data-scroll-target="#word2vec"><span class="header-section-number">14.8.4</span> Word2Vec</a></li>
  <li><a href="#takeaways-2" id="toc-takeaways-2" class="nav-link" data-scroll-target="#takeaways-2"><span class="header-section-number">14.8.5</span> Takeaways</a></li>
  </ul></li>
  <li><a href="#concepts-and-explanations---chapter-9" id="toc-concepts-and-explanations---chapter-9" class="nav-link" data-scroll-target="#concepts-and-explanations---chapter-9"><span class="header-section-number">14.9</span> Concepts and Explanations - Chapter 9</a>
  <ul class="collapse">
  <li><a href="#ambiguity-in-language" id="toc-ambiguity-in-language" class="nav-link" data-scroll-target="#ambiguity-in-language"><span class="header-section-number">14.9.1</span> Ambiguity in Language</a></li>
  <li><a href="#simple-neural-networks-and-neural-language-models" id="toc-simple-neural-networks-and-neural-language-models" class="nav-link" data-scroll-target="#simple-neural-networks-and-neural-language-models"><span class="header-section-number">14.9.2</span> Simple Neural Networks and Neural Language Models</a></li>
  <li><a href="#activation-functions" id="toc-activation-functions" class="nav-link" data-scroll-target="#activation-functions"><span class="header-section-number">14.9.3</span> Activation Functions</a></li>
  <li><a href="#multi-layer-neural-networks-and-xor" id="toc-multi-layer-neural-networks-and-xor" class="nav-link" data-scroll-target="#multi-layer-neural-networks-and-xor"><span class="header-section-number">14.9.4</span> Multi-Layer Neural Networks and XOR</a></li>
  <li><a href="#feedforward-neural-networks-multilayer-perceptrons" id="toc-feedforward-neural-networks-multilayer-perceptrons" class="nav-link" data-scroll-target="#feedforward-neural-networks-multilayer-perceptrons"><span class="header-section-number">14.9.5</span> Feedforward Neural Networks (Multilayer Perceptrons)</a></li>
  <li><a href="#applying-feedforward-networks-to-nlp-tasks" id="toc-applying-feedforward-networks-to-nlp-tasks" class="nav-link" data-scroll-target="#applying-feedforward-networks-to-nlp-tasks"><span class="header-section-number">14.9.6</span> Applying Feedforward Networks to NLP Tasks</a></li>
  <li><a href="#training-neural-networks" id="toc-training-neural-networks" class="nav-link" data-scroll-target="#training-neural-networks"><span class="header-section-number">14.9.7</span> Training Neural Networks</a></li>
  </ul></li>
  <li><a href="#concepts-and-explanations---chapter-10" id="toc-concepts-and-explanations---chapter-10" class="nav-link" data-scroll-target="#concepts-and-explanations---chapter-10"><span class="header-section-number">14.10</span> Concepts and Explanations - Chapter 10</a>
  <ul class="collapse">
  <li><a href="#descartes-and-the-nature-of-thought" id="toc-descartes-and-the-nature-of-thought" class="nav-link" data-scroll-target="#descartes-and-the-nature-of-thought"><span class="header-section-number">14.10.1</span> Descartes and the Nature of Thought</a></li>
  <li><a href="#the-turing-test" id="toc-the-turing-test" class="nav-link" data-scroll-target="#the-turing-test"><span class="header-section-number">14.10.2</span> The Turing Test</a></li>
  <li><a href="#singularity-and-agi" id="toc-singularity-and-agi" class="nav-link" data-scroll-target="#singularity-and-agi"><span class="header-section-number">14.10.3</span> Singularity and AGI</a></li>
  <li><a href="#the-challenge-of-language" id="toc-the-challenge-of-language" class="nav-link" data-scroll-target="#the-challenge-of-language"><span class="header-section-number">14.10.4</span> The Challenge of Language</a></li>
  <li><a href="#miracle-of-llms-3-key-insights" id="toc-miracle-of-llms-3-key-insights" class="nav-link" data-scroll-target="#miracle-of-llms-3-key-insights"><span class="header-section-number">14.10.5</span> Miracle of LLMs – 3 Key Insights</a></li>
  <li><a href="#word-embeddings" id="toc-word-embeddings" class="nav-link" data-scroll-target="#word-embeddings"><span class="header-section-number">14.10.6</span> Word Embeddings</a></li>
  <li><a href="#transformers" id="toc-transformers" class="nav-link" data-scroll-target="#transformers"><span class="header-section-number">14.10.7</span> Transformers</a></li>
  <li><a href="#bert-and-decoder-only-models" id="toc-bert-and-decoder-only-models" class="nav-link" data-scroll-target="#bert-and-decoder-only-models"><span class="header-section-number">14.10.8</span> BERT and Decoder-Only Models</a></li>
  <li><a href="#large-language-models-llms" id="toc-large-language-models-llms" class="nav-link" data-scroll-target="#large-language-models-llms"><span class="header-section-number">14.10.9</span> Large Language Models (LLMs)</a></li>
  <li><a href="#decoding-and-sampling" id="toc-decoding-and-sampling" class="nav-link" data-scroll-target="#decoding-and-sampling"><span class="header-section-number">14.10.10</span> Decoding and Sampling</a></li>
  <li><a href="#pretraining-and-self-supervised-learning" id="toc-pretraining-and-self-supervised-learning" class="nav-link" data-scroll-target="#pretraining-and-self-supervised-learning"><span class="header-section-number">14.10.11</span> Pretraining and Self-Supervised Learning</a></li>
  <li><a href="#working-with-large-language-models" id="toc-working-with-large-language-models" class="nav-link" data-scroll-target="#working-with-large-language-models"><span class="header-section-number">14.10.12</span> Working with Large Language Models</a></li>
  <li><a href="#takeaways-3" id="toc-takeaways-3" class="nav-link" data-scroll-target="#takeaways-3"><span class="header-section-number">14.10.13</span> Takeaways</a></li>
  </ul></li>
  <li><a href="#concepts-and-explanations---chapter-11" id="toc-concepts-and-explanations---chapter-11" class="nav-link" data-scroll-target="#concepts-and-explanations---chapter-11"><span class="header-section-number">14.11</span> Concepts and Explanations - Chapter 11</a>
  <ul class="collapse">
  <li><a href="#information-retrieval-ir" id="toc-information-retrieval-ir" class="nav-link" data-scroll-target="#information-retrieval-ir"><span class="header-section-number">14.11.1</span> Information Retrieval (IR)</a></li>
  <li><a href="#vector-space-model-tf-idf" id="toc-vector-space-model-tf-idf" class="nav-link" data-scroll-target="#vector-space-model-tf-idf"><span class="header-section-number">14.11.2</span> Vector Space Model &amp; TF-IDF</a></li>
  <li><a href="#dense-retrieval-with-neural-models" id="toc-dense-retrieval-with-neural-models" class="nav-link" data-scroll-target="#dense-retrieval-with-neural-models"><span class="header-section-number">14.11.3</span> Dense Retrieval with Neural Models</a></li>
  <li><a href="#question-answering-datasets" id="toc-question-answering-datasets" class="nav-link" data-scroll-target="#question-answering-datasets"><span class="header-section-number">14.11.4</span> Question Answering Datasets</a></li>
  <li><a href="#extractive-question-answering" id="toc-extractive-question-answering" class="nav-link" data-scroll-target="#extractive-question-answering"><span class="header-section-number">14.11.5</span> Extractive Question Answering</a></li>
  <li><a href="#entity-linking-wikification" id="toc-entity-linking-wikification" class="nav-link" data-scroll-target="#entity-linking-wikification"><span class="header-section-number">14.11.6</span> Entity Linking (Wikification)</a></li>
  <li><a href="#knowledge-based-question-answering-kbqa" id="toc-knowledge-based-question-answering-kbqa" class="nav-link" data-scroll-target="#knowledge-based-question-answering-kbqa"><span class="header-section-number">14.11.7</span> Knowledge-based Question Answering (KBQA)</a></li>
  <li><a href="#supervision-for-semantic-parsing" id="toc-supervision-for-semantic-parsing" class="nav-link" data-scroll-target="#supervision-for-semantic-parsing"><span class="header-section-number">14.11.8</span> Supervision for Semantic Parsing</a></li>
  <li><a href="#retrieve-and-generate-rag" id="toc-retrieve-and-generate-rag" class="nav-link" data-scroll-target="#retrieve-and-generate-rag"><span class="header-section-number">14.11.9</span> Retrieve-and-Generate (RAG)</a></li>
  <li><a href="#evaluation-of-rag-systems" id="toc-evaluation-of-rag-systems" class="nav-link" data-scroll-target="#evaluation-of-rag-systems"><span class="header-section-number">14.11.10</span> Evaluation of RAG Systems</a></li>
  <li><a href="#agents" id="toc-agents" class="nav-link" data-scroll-target="#agents"><span class="header-section-number">14.11.11</span> Agents</a></li>
  <li><a href="#formal-vs.-functional-abilities-of-llms" id="toc-formal-vs.-functional-abilities-of-llms" class="nav-link" data-scroll-target="#formal-vs.-functional-abilities-of-llms"><span class="header-section-number">14.11.12</span> Formal vs.&nbsp;Functional Abilities of LLMs</a></li>
  <li><a href="#takeaways-4" id="toc-takeaways-4" class="nav-link" data-scroll-target="#takeaways-4"><span class="header-section-number">14.11.13</span> Takeaways</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Concepts and Explanations Long</span></h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Tim Roessling </p>
          </div>
  </div>
    
  
    
  </div>
  


</header>


<section id="concepts-and-explanations---chapter-1" class="level2" data-number="14.1">
<h2 data-number="14.1" class="anchored" data-anchor-id="concepts-and-explanations---chapter-1"><span class="header-section-number">14.1</span> Concepts and Explanations - Chapter 1</h2>
<section id="the-revolution-in-nlp" class="level3" data-number="14.1.1">
<h3 data-number="14.1.1" class="anchored" data-anchor-id="the-revolution-in-nlp"><span class="header-section-number">14.1.1</span> The Revolution in NLP</h3>
<ul>
<li>The introduction of transformer models has dramatically advanced NLP, enabling models to learn from massive datasets using objectives like masked language modeling.</li>
<li>Reinforcement Learning from Human Feedback (RLHF) further improves model alignment with human preferences.</li>
</ul>
</section>
<section id="language-is-hard" class="level3" data-number="14.1.2">
<h3 data-number="14.1.2" class="anchored" data-anchor-id="language-is-hard"><span class="header-section-number">14.1.2</span> Language is Hard</h3>
<ul>
<li>Language is complex due to infinite possible sentences and inherent ambiguity (both lexical and structural).</li>
<li>Machines struggle with true understanding, and even advanced models may not fully replicate human-like comprehension.</li>
</ul>
</section>
<section id="brief-history" class="level3" data-number="14.1.3">
<h3 data-number="14.1.3" class="anchored" data-anchor-id="brief-history"><span class="header-section-number">14.1.3</span> Brief History</h3>
<ul>
<li>Descartes argued machines could never truly imitate humans, while Turing proposed the Turing Test to evaluate machine intelligence through conversation.</li>
<li>Passing the Turing Test means a machine’s responses are indistinguishable from a human’s.</li>
</ul>
</section>
<section id="nlp-the-basic-approach" class="level3" data-number="14.1.4">
<h3 data-number="14.1.4" class="anchored" data-anchor-id="nlp-the-basic-approach"><span class="header-section-number">14.1.4</span> NLP: The Basic Approach</h3>
<ul>
<li>Text data is unstructured and context-dependent, unlike numerical or categorical data.</li>
<li>Bag-of-words representations convert text into structured data but ignore word order and context, limiting understanding.</li>
</ul>
</section>
<section id="supervised-ml-for-text-processing" class="level3" data-number="14.1.5">
<h3 data-number="14.1.5" class="anchored" data-anchor-id="supervised-ml-for-text-processing"><span class="header-section-number">14.1.5</span> Supervised ML for Text Processing</h3>
<ul>
<li>Labeled text data enables tasks like spam detection, sentiment analysis, and topic detection.</li>
<li>Raises questions about whether these approaches capture real language understanding.</li>
</ul>
</section>
<section id="bag-of-words-limitation" class="level3" data-number="14.1.6">
<h3 data-number="14.1.6" class="anchored" data-anchor-id="bag-of-words-limitation"><span class="header-section-number">14.1.6</span> Bag-of-Words Limitation</h3>
<ul>
<li>Bag-of-words ignores word order, so it cannot distinguish between sentences with the same words in different orders.</li>
</ul>
</section>
<section id="language-modeling-with-n-grams" class="level3" data-number="14.1.7">
<h3 data-number="14.1.7" class="anchored" data-anchor-id="language-modeling-with-n-grams"><span class="header-section-number">14.1.7</span> Language Modeling with N-grams</h3>
<ul>
<li>N-gram models assign probabilities to word sequences based on the Markov assumption (dependence on previous n-1 words).</li>
<li>They capture local word order but struggle with long-range dependencies and rare phrases.</li>
</ul>
</section>
<section id="training-an-llm" class="level3" data-number="14.1.8">
<h3 data-number="14.1.8" class="anchored" data-anchor-id="training-an-llm"><span class="header-section-number">14.1.8</span> Training an LLM</h3>
<ul>
<li>LLMs are trained to predict the next word in a sequence, using neural networks and the softmax function to assign probabilities.</li>
<li>Training involves computing loss based on prediction accuracy and updating model weights to improve future predictions.</li>
</ul>
</section>
<section id="probing-gpt" class="level3" data-number="14.1.9">
<h3 data-number="14.1.9" class="anchored" data-anchor-id="probing-gpt"><span class="header-section-number">14.1.9</span> Probing GPT</h3>
<ul>
<li>LLMs generate coherent, grammatical text and show some understanding of linguistic structure.</li>
<li>They challenge previous claims about the impossibility of learning abstract linguistic knowledge from input alone.</li>
</ul>
</section>
<section id="ai-where-are-we-heading" class="level3" data-number="14.1.10">
<h3 data-number="14.1.10" class="anchored" data-anchor-id="ai-where-are-we-heading"><span class="header-section-number">14.1.10</span> AI: Where Are We Heading?</h3>
<ul>
<li>Artificial General Intelligence (AGI) aims for machines to match or surpass human cognitive abilities across tasks.</li>
<li>Benchmarks include passing university exams, performing jobs, assembling furniture, and completing complex real-world tasks.</li>
</ul>
</section>
<section id="applications-of-llms" class="level3" data-number="14.1.11">
<h3 data-number="14.1.11" class="anchored" data-anchor-id="applications-of-llms"><span class="header-section-number">14.1.11</span> Applications of LLMs</h3>
<ul>
<li>LLMs are used in customer support, research portals, automated help desks, and accessibility tools.</li>
</ul>
</section>
<section id="takeaways" class="level3" data-number="14.1.12">
<h3 data-number="14.1.12" class="anchored" data-anchor-id="takeaways"><span class="header-section-number">14.1.12</span> Takeaways</h3>
<ul>
<li>Language is inherently difficult for machines due to its infinite and ambiguous nature.</li>
<li>LLMs are approaching human-level language ability, and the field is advancing rapidly with significant societal impact expected.</li>
</ul>
</section>
</section>
<section id="concepts-and-explanations---chapter-2" class="level2" data-number="14.2">
<h2 data-number="14.2" class="anchored" data-anchor-id="concepts-and-explanations---chapter-2"><span class="header-section-number">14.2</span> Concepts and Explanations - Chapter 2</h2>
<section id="nlp-libraries" class="level3" data-number="14.2.1">
<h3 data-number="14.2.1" class="anchored" data-anchor-id="nlp-libraries"><span class="header-section-number">14.2.1</span> NLP Libraries</h3>
<ul>
<li><strong>NLTK</strong> and <strong>spaCy</strong> are two major Python libraries for NLP. NLTK is flexible and feature-rich but slower, while spaCy is faster and uses deep learning for tasks like POS tagging and NER.</li>
<li><strong>TextBlob</strong> is another library built on NLTK, offering a simple API for basic NLP tasks, but it is not suitable for large-scale production.</li>
</ul>
</section>
<section id="normalization" class="level3" data-number="14.2.2">
<h3 data-number="14.2.2" class="anchored" data-anchor-id="normalization"><span class="header-section-number">14.2.2</span> Normalization</h3>
<ul>
<li>Normalization standardizes text by converting to lowercase, removing punctuation, stopwords, extra spaces, and special characters.</li>
<li>It prepares text for further processing and analysis by reducing noise and inconsistencies.</li>
</ul>
</section>
<section id="morphological-normalization" class="level3" data-number="14.2.3">
<h3 data-number="14.2.3" class="anchored" data-anchor-id="morphological-normalization"><span class="header-section-number">14.2.3</span> Morphological Normalization</h3>
<ul>
<li>Involves reducing words to their root or base form using <strong>stemming</strong> or <strong>lemmatization</strong>.</li>
<li>Roots are the core part of words, while affixes (prefixes/suffixes) modify meaning or grammatical form.</li>
</ul>
</section>
<section id="tokenization" class="level3" data-number="14.2.4">
<h3 data-number="14.2.4" class="anchored" data-anchor-id="tokenization"><span class="header-section-number">14.2.4</span> Tokenization</h3>
<ul>
<li>Tokenization splits text into smaller units (tokens) such as words or sentences.</li>
<li>Tokens are used for further processing like stemming, lemmatization, and POS tagging; a corpus is a collection of such texts.</li>
</ul>
</section>
<section id="stemming" class="level3" data-number="14.2.5">
<h3 data-number="14.2.5" class="anchored" data-anchor-id="stemming"><span class="header-section-number">14.2.5</span> Stemming</h3>
<ul>
<li>Stemming reduces words to their root form by removing suffixes, often using rule-based algorithms like Porter or Snowball stemmers.</li>
<li>It is fast but may not always produce valid words, making it less accurate than lemmatization.</li>
</ul>
</section>
<section id="lemmatization" class="level3" data-number="14.2.6">
<h3 data-number="14.2.6" class="anchored" data-anchor-id="lemmatization"><span class="header-section-number">14.2.6</span> Lemmatization</h3>
<ul>
<li>Lemmatization maps words to their base or dictionary form (lemma) using vocabulary and POS information.</li>
<li>It is more accurate than stemming but slower and requires more resources.</li>
</ul>
</section>
<section id="regular-expressions" class="level3" data-number="14.2.7">
<h3 data-number="14.2.7" class="anchored" data-anchor-id="regular-expressions"><span class="header-section-number">14.2.7</span> Regular Expressions</h3>
<ul>
<li>Regular expressions (regex) are used for pattern matching and text manipulation, such as searching, extracting, or replacing text patterns.</li>
<li>Python’s <code>re</code> module provides methods like <code>match</code>, <code>search</code>, <code>findall</code>, <code>split</code>, and <code>sub</code> for regex operations.</li>
</ul>
</section>
<section id="pos-tagging" class="level3" data-number="14.2.8">
<h3 data-number="14.2.8" class="anchored" data-anchor-id="pos-tagging"><span class="header-section-number">14.2.8</span> POS Tagging</h3>
<ul>
<li>Part-of-speech (POS) tagging assigns grammatical categories (noun, verb, adjective, etc.) to each word in a text.</li>
<li>Methods include rule-based (using hand-crafted rules) and statistical (using machine learning models); spaCy and NLTK both support POS tagging.</li>
</ul>
</section>
<section id="named-entity-recognition-ner" class="level3" data-number="14.2.9">
<h3 data-number="14.2.9" class="anchored" data-anchor-id="named-entity-recognition-ner"><span class="header-section-number">14.2.9</span> Named Entity Recognition (NER)</h3>
<ul>
<li>NER identifies and classifies named entities (persons, organizations, locations, dates, etc.) in text.</li>
<li>It is crucial for extracting structured information from unstructured text and is supported by both NLTK and spaCy.</li>
</ul>
</section>
</section>
<section id="concepts-and-explanations---chapter-3" class="level2" data-number="14.3">
<h2 data-number="14.3" class="anchored" data-anchor-id="concepts-and-explanations---chapter-3"><span class="header-section-number">14.3</span> Concepts and Explanations - Chapter 3</h2>
<section id="n-gram-language-models" class="level3" data-number="14.3.1">
<h3 data-number="14.3.1" class="anchored" data-anchor-id="n-gram-language-models"><span class="header-section-number">14.3.1</span> N-Gram Language Models</h3>
<ul>
<li>N-gram models predict the probability of a word based on the previous n-1 words, commonly used for tasks like speech recognition and text generation.</li>
<li>Types include unigram (single word), bigram (pair), and trigram (triplet), with the number of n-grams calculated as (total words) - (n - 1).</li>
</ul>
</section>
<section id="conditional-probability-and-chain-rule" class="level3" data-number="14.3.2">
<h3 data-number="14.3.2" class="anchored" data-anchor-id="conditional-probability-and-chain-rule"><span class="header-section-number">14.3.2</span> Conditional Probability and Chain Rule</h3>
<ul>
<li>Joint probability measures the likelihood of multiple events occurring together, while conditional probability measures the likelihood of one event given another.</li>
<li>The chain rule allows calculation of the probability of a word sequence by multiplying conditional probabilities for each word given its context.</li>
</ul>
</section>
<section id="markov-assumption" class="level3" data-number="14.3.3">
<h3 data-number="14.3.3" class="anchored" data-anchor-id="markov-assumption"><span class="header-section-number">14.3.3</span> Markov Assumption</h3>
<ul>
<li>The Markov assumption simplifies language modeling by assuming a word depends only on the previous n-1 words, not the entire history.</li>
<li>First-order (bigram) and second-order (trigram) Markov models are common, reducing computational complexity.</li>
</ul>
</section>
<section id="n-gram-probability-calculation-mle" class="level3" data-number="14.3.4">
<h3 data-number="14.3.4" class="anchored" data-anchor-id="n-gram-probability-calculation-mle"><span class="header-section-number">14.3.4</span> N-Gram Probability Calculation &amp; MLE</h3>
<ul>
<li>The probability of a word sequence is estimated by counting n-gram occurrences and dividing by the count of the previous (n-1)-gram.</li>
<li>Maximum Likelihood Estimation (MLE) can assign zero probability to unseen n-grams, which is a limitation for generalization.</li>
</ul>
</section>
<section id="padding" class="level3" data-number="14.3.5">
<h3 data-number="14.3.5" class="anchored" data-anchor-id="padding"><span class="header-section-number">14.3.5</span> Padding</h3>
<ul>
<li>Padding uses special tokens (e.g., <code>&lt;s&gt;</code>, <code>&lt;/s&gt;</code>) at sentence boundaries to handle cases where context is missing at the start or end.</li>
<li>The number of padding tokens depends on the n-gram order (e.g., two for trigrams).</li>
</ul>
</section>
<section id="underflow" class="level3" data-number="14.3.6">
<h3 data-number="14.3.6" class="anchored" data-anchor-id="underflow"><span class="header-section-number">14.3.6</span> Underflow</h3>
<ul>
<li>Underflow occurs when multiplying many small probabilities, resulting in values too small for computers to represent.</li>
<li>Using log probabilities converts multiplication into addition, preventing underflow.</li>
</ul>
</section>
<section id="smoothing-techniques" class="level3" data-number="14.3.7">
<h3 data-number="14.3.7" class="anchored" data-anchor-id="smoothing-techniques"><span class="header-section-number">14.3.7</span> Smoothing Techniques</h3>
<ul>
<li>Smoothing addresses zero probabilities for unseen n-grams by assigning them small non-zero values.</li>
<li>Techniques include Laplace (add-one), add-k, Good-Turing, backoff/interpolation, and Kneser-Ney smoothing, each with different strategies for adjusting probabilities.</li>
</ul>
</section>
<section id="perplexity" class="level3" data-number="14.3.8">
<h3 data-number="14.3.8" class="anchored" data-anchor-id="perplexity"><span class="header-section-number">14.3.8</span> Perplexity</h3>
<ul>
<li>Perplexity measures how well a language model predicts a text; lower perplexity indicates a better model.</li>
<li>It is calculated as the inverse probability of the test set, normalized by the number of words.</li>
</ul>
</section>
<section id="practical-implementation" class="level3" data-number="14.3.9">
<h3 data-number="14.3.9" class="anchored" data-anchor-id="practical-implementation"><span class="header-section-number">14.3.9</span> Practical Implementation</h3>
<ul>
<li>Steps for building an n-gram model include tokenizing text, adding padding, generating n-grams, counting unique n-grams, calculating probabilities, and evaluating with perplexity.</li>
<li>Python code examples demonstrate these steps, including smoothing and sentence generation.</li>
</ul>
</section>
</section>
<section id="concepts-and-explanations---chapter-4" class="level2" data-number="14.4">
<h2 data-number="14.4" class="anchored" data-anchor-id="concepts-and-explanations---chapter-4"><span class="header-section-number">14.4</span> Concepts and Explanations - Chapter 4</h2>
<section id="text-classification" class="level3" data-number="14.4.1">
<h3 data-number="14.4.1" class="anchored" data-anchor-id="text-classification"><span class="header-section-number">14.4.1</span> Text Classification</h3>
<ul>
<li>Text classification assigns predefined categories to text documents using supervised, unsupervised, or deep learning methods.</li>
<li>It is widely used in applications like spam detection, sentiment analysis, and topic categorization.</li>
<li>The process involves data preprocessing, feature extraction, model training, and evaluation, with the choice of features and algorithms affecting performance.</li>
</ul>
</section>
<section id="types-of-text-classification-techniques" class="level3" data-number="14.4.2">
<h3 data-number="14.4.2" class="anchored" data-anchor-id="types-of-text-classification-techniques"><span class="header-section-number">14.4.2</span> Types of Text Classification Techniques</h3>
<ul>
<li><strong>Supervised Learning</strong>: Uses labeled data to train models (e.g., Naive Bayes, Logistic Regression).</li>
<li><strong>Unsupervised Learning</strong>: Finds patterns or clusters in unlabeled data (e.g., LDA, K-means).</li>
<li><strong>Deep Learning</strong>: Employs neural networks to automatically learn complex features (e.g., CNNs, RNNs, Transformers).</li>
</ul>
</section>
<section id="bayes-theorem" class="level3" data-number="14.4.3">
<h3 data-number="14.4.3" class="anchored" data-anchor-id="bayes-theorem"><span class="header-section-number">14.4.3</span> Bayes Theorem</h3>
<ul>
<li>Bayes theorem relates conditional probabilities and is used to update the probability of a hypothesis given new evidence.</li>
<li>In Naive Bayes, the “naive” assumption is that features are conditionally independent given the class label, simplifying probability calculations.</li>
</ul>
</section>
<section id="types-of-naive-bayes-classifiers" class="level3" data-number="14.4.4">
<h3 data-number="14.4.4" class="anchored" data-anchor-id="types-of-naive-bayes-classifiers"><span class="header-section-number">14.4.4</span> Types of Naive Bayes Classifiers</h3>
<ul>
<li><strong>Gaussian Naive Bayes</strong>: For continuous features, assumes a normal distribution.</li>
<li><strong>Multinomial Naive Bayes</strong>: For discrete features like word counts.</li>
<li><strong>Bernoulli Naive Bayes</strong>: For binary features, such as word presence/absence.</li>
</ul>
</section>
<section id="implementing-naive-bayes" class="level3" data-number="14.4.5">
<h3 data-number="14.4.5" class="anchored" data-anchor-id="implementing-naive-bayes"><span class="header-section-number">14.4.5</span> Implementing Naive Bayes</h3>
<ul>
<li>Steps include loading and preprocessing data, splitting into train/test sets, extracting features (e.g., Bag of Words), training the classifier, and evaluating performance.</li>
</ul>
</section>
<section id="evaluation-metrics" class="level3" data-number="14.4.6">
<h3 data-number="14.4.6" class="anchored" data-anchor-id="evaluation-metrics"><span class="header-section-number">14.4.6</span> Evaluation Metrics</h3>
<ul>
<li><strong>Accuracy</strong>: Proportion of correct predictions.</li>
<li><strong>Precision</strong>: Proportion of true positives among predicted positives.</li>
<li><strong>Recall</strong>: Proportion of true positives among actual positives.</li>
<li><strong>F1-Score</strong>: Harmonic mean of precision and recall, useful for imbalanced datasets.</li>
<li><strong>Classification Report</strong>: Summarizes these metrics for each class.</li>
</ul>
</section>
<section id="disadvantages-of-naive-bayes" class="level3" data-number="14.4.7">
<h3 data-number="14.4.7" class="anchored" data-anchor-id="disadvantages-of-naive-bayes"><span class="header-section-number">14.4.7</span> Disadvantages of Naive Bayes</h3>
<ul>
<li>The independence assumption may not hold, leading to suboptimal results.</li>
<li>Zero probability for unseen features can be problematic.</li>
<li>Probability estimates may be biased, especially for rare events.</li>
<li>Performance drops with highly correlated features or large, sparse feature spaces.</li>
<li>As a linear classifier, it may not capture complex relationships.</li>
</ul>
</section>
<section id="handling-class-imbalance" class="level3" data-number="14.4.8">
<h3 data-number="14.4.8" class="anchored" data-anchor-id="handling-class-imbalance"><span class="header-section-number">14.4.8</span> Handling Class Imbalance</h3>
<ul>
<li><strong>Resampling</strong>: Oversample the minority class or undersample the majority class to balance the dataset.</li>
<li><strong>Class Weights</strong>: Assign higher weights to the minority class to penalize misclassification.</li>
<li><strong>Ensemble Methods</strong>: Use models like Random Forest or Gradient Boosting to improve robustness and performance on imbalanced data.</li>
</ul>
</section>
</section>
<section id="concepts-and-explanations---chapter-5" class="level2" data-number="14.5">
<h2 data-number="14.5" class="anchored" data-anchor-id="concepts-and-explanations---chapter-5"><span class="header-section-number">14.5</span> Concepts and Explanations - Chapter 5</h2>
<section id="logistic-regression" class="level3" data-number="14.5.1">
<h3 data-number="14.5.1" class="anchored" data-anchor-id="logistic-regression"><span class="header-section-number">14.5.1</span> Logistic Regression</h3>
<ul>
<li>Logistic regression is a linear model for binary classification that estimates the probability of a class using the logistic (sigmoid) function.</li>
<li>It outputs probabilities between 0 and 1, making it suitable for tasks like predicting customer churn or disease diagnosis.</li>
<li>The model learns coefficients for each feature and a bias term, which together determine the decision boundary.</li>
</ul>
</section>
<section id="log-odds-logit" class="level3" data-number="14.5.2">
<h3 data-number="14.5.2" class="anchored" data-anchor-id="log-odds-logit"><span class="header-section-number">14.5.2</span> Log-Odds (Logit)</h3>
<ul>
<li>The log-odds (logit) is the logarithm of the odds ratio, representing the relationship between the probability of an event and its complement.</li>
<li>In logistic regression, the log-odds are modeled as a linear combination of input features and their coefficients.</li>
<li>This transformation allows for a linear relationship between features and the log-odds, even though the probability itself is non-linear.</li>
</ul>
</section>
<section id="training-logistic-regression" class="level3" data-number="14.5.3">
<h3 data-number="14.5.3" class="anchored" data-anchor-id="training-logistic-regression"><span class="header-section-number">14.5.3</span> Training Logistic Regression</h3>
<ul>
<li>Coefficients are estimated using Maximum Likelihood Estimation (MLE), which finds the values that maximize the likelihood of the observed data.</li>
<li>Regularization techniques like L2 (Ridge), L1 (Lasso), and Elastic Net help prevent overfitting and can perform feature selection.</li>
<li>Scikit-learn offers several solvers (liblinear, saga, newton-cg, lbfgs) optimized for different dataset sizes and regularization types.</li>
</ul>
</section>
<section id="bag-of-words-vs.-tf-idf" class="level3" data-number="14.5.4">
<h3 data-number="14.5.4" class="anchored" data-anchor-id="bag-of-words-vs.-tf-idf"><span class="header-section-number">14.5.4</span> Bag of Words vs.&nbsp;TF-IDF</h3>
<ul>
<li><strong>Bag of Words (BoW)</strong> represents text as word count vectors, ignoring word order and context.
<ul>
<li>Simple and effective for many tasks, but can result in high-dimensional, sparse data and does not capture word importance.</li>
</ul></li>
<li><strong>TF-IDF</strong> weighs words by their frequency in a document and their rarity across the corpus, highlighting more informative words.
<ul>
<li>Better for capturing word importance and semantic relevance, but still ignores word order and can be complex to implement.</li>
</ul></li>
</ul>
</section>
<section id="lr-model-assumptions" class="level3" data-number="14.5.5">
<h3 data-number="14.5.5" class="anchored" data-anchor-id="lr-model-assumptions"><span class="header-section-number">14.5.5</span> LR Model Assumptions</h3>
<ul>
<li>Assumes a linear relationship between features and the log-odds of the outcome.</li>
<li>Observations should be independent, and features should not be highly correlated (no multicollinearity).</li>
<li>Assumes absence of extreme outliers, as these can distort model estimates.</li>
</ul>
</section>
<section id="lr-limitations" class="level3" data-number="14.5.6">
<h3 data-number="14.5.6" class="anchored" data-anchor-id="lr-limitations"><span class="header-section-number">14.5.6</span> LR Limitations</h3>
<ul>
<li>The linearity assumption may not hold for all datasets, limiting model performance.</li>
<li>Sensitive to multicollinearity and outliers, which can affect coefficient stability and predictions.</li>
<li>Cannot capture complex, non-linear relationships between features and the target variable.</li>
<li>Requires a sufficiently large sample size (at least 10 times the number of features) to produce reliable results.</li>
</ul>
</section>
</section>
<section id="concepts-and-explanations---chapter-6" class="level2" data-number="14.6">
<h2 data-number="14.6" class="anchored" data-anchor-id="concepts-and-explanations---chapter-6"><span class="header-section-number">14.6</span> Concepts and Explanations - Chapter 6</h2>
<section id="sentiment-analysis-sa" class="level3" data-number="14.6.1">
<h3 data-number="14.6.1" class="anchored" data-anchor-id="sentiment-analysis-sa"><span class="header-section-number">14.6.1</span> Sentiment Analysis (SA)</h3>
<ul>
<li>Sentiment analysis determines the sentiment or opinion expressed in text, classifying it as positive, negative, or neutral.</li>
<li>A sentiment lexicon is a collection of words or phrases associated with specific sentiment scores, used to analyze the sentiment of text.</li>
<li>Ekman’s Six Basic Emotions (happiness, sadness, anger, fear, surprise, disgust) provide a foundation for categorizing emotional content.</li>
</ul>
</section>
<section id="resources-for-sentiment-lexicons" class="level3" data-number="14.6.2">
<h3 data-number="14.6.2" class="anchored" data-anchor-id="resources-for-sentiment-lexicons"><span class="header-section-number">14.6.2</span> Resources for Sentiment Lexicons</h3>
<ul>
<li><strong>VADER</strong>: Designed for social media, VADER combines lexical features and rules to handle negations and intensifiers, providing sentiment scores from -1 to 1.</li>
<li><strong>AFINN</strong>: Contains 3300+ English words rated for sentiment on a scale from -5 to +5, useful for social media and online reviews.</li>
<li><strong>SentiWordNet</strong>: Assigns positive, negative, and objective scores to WordNet synsets, enabling nuanced sentiment analysis based on word context.</li>
<li><strong>LIWC</strong>: Categorizes words into psychological and linguistic categories, offering insights into emotional and cognitive aspects of text.</li>
</ul>
</section>
<section id="sentiment-analysis-approaches" class="level3" data-number="14.6.3">
<h3 data-number="14.6.3" class="anchored" data-anchor-id="sentiment-analysis-approaches"><span class="header-section-number">14.6.3</span> Sentiment Analysis Approaches</h3>
<ul>
<li><strong>Rule-based</strong>: Uses predefined rules and lexicons to assign sentiment, relying on word lists and linguistic patterns. Simple and interpretable, but limited in handling context, sarcasm, and complex expressions.</li>
<li><strong>Machine learning-based</strong>: Trains models on labeled data to classify sentiment, using features like word frequencies or embeddings. More flexible and context-aware, but requires labeled data and may be less interpretable.</li>
<li><strong>Hybrid</strong>: Combines rule-based and machine learning methods to leverage the strengths of both, often using lexicons for initial scoring and ML models for refinement.</li>
</ul>
</section>
<section id="model-comparison" class="level3" data-number="14.6.4">
<h3 data-number="14.6.4" class="anchored" data-anchor-id="model-comparison"><span class="header-section-number">14.6.4</span> Model Comparison</h3>
<ul>
<li>Lexicon-based models (TextBlob, VADER) are simple and interpretable but struggle with negation and sarcasm.</li>
<li>Deep learning models (LSTM, BERT) learn context and can handle complex sentiment, with BERT excelling at sarcasm and nuanced language.</li>
</ul>
</section>
<section id="python-libraries-and-tools-for-sa" class="level3" data-number="14.6.5">
<h3 data-number="14.6.5" class="anchored" data-anchor-id="python-libraries-and-tools-for-sa"><span class="header-section-number">14.6.5</span> Python Libraries and Tools for SA</h3>
<ul>
<li><strong>NLTK</strong>: Provides tools for preprocessing and sentiment analysis, including VADER and WordNet integration.</li>
<li><strong>TextBlob</strong>: Offers a user-friendly API for sentiment analysis and other NLP tasks.</li>
<li><strong>TensorFlow</strong> and <strong>PyTorch</strong>: Enable building and training deep learning models for sentiment analysis.</li>
<li><strong>Hugging Face Transformers</strong>: Supplies pre-trained models (e.g., BERT) for advanced sentiment analysis and other NLP tasks.</li>
</ul>
</section>
<section id="sa-challenges" class="level3" data-number="14.6.6">
<h3 data-number="14.6.6" class="anchored" data-anchor-id="sa-challenges"><span class="header-section-number">14.6.6</span> SA Challenges</h3>
<ul>
<li>Lexicon-based methods struggle with context, sarcasm, ambiguity, cultural/domain-specific language, and negation.</li>
<li>Words can have different meanings depending on context, and sarcasm or irony can invert the intended sentiment, making accurate classification difficult.</li>
</ul>
</section>
</section>
<section id="concepts-and-explanations---chapter-7" class="level2" data-number="14.7">
<h2 data-number="14.7" class="anchored" data-anchor-id="concepts-and-explanations---chapter-7"><span class="header-section-number">14.7</span> Concepts and Explanations - Chapter 7</h2>
<section id="topic-modeling" class="level3" data-number="14.7.1">
<h3 data-number="14.7.1" class="anchored" data-anchor-id="topic-modeling"><span class="header-section-number">14.7.1</span> Topic Modeling</h3>
<ul>
<li>Topic modeling is an unsupervised machine learning technique for discovering abstract topics in a collection of documents.</li>
<li>It helps organize, summarize, and search large text corpora by uncovering hidden thematic structures.</li>
</ul>
</section>
<section id="latent-dirichlet-allocation-lda" class="level3" data-number="14.7.2">
<h3 data-number="14.7.2" class="anchored" data-anchor-id="latent-dirichlet-allocation-lda"><span class="header-section-number">14.7.2</span> Latent Dirichlet Allocation (LDA)</h3>
<ul>
<li>LDA is a generative probabilistic model where each document is a mixture of topics, and each topic is a distribution over words.</li>
<li>The model infers hidden topics by assuming documents are generated by mixing topics, and topics are generated by mixing words.</li>
</ul>
</section>
<section id="lda-intuition-and-components" class="level3" data-number="14.7.3">
<h3 data-number="14.7.3" class="anchored" data-anchor-id="lda-intuition-and-components"><span class="header-section-number">14.7.3</span> LDA Intuition and Components</h3>
<ul>
<li>Each document has a topic distribution, and each topic has a word distribution.</li>
<li>The model assigns each word in a document to a topic, aiming to uncover the latent structure that best explains the observed words.</li>
</ul>
</section>
<section id="lda-parameters-and-trade-offs" class="level3" data-number="14.7.4">
<h3 data-number="14.7.4" class="anchored" data-anchor-id="lda-parameters-and-trade-offs"><span class="header-section-number">14.7.4</span> LDA Parameters and Trade-offs</h3>
<ul>
<li>Key parameters: document density (<code>α</code>), topic word density (<code>β</code>), and number of topics (<code>K</code>).</li>
<li>LDA balances document sparsity (few topics per document) and topic sparsity (few words per topic).</li>
</ul>
</section>
<section id="parameter-estimation-in-lda" class="level3" data-number="14.7.5">
<h3 data-number="14.7.5" class="anchored" data-anchor-id="parameter-estimation-in-lda"><span class="header-section-number">14.7.5</span> Parameter Estimation in LDA</h3>
<ul>
<li>LDA uses Bayes’ Theorem to estimate hidden variables, but exact inference is intractable.</li>
<li>Common estimation methods include Expectation Maximization, Variational Inference, and Gibbs sampling (a Markov Chain Monte Carlo method).</li>
</ul>
</section>
<section id="gibbs-sampling-in-lda" class="level3" data-number="14.7.6">
<h3 data-number="14.7.6" class="anchored" data-anchor-id="gibbs-sampling-in-lda"><span class="header-section-number">14.7.6</span> Gibbs Sampling in LDA</h3>
<ul>
<li>Gibbs sampling iteratively updates topic assignments for each word, approximating the true topic structure.</li>
<li>It encourages sparsity by favoring few topics per document and few words per topic.</li>
</ul>
</section>
<section id="practical-considerations-for-lda" class="level3" data-number="14.7.7">
<h3 data-number="14.7.7" class="anchored" data-anchor-id="practical-considerations-for-lda"><span class="header-section-number">14.7.7</span> Practical Considerations for LDA</h3>
<ul>
<li>LDA requires specifying the number of topics and works best with well-preprocessed, longer documents.</li>
<li>It assumes a bag-of-words model, ignoring word order and semantic context.</li>
</ul>
</section>
<section id="visualizing-and-using-lda-results" class="level3" data-number="14.7.8">
<h3 data-number="14.7.8" class="anchored" data-anchor-id="visualizing-and-using-lda-results"><span class="header-section-number">14.7.8</span> Visualizing and Using LDA Results</h3>
<ul>
<li>LDA outputs document-topic and word-topic distributions, which can be used for document similarity, clustering, and visualization.</li>
</ul>
</section>
<section id="top2vec" class="level3" data-number="14.7.9">
<h3 data-number="14.7.9" class="anchored" data-anchor-id="top2vec"><span class="header-section-number">14.7.9</span> Top2Vec</h3>
<ul>
<li>Top2Vec is a modern topic modeling algorithm that embeds documents and words in a vector space, clusters them, and finds topics without needing to specify the number of topics.</li>
<li>It uses dimensionality reduction (UMAP) and clustering (HDBSCAN), requiring minimal preprocessing but is computationally intensive.</li>
</ul>
</section>
<section id="bertopic" class="level3" data-number="14.7.10">
<h3 data-number="14.7.10" class="anchored" data-anchor-id="bertopic"><span class="header-section-number">14.7.10</span> BERTopic</h3>
<ul>
<li>BERTopic builds on Top2Vec by using transformer-based embeddings (e.g., BERT) for better topic quality, especially in nuanced or short texts.</li>
<li>It uses class-based TF-IDF for topic extraction and is resource-intensive.</li>
</ul>
</section>
<section id="k-means-clustering-with-word2vec" class="level3" data-number="14.7.11">
<h3 data-number="14.7.11" class="anchored" data-anchor-id="k-means-clustering-with-word2vec"><span class="header-section-number">14.7.11</span> K-means Clustering with Word2Vec</h3>
<ul>
<li>Documents are embedded using Word2Vec and clustered with k-means, making it suitable for short texts.</li>
<li>This approach is simple and fast but requires specifying the number of clusters.</li>
</ul>
</section>
<section id="the-revolution-in-nlp-transformers-and-llms" class="level3" data-number="14.7.12">
<h3 data-number="14.7.12" class="anchored" data-anchor-id="the-revolution-in-nlp-transformers-and-llms"><span class="header-section-number">14.7.12</span> The Revolution in NLP: Transformers and LLMs</h3>
<ul>
<li>The transformer model has revolutionized NLP, enabling large language models (LLMs) trained on massive datasets with objectives like masked language modeling.</li>
<li>LLMs are further improved with Reinforcement Learning from Human Feedback (RLHF), aligning models with human preferences.</li>
</ul>
</section>
<section id="language-is-hard-1" class="level3" data-number="14.7.13">
<h3 data-number="14.7.13" class="anchored" data-anchor-id="language-is-hard-1"><span class="header-section-number">14.7.13</span> Language is Hard</h3>
<ul>
<li>Language is challenging for AI due to infinite possibilities, ambiguity (lexical and structural), and context dependence.</li>
<li>LLMs have made significant progress but true human-like understanding remains debated.</li>
</ul>
</section>
<section id="brief-history-turing-test-and-agi-benchmarks" class="level3" data-number="14.7.14">
<h3 data-number="14.7.14" class="anchored" data-anchor-id="brief-history-turing-test-and-agi-benchmarks"><span class="header-section-number">14.7.14</span> Brief History: Turing Test and AGI Benchmarks</h3>
<ul>
<li>The Turing Test evaluates if a machine’s behavior is indistinguishable from a human.</li>
<li>AGI benchmarks include tasks like passing university exams, performing jobs, assembling furniture, and adapting to new environments.</li>
</ul>
</section>
<section id="nlp-the-basic-approach-1" class="level3" data-number="14.7.15">
<h3 data-number="14.7.15" class="anchored" data-anchor-id="nlp-the-basic-approach-1"><span class="header-section-number">14.7.15</span> NLP: The Basic Approach</h3>
<ul>
<li>Text data is converted into structured representations (e.g., bag-of-words, TF-IDF) for machine learning.</li>
<li>Supervised ML enables tasks like spam detection and sentiment analysis, but bag-of-words ignores word order and context.</li>
</ul>
</section>
<section id="language-modeling-with-n-grams-1" class="level3" data-number="14.7.16">
<h3 data-number="14.7.16" class="anchored" data-anchor-id="language-modeling-with-n-grams-1"><span class="header-section-number">14.7.16</span> Language Modeling with N-grams</h3>
<ul>
<li>N-gram models assign probabilities to word sequences using the Markov assumption, capturing local word order but struggling with long-range dependencies.</li>
</ul>
</section>
<section id="training-an-llm-1" class="level3" data-number="14.7.17">
<h3 data-number="14.7.17" class="anchored" data-anchor-id="training-an-llm-1"><span class="header-section-number">14.7.17</span> Training an LLM</h3>
<ul>
<li>LLMs are trained to predict the next word in a sequence using neural networks and the softmax function.</li>
<li>Training involves computing loss based on prediction accuracy and updating model weights iteratively.</li>
</ul>
</section>
<section id="probing-gpt-and-llm-capabilities" class="level3" data-number="14.7.18">
<h3 data-number="14.7.18" class="anchored" data-anchor-id="probing-gpt-and-llm-capabilities"><span class="header-section-number">14.7.18</span> Probing GPT and LLM Capabilities</h3>
<ul>
<li>LLMs generate coherent, grammatical text and demonstrate understanding of linguistic structure, challenging previous assumptions about language learning.</li>
</ul>
</section>
<section id="applications-of-llms-1" class="level3" data-number="14.7.19">
<h3 data-number="14.7.19" class="anchored" data-anchor-id="applications-of-llms-1"><span class="header-section-number">14.7.19</span> Applications of LLMs</h3>
<ul>
<li>LLMs are used in customer support, research portals, automated help desks, and accessibility tools, with growing societal impact.</li>
</ul>
</section>
<section id="takeaways-1" class="level3" data-number="14.7.20">
<h3 data-number="14.7.20" class="anchored" data-anchor-id="takeaways-1"><span class="header-section-number">14.7.20</span> Takeaways</h3>
<ul>
<li>Language is complex and ambiguous, but LLMs are rapidly advancing toward human-level language ability.</li>
<li>The field is evolving quickly, with exciting research and significant real-world applications.</li>
</ul>
</section>
</section>
<section id="concepts-and-explanations---chapter-8" class="level2" data-number="14.8">
<h2 data-number="14.8" class="anchored" data-anchor-id="concepts-and-explanations---chapter-8"><span class="header-section-number">14.8</span> Concepts and Explanations - Chapter 8</h2>
<section id="word-meanings" class="level3" data-number="14.8.1">
<h3 data-number="14.8.1" class="anchored" data-anchor-id="word-meanings"><span class="header-section-number">14.8.1</span> Word Meanings</h3>
<ul>
<li>Understanding word meanings is central to NLP, but meanings are often complex and context-dependent.</li>
<li>Words can have multiple unrelated meanings (<strong>homonymy</strong>) or multiple related senses (<strong>polysemy</strong>), and the mapping between words and concepts is many-to-many.</li>
</ul>
</section>
<section id="word-relations" class="level3" data-number="14.8.2">
<h3 data-number="14.8.2" class="anchored" data-anchor-id="word-relations"><span class="header-section-number">14.8.2</span> Word Relations</h3>
<ul>
<li><strong>Synonymy</strong>: Words with similar meanings (e.g., “big” and “large”).</li>
<li><strong>Antonymy</strong>: Words with opposite meanings (e.g., “hot” and “cold”).</li>
<li><strong>Similarity and Relatedness</strong>: Words can be similar (e.g., “cup” and “mug”) or just related (e.g., “doctor” and “hospital”).</li>
<li><strong>Connotation</strong>: Words carry emotional or cultural associations beyond their literal meaning (e.g., “childish” vs.&nbsp;“youthful”).</li>
</ul>
<hr>
</section>
<section id="vector-semantics" class="level3" data-number="14.8.3">
<h3 data-number="14.8.3" class="anchored" data-anchor-id="vector-semantics"><span class="header-section-number">14.8.3</span> Vector Semantics</h3>
<ul>
<li>Vector semantics represents words and documents as points (vectors) in a multidimensional space, allowing mathematical comparison of meanings.</li>
<li>The distributional hypothesis states that words appearing in similar contexts tend to have similar meanings (“You shall know a word by the company it keeps”).</li>
<li>Words or documents are represented as vectors, often using term-document or word-context matrices.</li>
</ul>
<section id="comparing-word-vectors" class="level4" data-number="14.8.3.1">
<h4 data-number="14.8.3.1" class="anchored" data-anchor-id="comparing-word-vectors"><span class="header-section-number">14.8.3.1</span> Comparing Word Vectors</h4>
<ul>
<li><strong>Cosine Similarity</strong>: Measures the angle between two vectors, indicating how similar their directions are.</li>
<li><strong>Euclidean Distance</strong>: Measures the straight-line distance between vectors, reflecting overall difference.</li>
<li><strong>Dot Product</strong>: Quantifies similarity based on both direction and magnitude of vectors.</li>
<li>These metrics help quantify semantic similarity or relatedness between words or documents.</li>
</ul>
<hr>
</section>
</section>
<section id="word2vec" class="level3" data-number="14.8.4">
<h3 data-number="14.8.4" class="anchored" data-anchor-id="word2vec"><span class="header-section-number">14.8.4</span> Word2Vec</h3>
<section id="learning-the-embeddings" class="level4" data-number="14.8.4.1">
<h4 data-number="14.8.4.1" class="anchored" data-anchor-id="learning-the-embeddings"><span class="header-section-number">14.8.4.1</span> Learning the Embeddings</h4>
<ul>
<li>Word2Vec learns dense vector representations (embeddings) for words by training a classifier to distinguish real context pairs from random pairs.</li>
<li>The model uses self-supervision: it learns from the natural co-occurrence of words in text, without human annotation.</li>
</ul>
</section>
<section id="how-word2vec-learns-embeddings" class="level4" data-number="14.8.4.2">
<h4 data-number="14.8.4.2" class="anchored" data-anchor-id="how-word2vec-learns-embeddings"><span class="header-section-number">14.8.4.2</span> How Word2Vec Learns Embeddings</h4>
<ul>
<li>For each word, the model pairs it with nearby context words (positive examples) and with random words (negative examples).</li>
<li>Logistic regression is used to train the model to distinguish positive from negative pairs, updating word vectors to maximize correct classification.</li>
<li>After training, the classifier is discarded and the learned vectors are used as word embeddings.</li>
</ul>
</section>
<section id="analogy-reasoning-with-embeddings" class="level4" data-number="14.8.4.3">
<h4 data-number="14.8.4.3" class="anchored" data-anchor-id="analogy-reasoning-with-embeddings"><span class="header-section-number">14.8.4.3</span> Analogy Reasoning with Embeddings</h4>
<ul>
<li>Word2Vec embeddings capture abstract relationships, enabling analogy tasks (e.g., “man” is to “king” as “woman” is to “queen”).</li>
<li>Vector arithmetic on embeddings can reveal such relationships, demonstrating the power of learned representations.</li>
</ul>
<hr>
</section>
</section>
<section id="takeaways-2" class="level3" data-number="14.8.5">
<h3 data-number="14.8.5" class="anchored" data-anchor-id="takeaways-2"><span class="header-section-number">14.8.5</span> Takeaways</h3>
<ul>
<li><strong>Vector Semantics</strong>: Dense vector representations enable mathematical operations on word meanings and support a wide range of NLP tasks.</li>
<li><strong>Word2Vec</strong>: Provides powerful, static word embeddings that capture semantic and syntactic relationships, useful for translation, sentiment analysis, and more.</li>
<li>Practical steps include loading pre-trained embeddings, exploring similarities, solving analogies, and using embeddings for sentence classification or comparison with Bag-of-Words models.</li>
</ul>
</section>
</section>
<section id="concepts-and-explanations---chapter-9" class="level2" data-number="14.9">
<h2 data-number="14.9" class="anchored" data-anchor-id="concepts-and-explanations---chapter-9"><span class="header-section-number">14.9</span> Concepts and Explanations - Chapter 9</h2>
<section id="ambiguity-in-language" class="level3" data-number="14.9.1">
<h3 data-number="14.9.1" class="anchored" data-anchor-id="ambiguity-in-language"><span class="header-section-number">14.9.1</span> Ambiguity in Language</h3>
<ul>
<li>Ambiguity occurs when a sentence or word can be interpreted in more than one way, posing a challenge for NLP systems.</li>
<li>Types include <strong>lexical ambiguity</strong> (a word with multiple meanings) and <strong>structural ambiguity</strong> (a sentence with multiple possible parses).</li>
<li>Addressing ambiguity often requires models that capture sentence structure, such as parse trees or dependency graphs.</li>
</ul>
<hr>
</section>
<section id="simple-neural-networks-and-neural-language-models" class="level3" data-number="14.9.2">
<h3 data-number="14.9.2" class="anchored" data-anchor-id="simple-neural-networks-and-neural-language-models"><span class="header-section-number">14.9.2</span> Simple Neural Networks and Neural Language Models</h3>
<ul>
<li>Neural networks are composed of layers of units (neurons) that compute weighted sums of inputs, add a bias, and apply an activation function.</li>
<li>The basic structure includes an input layer, one or more hidden layers, and an output layer, enabling the network to learn complex mappings from input to output.</li>
</ul>
<hr>
</section>
<section id="activation-functions" class="level3" data-number="14.9.3">
<h3 data-number="14.9.3" class="anchored" data-anchor-id="activation-functions"><span class="header-section-number">14.9.3</span> Activation Functions</h3>
<ul>
<li><strong>Sigmoid</strong> maps values to (0, 1), useful for probabilities but can cause vanishing gradients.</li>
<li><strong>Tanh</strong> maps values to (-1, 1), is zero-centered, and often preferred over sigmoid.</li>
<li><strong>ReLU</strong> outputs the input if positive, otherwise zero; it is computationally efficient and helps mitigate vanishing gradients.</li>
<li>Activation functions introduce non-linearity, allowing networks to model complex relationships.</li>
</ul>
<hr>
</section>
<section id="multi-layer-neural-networks-and-xor" class="level3" data-number="14.9.4">
<h3 data-number="14.9.4" class="anchored" data-anchor-id="multi-layer-neural-networks-and-xor"><span class="header-section-number">14.9.4</span> Multi-Layer Neural Networks and XOR</h3>
<ul>
<li>A single-layer perceptron can only solve linearly separable problems (like AND/OR), but not XOR, which is not linearly separable.</li>
<li>Multi-layer networks with non-linear activation functions can learn complex functions like XOR by forming new representations in hidden layers.</li>
<li>This demonstrates the power of deep neural networks to model non-linear patterns.</li>
</ul>
<hr>
</section>
<section id="feedforward-neural-networks-multilayer-perceptrons" class="level3" data-number="14.9.5">
<h3 data-number="14.9.5" class="anchored" data-anchor-id="feedforward-neural-networks-multilayer-perceptrons"><span class="header-section-number">14.9.5</span> Feedforward Neural Networks (Multilayer Perceptrons)</h3>
<ul>
<li>Feedforward networks consist of multiple layers where information flows from input to output without cycles.</li>
<li>Each layer transforms its input through learned weights and activation functions, enabling the network to learn hierarchical representations.</li>
<li>Adding hidden layers allows the network to capture more complex patterns than simple linear models.</li>
</ul>
<hr>
</section>
<section id="applying-feedforward-networks-to-nlp-tasks" class="level3" data-number="14.9.6">
<h3 data-number="14.9.6" class="anchored" data-anchor-id="applying-feedforward-networks-to-nlp-tasks"><span class="header-section-number">14.9.6</span> Applying Feedforward Networks to NLP Tasks</h3>
<ul>
<li>Feedforward networks can be used for text classification, language modeling, and other NLP tasks by learning from word embeddings rather than hand-crafted features.</li>
<li>They can handle variable-length input using padding, truncation, or pooling methods (mean/max pooling).</li>
<li>For multi-class classification, a softmax output layer is used to produce class probabilities.</li>
</ul>
<hr>
</section>
<section id="training-neural-networks" class="level3" data-number="14.9.7">
<h3 data-number="14.9.7" class="anchored" data-anchor-id="training-neural-networks"><span class="header-section-number">14.9.7</span> Training Neural Networks</h3>
<ul>
<li>Training involves adjusting weights to minimize prediction error using <strong>backpropagation</strong> and <strong>gradient descent</strong>.</li>
<li>The process includes a forward pass (prediction), loss computation, backward pass (gradient calculation), and weight updates.</li>
<li>Training is iterative, and the learning rate controls the size of each update step; backpropagation efficiently computes gradients for all weights.</li>
</ul>
<hr>
</section>
</section>
<section id="concepts-and-explanations---chapter-10" class="level2" data-number="14.10">
<h2 data-number="14.10" class="anchored" data-anchor-id="concepts-and-explanations---chapter-10"><span class="header-section-number">14.10</span> Concepts and Explanations - Chapter 10</h2>
<section id="descartes-and-the-nature-of-thought" class="level3" data-number="14.10.1">
<h3 data-number="14.10.1" class="anchored" data-anchor-id="descartes-and-the-nature-of-thought"><span class="header-section-number">14.10.1</span> Descartes and the Nature of Thought</h3>
<ul>
<li>Philosophers have debated whether machines can truly “think” or only simulate understanding, raising questions about the nature of intelligence in AI.</li>
<li>This philosophical background frames the challenge of building AI that genuinely understands language.</li>
</ul>
</section>
<section id="the-turing-test" class="level3" data-number="14.10.2">
<h3 data-number="14.10.2" class="anchored" data-anchor-id="the-turing-test"><span class="header-section-number">14.10.2</span> The Turing Test</h3>
<ul>
<li>Proposed by Alan Turing, this test evaluates if a machine’s responses are indistinguishable from a human’s in conversation.</li>
<li>Passing the Turing Test is seen as a milestone for demonstrating machine intelligence.</li>
</ul>
</section>
<section id="singularity-and-agi" class="level3" data-number="14.10.3">
<h3 data-number="14.10.3" class="anchored" data-anchor-id="singularity-and-agi"><span class="header-section-number">14.10.3</span> Singularity and AGI</h3>
<ul>
<li>The Singularity is a hypothetical point where AI surpasses human intelligence, leading to rapid technological change.</li>
<li>AGI (Artificial General Intelligence) refers to machines capable of performing any intellectual task that humans can do.</li>
</ul>
<hr>
</section>
<section id="the-challenge-of-language" class="level3" data-number="14.10.4">
<h3 data-number="14.10.4" class="anchored" data-anchor-id="the-challenge-of-language"><span class="header-section-number">14.10.4</span> The Challenge of Language</h3>
<ul>
<li>Language allows infinite expression with finite words and rules, making it complex for machines to process.</li>
<li>Sentences can have long-range dependencies, requiring models to capture relationships between distant words.</li>
</ul>
<hr>
</section>
<section id="miracle-of-llms-3-key-insights" class="level3" data-number="14.10.5">
<h3 data-number="14.10.5" class="anchored" data-anchor-id="miracle-of-llms-3-key-insights"><span class="header-section-number">14.10.5</span> Miracle of LLMs – 3 Key Insights</h3>
<ol type="1">
<li><strong>Bag of Words / N-grams:</strong> Early models represented text as unordered words or short sequences, missing deeper context.</li>
<li><strong>Word Embeddings:</strong> Words are mapped to vectors capturing semantic relationships, enabling analogies and richer understanding.</li>
<li><strong>The Transformer:</strong> This architecture allows models to consider the entire context of a sentence, not just local word sequences.</li>
</ol>
<hr>
</section>
<section id="word-embeddings" class="level3" data-number="14.10.6">
<h3 data-number="14.10.6" class="anchored" data-anchor-id="word-embeddings"><span class="header-section-number">14.10.6</span> Word Embeddings</h3>
<ul>
<li>Word embeddings are high-dimensional vectors representing word meanings and relationships (e.g., gender, capitals, comparatives).</li>
<li>They enable models to perform analogy reasoning and capture semantic similarities.</li>
</ul>
<hr>
</section>
<section id="transformers" class="level3" data-number="14.10.7">
<h3 data-number="14.10.7" class="anchored" data-anchor-id="transformers"><span class="header-section-number">14.10.7</span> Transformers</h3>
<section id="the-problem-with-static-embeddings" class="level4" data-number="14.10.7.1">
<h4 data-number="14.10.7.1" class="anchored" data-anchor-id="the-problem-with-static-embeddings"><span class="header-section-number">14.10.7.1</span> The Problem with Static Embeddings</h4>
<ul>
<li>Static embeddings assign the same vector to a word regardless of context, failing to capture context-dependent meanings.</li>
</ul>
</section>
<section id="contextual-embeddings" class="level4" data-number="14.10.7.2">
<h4 data-number="14.10.7.2" class="anchored" data-anchor-id="contextual-embeddings"><span class="header-section-number">14.10.7.2</span> Contextual Embeddings</h4>
<ul>
<li>Contextual embeddings give each word a unique vector in each context, allowing the model to distinguish meanings based on surrounding words.</li>
</ul>
</section>
<section id="attention-mechanism" class="level4" data-number="14.10.7.3">
<h4 data-number="14.10.7.3" class="anchored" data-anchor-id="attention-mechanism"><span class="header-section-number">14.10.7.3</span> Attention Mechanism</h4>
<ul>
<li>Attention enables each word to “attend” to other words in a sentence, integrating relevant information to build context-aware representations.</li>
<li>This mechanism allows models to resolve ambiguities and capture complex dependencies.</li>
</ul>
</section>
<section id="position-embeddings" class="level4" data-number="14.10.7.4">
<h4 data-number="14.10.7.4" class="anchored" data-anchor-id="position-embeddings"><span class="header-section-number">14.10.7.4</span> Position Embeddings</h4>
<ul>
<li>Since transformers lack inherent word order, position embeddings are added to word vectors to encode sequence information.</li>
</ul>
</section>
<section id="output-logits-and-softmax" class="level4" data-number="14.10.7.5">
<h4 data-number="14.10.7.5" class="anchored" data-anchor-id="output-logits-and-softmax"><span class="header-section-number">14.10.7.5</span> Output: Logits and Softmax</h4>
<ul>
<li>The model outputs logits (raw scores) for each vocabulary word, which are converted to probabilities using the softmax function.</li>
</ul>
<hr>
</section>
</section>
<section id="bert-and-decoder-only-models" class="level3" data-number="14.10.8">
<h3 data-number="14.10.8" class="anchored" data-anchor-id="bert-and-decoder-only-models"><span class="header-section-number">14.10.8</span> BERT and Decoder-Only Models</h3>
<section id="bert-encoder-only" class="level4" data-number="14.10.8.1">
<h4 data-number="14.10.8.1" class="anchored" data-anchor-id="bert-encoder-only"><span class="header-section-number">14.10.8.1</span> BERT (Encoder-only)</h4>
<ul>
<li>BERT produces rich, context-aware embeddings for each token and is best for classification and embedding tasks.</li>
<li>It uses a special <code>[CLS]</code> token for sentence-level tasks and is fine-tuned for specific applications.</li>
</ul>
</section>
<section id="decoder-only-models-e.g.-gpt" class="level4" data-number="14.10.8.2">
<h4 data-number="14.10.8.2" class="anchored" data-anchor-id="decoder-only-models-e.g.-gpt"><span class="header-section-number">14.10.8.2</span> Decoder-Only Models (e.g., GPT)</h4>
<ul>
<li>These models generate text by predicting the next word in a sequence, making them suitable for text generation and chatbots.</li>
</ul>
<hr>
</section>
</section>
<section id="large-language-models-llms" class="level3" data-number="14.10.9">
<h3 data-number="14.10.9" class="anchored" data-anchor-id="large-language-models-llms"><span class="header-section-number">14.10.9</span> Large Language Models (LLMs)</h3>
<ul>
<li>LLMs are neural networks trained to predict the next word, enabling them to generate coherent and contextually relevant text.</li>
<li>They learn language, facts, and reasoning by training on massive datasets.</li>
</ul>
<hr>
</section>
<section id="decoding-and-sampling" class="level3" data-number="14.10.10">
<h3 data-number="14.10.10" class="anchored" data-anchor-id="decoding-and-sampling"><span class="header-section-number">14.10.10</span> Decoding and Sampling</h3>
<ul>
<li>Decoding is the process of choosing the next word during text generation, balancing quality and diversity.</li>
<li>Methods include random sampling, top-k, top-p (nucleus) sampling, and temperature adjustment, each affecting output creativity and coherence.</li>
</ul>
<hr>
</section>
<section id="pretraining-and-self-supervised-learning" class="level3" data-number="14.10.11">
<h3 data-number="14.10.11" class="anchored" data-anchor-id="pretraining-and-self-supervised-learning"><span class="header-section-number">14.10.11</span> Pretraining and Self-Supervised Learning</h3>
<ul>
<li>LLMs are pretrained on large text corpora using self-supervised learning, where the next word prediction serves as the training signal.</li>
<li>Cross-entropy loss is used to optimize the model, and pretraining imparts factual, linguistic, and commonsense knowledge.</li>
</ul>
<hr>
</section>
<section id="working-with-large-language-models" class="level3" data-number="14.10.12">
<h3 data-number="14.10.12" class="anchored" data-anchor-id="working-with-large-language-models"><span class="header-section-number">14.10.12</span> Working with Large Language Models</h3>
<section id="model-types" class="level4" data-number="14.10.12.1">
<h4 data-number="14.10.12.1" class="anchored" data-anchor-id="model-types"><span class="header-section-number">14.10.12.1</span> Model Types</h4>
<ul>
<li><strong>Base Model:</strong> Pretrained on large corpora.</li>
<li><strong>Instruct Model:</strong> Fine-tuned to follow instructions.</li>
<li><strong>Chat Model:</strong> Further tuned for dialogue.</li>
</ul>
</section>
<section id="key-settings" class="level4" data-number="14.10.12.2">
<h4 data-number="14.10.12.2" class="anchored" data-anchor-id="key-settings"><span class="header-section-number">14.10.12.2</span> Key Settings</h4>
<ul>
<li>Parameters like temperature, top-p, max length, and penalties control output randomness, diversity, and length.</li>
</ul>
</section>
<section id="prompt-structure-and-techniques" class="level4" data-number="14.10.12.3">
<h4 data-number="14.10.12.3" class="anchored" data-anchor-id="prompt-structure-and-techniques"><span class="header-section-number">14.10.12.3</span> Prompt Structure and Techniques</h4>
<ul>
<li>Effective prompts include instructions, context, input data, and output indicators.</li>
<li>Prompting techniques: zero-shot (no examples), few-shot (with examples), and chain-of-thought (step-by-step reasoning).</li>
</ul>
<hr>
</section>
</section>
<section id="takeaways-3" class="level3" data-number="14.10.13">
<h3 data-number="14.10.13" class="anchored" data-anchor-id="takeaways-3"><span class="header-section-number">14.10.13</span> Takeaways</h3>
<ul>
<li>Transformers use attention for context-aware word representations.</li>
<li>BERT is best for classification and embeddings; GPT is best for text generation.</li>
<li>Prompt engineering is crucial for effective use of LLMs.</li>
</ul>
</section>
</section>
<section id="concepts-and-explanations---chapter-11" class="level2" data-number="14.11">
<h2 data-number="14.11" class="anchored" data-anchor-id="concepts-and-explanations---chapter-11"><span class="header-section-number">14.11</span> Concepts and Explanations - Chapter 11</h2>
<section id="information-retrieval-ir" class="level3" data-number="14.11.1">
<h3 data-number="14.11.1" class="anchored" data-anchor-id="information-retrieval-ir"><span class="header-section-number">14.11.1</span> Information Retrieval (IR)</h3>
<ul>
<li>IR is the process of finding relevant documents from a large collection based on a user’s query.</li>
<li>Documents and queries are represented as vectors (e.g., bag-of-words, TF-IDF), and similarity (often cosine similarity) is used to rank results.</li>
<li>Key metrics include precision (fraction of retrieved documents that are relevant) and recall (fraction of relevant documents that are retrieved).</li>
</ul>
</section>
<section id="vector-space-model-tf-idf" class="level3" data-number="14.11.2">
<h3 data-number="14.11.2" class="anchored" data-anchor-id="vector-space-model-tf-idf"><span class="header-section-number">14.11.2</span> Vector Space Model &amp; TF-IDF</h3>
<ul>
<li>The vector space model represents documents and queries as vectors in a high-dimensional space, enabling similarity computation.</li>
<li>TF-IDF (Term Frequency–Inverse Document Frequency) weighs terms by their importance: frequent in a document but rare in the collection.</li>
<li>This approach helps distinguish important terms and improves retrieval effectiveness.</li>
</ul>
</section>
<section id="dense-retrieval-with-neural-models" class="level3" data-number="14.11.3">
<h3 data-number="14.11.3" class="anchored" data-anchor-id="dense-retrieval-with-neural-models"><span class="header-section-number">14.11.3</span> Dense Retrieval with Neural Models</h3>
<ul>
<li>Dense retrieval uses neural networks (e.g., BERT) to encode queries and documents into dense embeddings.</li>
<li>Similarity between embeddings is used to match queries and documents, allowing for semantic matching beyond exact word overlap.</li>
<li>This helps address vocabulary mismatch, where different words or phrases express the same meaning.</li>
</ul>
</section>
<section id="question-answering-datasets" class="level3" data-number="14.11.4">
<h3 data-number="14.11.4" class="anchored" data-anchor-id="question-answering-datasets"><span class="header-section-number">14.11.4</span> Question Answering Datasets</h3>
<ul>
<li>Datasets like SQuAD provide passages and questions, with answers as text spans within the passage.</li>
<li>SQuAD 2.0 introduces unanswerable questions to test model robustness.</li>
<li>These datasets are benchmarks for evaluating extractive QA systems.</li>
</ul>
</section>
<section id="extractive-question-answering" class="level3" data-number="14.11.5">
<h3 data-number="14.11.5" class="anchored" data-anchor-id="extractive-question-answering"><span class="header-section-number">14.11.5</span> Extractive Question Answering</h3>
<ul>
<li>Extractive QA involves selecting a span of text from a passage as the answer to a question.</li>
<li>Models (e.g., BERT) predict the start and end positions of the answer span within the passage.</li>
<li>This approach is widely used for reading comprehension tasks.</li>
</ul>
</section>
<section id="entity-linking-wikification" class="level3" data-number="14.11.6">
<h3 data-number="14.11.6" class="anchored" data-anchor-id="entity-linking-wikification"><span class="header-section-number">14.11.6</span> Entity Linking (Wikification)</h3>
<ul>
<li>Entity linking maps mentions in text to specific entities in a knowledge base (e.g., Wikipedia pages).</li>
<li>It involves mention detection, candidate generation, and disambiguation based on context.</li>
<li>This process grounds text to structured knowledge, improving retrieval and reasoning.</li>
</ul>
</section>
<section id="knowledge-based-question-answering-kbqa" class="level3" data-number="14.11.7">
<h3 data-number="14.11.7" class="anchored" data-anchor-id="knowledge-based-question-answering-kbqa"><span class="header-section-number">14.11.7</span> Knowledge-based Question Answering (KBQA)</h3>
<ul>
<li>KBQA answers questions by mapping them to queries over structured knowledge bases (e.g., DBpedia, Wikidata).</li>
<li>Approaches include graph-based QA (traversing entity-relation graphs) and semantic parsing (mapping questions to logical forms).</li>
<li>RDF triples (subject, predicate, object) are a common data structure for representing facts.</li>
</ul>
</section>
<section id="supervision-for-semantic-parsing" class="level3" data-number="14.11.8">
<h3 data-number="14.11.8" class="anchored" data-anchor-id="supervision-for-semantic-parsing"><span class="header-section-number">14.11.8</span> Supervision for Semantic Parsing</h3>
<ul>
<li>Fully supervised approaches use annotated logical forms for each question.</li>
<li>Weakly supervised approaches use only the answer (denotation), treating the logical form as a latent variable.</li>
<li>Supervision level affects the complexity and scalability of KBQA systems.</li>
</ul>
</section>
<section id="retrieve-and-generate-rag" class="level3" data-number="14.11.9">
<h3 data-number="14.11.9" class="anchored" data-anchor-id="retrieve-and-generate-rag"><span class="header-section-number">14.11.9</span> Retrieve-and-Generate (RAG)</h3>
<ul>
<li>RAG combines retrieval of relevant documents or passages with generative LLMs to produce answers.</li>
<li>The workflow involves retrieving relevant chunks, combining them with the question, and generating an answer using an LLM.</li>
<li>This approach enables up-to-date and contextually grounded answers.</li>
</ul>
</section>
<section id="evaluation-of-rag-systems" class="level3" data-number="14.11.10">
<h3 data-number="14.11.10" class="anchored" data-anchor-id="evaluation-of-rag-systems"><span class="header-section-number">14.11.10</span> Evaluation of RAG Systems</h3>
<ul>
<li>Retrieval evaluation checks if relevant chunks are retrieved (precision, recall).</li>
<li>Generation evaluation assesses if the answer is correct, relevant, and faithful to the retrieved context (metrics: BLEU, ROUGE, LLM-based judgments).</li>
<li>Faithfulness and answer/context relevance are key criteria; hallucinations should be avoided.</li>
</ul>
</section>
<section id="agents" class="level3" data-number="14.11.11">
<h3 data-number="14.11.11" class="anchored" data-anchor-id="agents"><span class="header-section-number">14.11.11</span> Agents</h3>
<ul>
<li>Agents are systems capable of forming intentions, making plans, and acting to achieve goals, often using internal representations.</li>
<li>In practice, LLM-based agents control application flow (e.g., LangChain agents) and can process text, audio, or visual input via modality fusion.</li>
<li>Agents extend LLMs’ capabilities to multi-step reasoning and action-taking.</li>
</ul>
</section>
<section id="formal-vs.-functional-abilities-of-llms" class="level3" data-number="14.11.12">
<h3 data-number="14.11.12" class="anchored" data-anchor-id="formal-vs.-functional-abilities-of-llms"><span class="header-section-number">14.11.12</span> Formal vs.&nbsp;Functional Abilities of LLMs</h3>
<ul>
<li>Formal linguistic competence refers to knowledge of grammar and language rules; functional competence is the ability to use language in real-world contexts.</li>
<li>LLMs excel at formal competence but may struggle with functional competence, such as real-world reasoning and understanding.</li>
<li>Evaluation should consider both types of competence and be aware of common fallacies (e.g., “good at language → good at thought”).</li>
</ul>
</section>
<section id="takeaways-4" class="level3" data-number="14.11.13">
<h3 data-number="14.11.13" class="anchored" data-anchor-id="takeaways-4"><span class="header-section-number">14.11.13</span> Takeaways</h3>
<ul>
<li>Question answering can be approached via IR, KBQA, or LLM-based methods (including RAG and agents).</li>
<li>Key datasets include SQuAD, SimpleQuestions, and RDF-based resources like DBpedia.</li>
<li>LLMs are powerful for formal linguistic tasks, but functional, real-world understanding remains a challenge.</li>
</ul>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./summary_concise.html" class="pagination-link" aria-label="Concepts and Explanations Short">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Concepts and Explanations Short</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
  </div>
</nav>
</div> <!-- /content -->




</body></html>