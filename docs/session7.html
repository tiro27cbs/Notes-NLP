<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.42">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Tim Roessling">

<title>8&nbsp; Chapter 7: Topic Modeling – NLP Notes</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./session8.html" rel="next">
<link href="./session6.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-2f5df379a58b258e96c21c0638c20c03.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-8da5b4427184b79ecddefad3d342027e.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./session7.html"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Chapter 7: Topic Modeling</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">NLP Notes</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">NLP Notes</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./session1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Chapter 1: Introduction to Natural Language Processing</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./session2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Chapter 2: NLP Libraries and Text Normalization</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./session3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Chapter 3: N-Gram Language Models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./session4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Chapter 4: Text Classification and Naive Bayes</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./session5.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Chapter 5: Logistic Regression and Text Representation</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./session6.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Chapter 6: Sentiment Analysis</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./session7.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Chapter 7: Topic Modeling</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./session8.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Chapter 8: Lexical Semantics and Vector Embeddings</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./session9.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Chapter 9: Simple Neural Networks</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./session10.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Chapter 10: The Transformer and Pre-trained Language Models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./session11.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Chapter 11: Question Answering and Information Retrieval</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#chapter-7-topic-modeling" id="toc-chapter-7-topic-modeling" class="nav-link active" data-scroll-target="#chapter-7-topic-modeling"><span class="header-section-number">9</span> Chapter 7: Topic Modeling</a>
  <ul class="collapse">
  <li><a href="#topic-modeling" id="toc-topic-modeling" class="nav-link" data-scroll-target="#topic-modeling"><span class="header-section-number">9.1</span> Topic Modeling</a>
  <ul class="collapse">
  <li><a href="#what-is-topic-modeling" id="toc-what-is-topic-modeling" class="nav-link" data-scroll-target="#what-is-topic-modeling"><span class="header-section-number">9.1.1</span> What is Topic Modeling?</a></li>
  </ul></li>
  <li><a href="#latent-dirichlet-allocation-lda" id="toc-latent-dirichlet-allocation-lda" class="nav-link" data-scroll-target="#latent-dirichlet-allocation-lda"><span class="header-section-number">9.2</span> Latent Dirichlet Allocation (LDA)</a>
  <ul class="collapse">
  <li><a href="#lda-intuition" id="toc-lda-intuition" class="nav-link" data-scroll-target="#lda-intuition"><span class="header-section-number">9.2.1</span> LDA Intuition</a></li>
  <li><a href="#how-lda-works" id="toc-how-lda-works" class="nav-link" data-scroll-target="#how-lda-works"><span class="header-section-number">9.2.2</span> How LDA Works</a></li>
  <li><a href="#key-components" id="toc-key-components" class="nav-link" data-scroll-target="#key-components"><span class="header-section-number">9.2.3</span> Key Components</a></li>
  <li><a href="#the-trade-off-document-vs.-topic-sparsity" id="toc-the-trade-off-document-vs.-topic-sparsity" class="nav-link" data-scroll-target="#the-trade-off-document-vs.-topic-sparsity"><span class="header-section-number">9.2.4</span> The Trade-off: Document vs.&nbsp;Topic Sparsity</a></li>
  <li><a href="#lda-parameters" id="toc-lda-parameters" class="nav-link" data-scroll-target="#lda-parameters"><span class="header-section-number">9.2.5</span> LDA Parameters</a></li>
  <li><a href="#parameter-estimation-in-lda" id="toc-parameter-estimation-in-lda" class="nav-link" data-scroll-target="#parameter-estimation-in-lda"><span class="header-section-number">9.2.6</span> Parameter Estimation in LDA</a></li>
  <li><a href="#gibbs-sampling-in-lda" id="toc-gibbs-sampling-in-lda" class="nav-link" data-scroll-target="#gibbs-sampling-in-lda"><span class="header-section-number">9.2.7</span> Gibbs Sampling in LDA</a></li>
  <li><a href="#practical-considerations-for-lda" id="toc-practical-considerations-for-lda" class="nav-link" data-scroll-target="#practical-considerations-for-lda"><span class="header-section-number">9.2.8</span> Practical Considerations for LDA</a></li>
  <li><a href="#visualizing-and-using-lda-results" id="toc-visualizing-and-using-lda-results" class="nav-link" data-scroll-target="#visualizing-and-using-lda-results"><span class="header-section-number">9.2.9</span> Visualizing and Using LDA Results</a></li>
  </ul></li>
  <li><a href="#top2vec-and-bertopic" id="toc-top2vec-and-bertopic" class="nav-link" data-scroll-target="#top2vec-and-bertopic"><span class="header-section-number">9.3</span> Top2Vec and BERTopic</a>
  <ul class="collapse">
  <li><a href="#top2vec-distributed-topic-representations" id="toc-top2vec-distributed-topic-representations" class="nav-link" data-scroll-target="#top2vec-distributed-topic-representations"><span class="header-section-number">9.3.1</span> Top2Vec: Distributed Topic Representations</a></li>
  <li><a href="#bertopic" id="toc-bertopic" class="nav-link" data-scroll-target="#bertopic"><span class="header-section-number">9.3.2</span> BERTopic</a></li>
  <li><a href="#k-means-clustering-with-word2vec" id="toc-k-means-clustering-with-word2vec" class="nav-link" data-scroll-target="#k-means-clustering-with-word2vec"><span class="header-section-number">9.3.3</span> K-means Clustering with Word2Vec</a></li>
  </ul></li>
  <li><a href="#the-revolution-in-nlp" id="toc-the-revolution-in-nlp" class="nav-link" data-scroll-target="#the-revolution-in-nlp"><span class="header-section-number">9.4</span> The Revolution in NLP</a></li>
  <li><a href="#language-is-hard" id="toc-language-is-hard" class="nav-link" data-scroll-target="#language-is-hard"><span class="header-section-number">9.5</span> Language is Hard</a>
  <ul class="collapse">
  <li><a href="#why-is-language-hard" id="toc-why-is-language-hard" class="nav-link" data-scroll-target="#why-is-language-hard"><span class="header-section-number">9.5.1</span> Why is Language Hard?</a></li>
  </ul></li>
  <li><a href="#brief-history" id="toc-brief-history" class="nav-link" data-scroll-target="#brief-history"><span class="header-section-number">9.6</span> Brief History</a></li>
  <li><a href="#nlp-the-basic-approach" id="toc-nlp-the-basic-approach" class="nav-link" data-scroll-target="#nlp-the-basic-approach"><span class="header-section-number">9.7</span> NLP: The Basic Approach</a>
  <ul class="collapse">
  <li><a href="#background" id="toc-background" class="nav-link" data-scroll-target="#background"><span class="header-section-number">9.7.1</span> Background</a></li>
  <li><a href="#language-modeling-with-n-grams" id="toc-language-modeling-with-n-grams" class="nav-link" data-scroll-target="#language-modeling-with-n-grams"><span class="header-section-number">9.7.2</span> Language Modeling with N-grams</a></li>
  <li><a href="#training-an-llm" id="toc-training-an-llm" class="nav-link" data-scroll-target="#training-an-llm"><span class="header-section-number">9.7.3</span> Training an LLM</a></li>
  <li><a href="#what-does-training-an-llm-look-like" id="toc-what-does-training-an-llm-look-like" class="nav-link" data-scroll-target="#what-does-training-an-llm-look-like"><span class="header-section-number">9.7.4</span> What Does Training an LLM Look Like?</a></li>
  <li><a href="#probing-gpt" id="toc-probing-gpt" class="nav-link" data-scroll-target="#probing-gpt"><span class="header-section-number">9.7.5</span> Probing GPT</a></li>
  </ul></li>
  <li><a href="#ai-where-are-we-heading" id="toc-ai-where-are-we-heading" class="nav-link" data-scroll-target="#ai-where-are-we-heading"><span class="header-section-number">9.8</span> AI: Where Are We Heading?</a></li>
  <li><a href="#applications-of-llms" id="toc-applications-of-llms" class="nav-link" data-scroll-target="#applications-of-llms"><span class="header-section-number">9.9</span> Applications of LLMs</a></li>
  <li><a href="#takeaways" id="toc-takeaways" class="nav-link" data-scroll-target="#takeaways"><span class="header-section-number">9.10</span> Takeaways</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Chapter 7: Topic Modeling</span></h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Tim Roessling </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">Invalid Date</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="chapter-7-topic-modeling" class="level1" data-number="9">
<h1 data-number="9"><span class="header-section-number">9</span> Chapter 7: Topic Modeling</h1>
<section id="topic-modeling" class="level2" data-number="9.1">
<h2 data-number="9.1" class="anchored" data-anchor-id="topic-modeling"><span class="header-section-number">9.1</span> Topic Modeling</h2>
<p>As the volume of information grows, it becomes increasingly difficult to locate relevant content. Topic modeling is a technique used to uncover the hidden thematic structure in a collection of documents.</p>
<p><strong>When might you need to find hidden topics in text?</strong> - Organizing large collections of documents - Summarizing unstructured text data - Improving information retrieval and search - Feature selection for downstream machine learning tasks</p>
<section id="what-is-topic-modeling" class="level3" data-number="9.1.1">
<h3 data-number="9.1.1" class="anchored" data-anchor-id="what-is-topic-modeling"><span class="header-section-number">9.1.1</span> What is Topic Modeling?</h3>
<ul>
<li>Topic modeling refers to a set of unsupervised machine learning techniques (such as clustering) for discovering the abstract “topics” that occur in a collection of documents.</li>
<li>It provides methods for automatically organizing, understanding, searching, and summarizing large electronic archives.</li>
<li>Common applications include document clustering, organizing large blocks of textual data, information retrieval from unstructured text, and feature selection.</li>
</ul>
<hr>
</section>
</section>
<section id="latent-dirichlet-allocation-lda" class="level2" data-number="9.2">
<h2 data-number="9.2" class="anchored" data-anchor-id="latent-dirichlet-allocation-lda"><span class="header-section-number">9.2</span> Latent Dirichlet Allocation (LDA)</h2>
<p>Latent Dirichlet Allocation (LDA) is a foundational topic modeling technique. Let’s break down the name:</p>
<ul>
<li><strong>Latent:</strong> Hidden or underlying (the topics are not directly observed).</li>
<li><strong>Dirichlet:</strong> Refers to the Dirichlet distribution, which models the distribution of topics in documents and words in topics.</li>
<li><strong>Allocation:</strong> Assigning topics to words in documents.</li>
</ul>
<hr>
<section id="lda-intuition" class="level3" data-number="9.2.1">
<h3 data-number="9.2.1" class="anchored" data-anchor-id="lda-intuition"><span class="header-section-number">9.2.1</span> LDA Intuition</h3>
<p>LDA is a <strong>generative model</strong>: it assumes that documents are created by mixing topics, and each topic is a collection of words.</p>
<ul>
<li>Each <strong>topic</strong> is a distribution over words.</li>
<li>Each <strong>document</strong> is a mixture of topics.</li>
<li>Each <strong>word</strong> in a document is drawn from one of the document’s topics.</li>
</ul>
<p><strong>In short:</strong><br>
Documents are treated as bags of words. Each document is generated by a mixture of topics, and each topic is a mixture of words. The goal of LDA is to uncover these hidden topics from the observed words.</p>
<hr>
</section>
<section id="how-lda-works" class="level3" data-number="9.2.2">
<h3 data-number="9.2.2" class="anchored" data-anchor-id="how-lda-works"><span class="header-section-number">9.2.2</span> How LDA Works</h3>
<ol type="1">
<li>For each document, choose a distribution over topics.</li>
<li>For each word in the document:
<ul>
<li>Pick a topic from the document’s topic distribution.</li>
<li>Pick a word from the topic’s word distribution.</li>
</ul></li>
</ol>
<p>LDA tries to reverse-engineer this process: given the words, it infers the topics.</p>
<hr>
</section>
<section id="key-components" class="level3" data-number="9.2.3">
<h3 data-number="9.2.3" class="anchored" data-anchor-id="key-components"><span class="header-section-number">9.2.3</span> Key Components</h3>
<ul>
<li><strong>Word distribution per topic</strong> (<code>β_k</code>): Which words are likely in each topic.</li>
<li><strong>Topic distribution per document</strong> (<code>θ_d</code>): Which topics are likely in each document.</li>
<li><strong>Topic assignment for each word</strong> (<code>z_{d,n}</code>): Which topic generated each word.</li>
</ul>
<div id="fig-tree" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-tree-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/chapter7_1.png" class="img-fluid figure-img" style="width:80.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-tree-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9.1: LDA as a graphical model
</figcaption>
</figure>
</div>
<hr>
</section>
<section id="the-trade-off-document-vs.-topic-sparsity" class="level3" data-number="9.2.4">
<h3 data-number="9.2.4" class="anchored" data-anchor-id="the-trade-off-document-vs.-topic-sparsity"><span class="header-section-number">9.2.4</span> The Trade-off: Document vs.&nbsp;Topic Sparsity</h3>
<p>LDA balances two goals:</p>
<ul>
<li><strong>Document sparsity:</strong> Each document should use as few topics as possible.</li>
<li><strong>Topic sparsity:</strong> Each topic should use as few words as possible.</li>
</ul>
<p>These are in tension:<br>
- If all words in a document come from one topic, that topic must cover many words (less topic sparsity). - If each topic uses only a few words, documents must use more topics (less document sparsity).</p>
<p><strong>LDA finds a balance, resulting in topics that best explain the documents.</strong></p>
<hr>
</section>
<section id="lda-parameters" class="level3" data-number="9.2.5">
<h3 data-number="9.2.5" class="anchored" data-anchor-id="lda-parameters"><span class="header-section-number">9.2.5</span> LDA Parameters</h3>
<ul>
<li><strong>Document density factor (<code>α</code>):</strong> Controls how many topics are expected per document.</li>
<li><strong>Topic word density factor (<code>β</code>):</strong> Controls how many words are expected per topic.</li>
<li><strong>Number of topics (<code>K</code>):</strong> How many topics to extract.</li>
</ul>
<hr>
</section>
<section id="parameter-estimation-in-lda" class="level3" data-number="9.2.6">
<h3 data-number="9.2.6" class="anchored" data-anchor-id="parameter-estimation-in-lda"><span class="header-section-number">9.2.6</span> Parameter Estimation in LDA</h3>
<p>LDA uses <strong>Bayes’ Theorem</strong> to estimate the hidden variables (topics):</p>
<p>[ P( ) = ]</p>
<ul>
<li><strong>Prior:</strong> What we believe before seeing the data.</li>
<li><strong>Posterior:</strong> What we believe after seeing the data.</li>
</ul>
<p><strong>Key challenge:</strong><br>
Computing the exact posterior is intractable.</p>
<p><strong>Common solutions:</strong> - <strong>Direct methods:</strong> Expectation Maximization, Variational Inference, Expectation Propagation. - <strong>Indirect methods:</strong> Estimate the posterior using sampling (e.g., Gibbs sampling, a type of Markov Chain Monte Carlo).</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Python Example: LDA with scikit-learn and gensim</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.feature_extraction.text <span class="im">import</span> CountVectorizer</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.decomposition <span class="im">import</span> LatentDirichletAllocation</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> fetch_20newsgroups</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> gensim</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> gensim <span class="im">import</span> corpora</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pyLDAvis.gensim_models</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pyLDAvis</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Example 1: LDA with scikit-learn</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Load sample data</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>newsgroups <span class="op">=</span> fetch_20newsgroups(subset<span class="op">=</span><span class="st">'train'</span>, </span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>                               categories<span class="op">=</span>[<span class="st">'alt.atheism'</span>, <span class="st">'sci.space'</span>, <span class="st">'rec.sport.hockey'</span>],</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>                               remove<span class="op">=</span>(<span class="st">'headers'</span>, <span class="st">'footers'</span>, <span class="st">'quotes'</span>))</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Preprocess and vectorize</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>vectorizer <span class="op">=</span> CountVectorizer(max_features<span class="op">=</span><span class="dv">1000</span>, stop_words<span class="op">=</span><span class="st">'english'</span>, </span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>                           min_df<span class="op">=</span><span class="dv">2</span>, max_df<span class="op">=</span><span class="fl">0.95</span>)</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>doc_term_matrix <span class="op">=</span> vectorizer.fit_transform(newsgroups.data)</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit LDA model</span></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>lda <span class="op">=</span> LatentDirichletAllocation(n_components<span class="op">=</span><span class="dv">3</span>, random_state<span class="op">=</span><span class="dv">42</span>, </span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>                              max_iter<span class="op">=</span><span class="dv">10</span>, learning_method<span class="op">=</span><span class="st">'online'</span>)</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>lda.fit(doc_term_matrix)</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a><span class="co"># Display top words per topic</span></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>feature_names <span class="op">=</span> vectorizer.get_feature_names_out()</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> topic_idx, topic <span class="kw">in</span> <span class="bu">enumerate</span>(lda.components_):</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>    top_words <span class="op">=</span> [feature_names[i] <span class="cf">for</span> i <span class="kw">in</span> topic.argsort()[<span class="op">-</span><span class="dv">10</span>:]]</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Topic </span><span class="sc">{</span>topic_idx<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span><span class="st">', '</span><span class="sc">.</span>join(top_words)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a><span class="co"># Example 2: LDA with gensim (more advanced)</span></span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a><span class="co"># Prepare documents</span></span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>texts <span class="op">=</span> [doc.split() <span class="cf">for</span> doc <span class="kw">in</span> newsgroups.data[:<span class="dv">100</span>]]  <span class="co"># Simple tokenization</span></span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>dictionary <span class="op">=</span> corpora.Dictionary(texts)</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>corpus <span class="op">=</span> [dictionary.doc2bow(text) <span class="cf">for</span> text <span class="kw">in</span> texts]</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a><span class="co"># Train LDA model</span></span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>lda_model <span class="op">=</span> gensim.models.LdaModel(corpus<span class="op">=</span>corpus, id2word<span class="op">=</span>dictionary, </span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>                                 num_topics<span class="op">=</span><span class="dv">3</span>, random_state<span class="op">=</span><span class="dv">42</span>,</span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a>                                 alpha<span class="op">=</span><span class="st">'auto'</span>, per_word_topics<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a><span class="co"># Print topics</span></span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> idx, topic <span class="kw">in</span> lda_model.print_topics(<span class="op">-</span><span class="dv">1</span>):</span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f'Topic </span><span class="sc">{</span>idx<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>topic<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a><span class="co"># Get document-topic probabilities</span></span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a>doc_topics <span class="op">=</span> lda_model[corpus[<span class="dv">0</span>]]</span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Document 0 topic distribution: </span><span class="sc">{</span>doc_topics[<span class="dv">0</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize (requires pyLDAvis)</span></span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a>vis <span class="op">=</span> pyLDAvis.gensim_models.prepare(lda_model, corpus, dictionary)</span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a>pyLDAvis.display(vis)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
</section>
<section id="gibbs-sampling-in-lda" class="level3" data-number="9.2.7">
<h3 data-number="9.2.7" class="anchored" data-anchor-id="gibbs-sampling-in-lda"><span class="header-section-number">9.2.7</span> Gibbs Sampling in LDA</h3>
<p>Gibbs sampling is a Markov Chain Monte Carlo (MCMC) method used to estimate the posterior distribution of the hidden variables in LDA, specifically the topic assignments for each word.</p>
<section id="what-does-gibbs-sampling-do-in-lda" class="level4" data-number="9.2.7.1">
<h4 data-number="9.2.7.1" class="anchored" data-anchor-id="what-does-gibbs-sampling-do-in-lda"><span class="header-section-number">9.2.7.1</span> What does Gibbs sampling do in LDA?</h4>
<ul>
<li>It generates samples from the joint probability distribution of the topic assignments for all words in all documents.</li>
<li>For each word, it samples a new topic assignment based on the current state of all other assignments.</li>
<li>This iterative process gradually approximates the true posterior distribution.</li>
</ul>
</section>
<section id="the-gibbs-sampling-algorithm" class="level4" data-number="9.2.7.2">
<h4 data-number="9.2.7.2" class="anchored" data-anchor-id="the-gibbs-sampling-algorithm"><span class="header-section-number">9.2.7.2</span> The Gibbs Sampling Algorithm</h4>
<ol type="1">
<li><p><strong>Initialization:</strong><br>
Randomly assign each word in each document to one of the K topics.</p></li>
<li><p><strong>Iterative Sampling:</strong><br>
For each word ( w_{d,n} ) in document ( d ):</p>
<ul>
<li>Remove the current topic assignment for ( w_{d,n} ).</li>
<li>Compute the probability of assigning topic ( k ) to ( w_{d,n} ) as: [ P(z_{d,n} = k z_{-}, w) ] where:
<ul>
<li>( n_{d,k}^{-n} ): Number of words in document ( d ) assigned to topic ( k ), excluding the current word.</li>
<li>( n_{k,w}^{-n} ): Number of times word ( w ) is assigned to topic ( k ), excluding the current word.</li>
<li>( ), ( ): Dirichlet priors.</li>
<li>( K ): Number of topics.</li>
<li>( V ): Vocabulary size.</li>
</ul></li>
<li>Sample a new topic for ( w_{d,n} ) from this distribution.</li>
<li>Update the counts accordingly.</li>
</ul></li>
<li><p><strong>Repeat:</strong><br>
Iterate over all words multiple times (epochs) until the topic assignments stabilize.</p></li>
</ol>
</section>
<section id="intuition" class="level4" data-number="9.2.7.3">
<h4 data-number="9.2.7.3" class="anchored" data-anchor-id="intuition"><span class="header-section-number">9.2.7.3</span> Intuition</h4>
<ul>
<li><strong>Document-topic counts</strong> encourage each document to use as few topics as possible (document sparsity).</li>
<li><strong>Topic-word counts</strong> encourage each topic to use as few words as possible (topic sparsity).</li>
<li>Over many iterations, the assignments converge to a good approximation of the true topic structure.</li>
</ul>
</section>
<section id="summary" class="level4" data-number="9.2.7.4">
<h4 data-number="9.2.7.4" class="anchored" data-anchor-id="summary"><span class="header-section-number">9.2.7.4</span> Summary</h4>
<p>Gibbs sampling provides an efficient way to estimate the hidden topic structure in LDA by iteratively updating topic assignments for each word based on the current state of the model.</p>
<hr>
</section>
</section>
<section id="practical-considerations-for-lda" class="level3" data-number="9.2.8">
<h3 data-number="9.2.8" class="anchored" data-anchor-id="practical-considerations-for-lda"><span class="header-section-number">9.2.8</span> Practical Considerations for LDA</h3>
<ul>
<li><strong>Similar to k-means clustering:</strong> LDA is unsupervised and requires the number of topics to be specified in advance.</li>
<li><strong>Bag-of-words assumption:</strong> Ignores word order and semantic context.</li>
<li><strong>Preprocessing is crucial:</strong> Good results require cleaning (stop words, stemming, lemmatization, etc.).</li>
<li><strong>Not ideal for short texts:</strong> LDA struggles with very short documents (e.g., tweets, headlines).</li>
</ul>
<hr>
</section>
<section id="visualizing-and-using-lda-results" class="level3" data-number="9.2.9">
<h3 data-number="9.2.9" class="anchored" data-anchor-id="visualizing-and-using-lda-results"><span class="header-section-number">9.2.9</span> Visualizing and Using LDA Results</h3>
<ul>
<li><strong>Document-topic probabilities:</strong> Each document can be represented by its topic probability vector.</li>
<li><strong>Word-topic assignments:</strong> Each word in a document can be colored or labeled by its assigned topic.</li>
<li><strong>Document similarity:</strong> Documents are similar if they have similar topic distributions (e.g., using Euclidean distance).</li>
</ul>
<hr>
</section>
</section>
<section id="top2vec-and-bertopic" class="level2" data-number="9.3">
<h2 data-number="9.3" class="anchored" data-anchor-id="top2vec-and-bertopic"><span class="header-section-number">9.3</span> Top2Vec and BERTopic</h2>
<section id="top2vec-distributed-topic-representations" class="level3" data-number="9.3.1">
<h3 data-number="9.3.1" class="anchored" data-anchor-id="top2vec-distributed-topic-representations"><span class="header-section-number">9.3.1</span> Top2Vec: Distributed Topic Representations</h3>
<p>Top2Vec is a modern topic modeling algorithm (arXiv 2020, <a href="https://github.com/ddangelov/Top2Vec">GitHub: ddangelov/Top2Vec</a>) that automatically discovers topics in text data <strong>without requiring you to specify the number of topics in advance</strong>. It also works <strong>without extensive preprocessing</strong> (e.g., stop word removal, stemming, or lemmatization).</p>
<section id="key-ideas" class="level4" data-number="9.3.1.1">
<h4 data-number="9.3.1.1" class="anchored" data-anchor-id="key-ideas"><span class="header-section-number">9.3.1.1</span> Key Ideas</h4>
<ul>
<li><strong>Vector Space Representation:</strong>
<ul>
<li>Each document and word is embedded into a high-dimensional vector space.</li>
<li>Terms are axes; documents and words are points or vectors.</li>
</ul></li>
<li><strong>Dimensionality Reduction:</strong>
<ul>
<li>UMAP is used to reduce the dimensionality of the embeddings, preserving semantic relationships (using cosine similarity).</li>
</ul></li>
<li><strong>Clustering:</strong>
<ul>
<li>HDBSCAN clusters the reduced vectors to identify topic groups.</li>
</ul></li>
<li><strong>Topic Vectors:</strong>
<ul>
<li>For each cluster, a topic vector is calculated.</li>
<li>The closest words to each topic vector define the topic.</li>
</ul></li>
</ul>
</section>
<section id="top2vec-workflow" class="level4" data-number="9.3.1.2">
<h4 data-number="9.3.1.2" class="anchored" data-anchor-id="top2vec-workflow"><span class="header-section-number">9.3.1.2</span> Top2Vec Workflow</h4>
<ol type="1">
<li><strong>Embed documents and words</strong> (e.g., using doc2vec or word2vec).</li>
<li><strong>Reduce dimensionality</strong> (UMAP).</li>
<li><strong>Cluster embeddings</strong> (HDBSCAN).</li>
<li><strong>Calculate topic vectors</strong> for each cluster.</li>
<li><strong>Find closest words</strong> to each topic vector to define topics.</li>
</ol>
<div id="fig-tree" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-tree-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/chapter7_2.png" class="img-fluid figure-img" style="width:80.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-tree-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9.2: Top2Vec
</figcaption>
</figure>
</div>
</section>
<section id="hyperparameters" class="level4" data-number="9.3.1.3">
<h4 data-number="9.3.1.3" class="anchored" data-anchor-id="hyperparameters"><span class="header-section-number">9.3.1.3</span> Hyperparameters</h4>
<ul>
<li><code>min_count</code>: Minimum word frequency to include in the model.</li>
<li><code>embedding_model</code>: Embedding method (e.g., ‘doc2vec’, ‘word2vec’).</li>
<li><code>umap_args</code>: UMAP settings for dimensionality reduction.</li>
<li><code>hdbscan_args</code>: HDBSCAN clustering settings.</li>
<li><code>vector_size</code>: Size of embedding vectors.</li>
</ul>
</section>
<section id="pros-and-cons" class="level4" data-number="9.3.1.4">
<h4 data-number="9.3.1.4" class="anchored" data-anchor-id="pros-and-cons"><span class="header-section-number">9.3.1.4</span> Pros and Cons</h4>
<p><strong>Pros:</strong> - Automatically determines the number of topics. - Preserves semantic meaning better than traditional models. - Minimal preprocessing required.</p>
<p><strong>Cons:</strong> - Computationally intensive—use small datasets for initial testing. - Many hyperparameters to tune; results can vary. - Requires experimentation to achieve optimal results.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Python Example: Top2Vec</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> top2vec <span class="im">import</span> Top2Vec</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> fetch_20newsgroups</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Load sample data</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>newsgroups <span class="op">=</span> fetch_20newsgroups(subset<span class="op">=</span><span class="st">'train'</span>, </span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>                               categories<span class="op">=</span>[<span class="st">'alt.atheism'</span>, <span class="st">'sci.space'</span>, <span class="st">'rec.sport.hockey'</span>],</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>                               remove<span class="op">=</span>(<span class="st">'headers'</span>, <span class="st">'footers'</span>, <span class="st">'quotes'</span>))</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Take a smaller subset for faster processing</span></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>documents <span class="op">=</span> newsgroups.data[:<span class="dv">200</span>]  <span class="co"># Use only 200 documents for demo</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Create Top2Vec model</span></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Note: This may take several minutes to run</span></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> Top2Vec(documents, </span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>                embedding_model<span class="op">=</span><span class="st">'universal-sentence-encoder'</span>,  <span class="co"># Easier to use, no C compiler needed</span></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>                speed<span class="op">=</span><span class="st">'fast'</span>,  <span class="co"># Options: 'fast', 'deep', 'learn'</span></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>                workers<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Get number of topics found</span></span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>num_topics <span class="op">=</span> model.get_num_topics()</span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Number of topics found: </span><span class="sc">{</span>num_topics<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Get topic words for each topic</span></span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>topic_words, word_scores, topic_nums <span class="op">=</span> model.get_topics()</span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Display topics</span></span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, topic_num <span class="kw">in</span> <span class="bu">enumerate</span>(topic_nums):</span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Topic </span><span class="sc">{</span>topic_num<span class="sc">}</span><span class="ss">:"</span>)</span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Words: </span><span class="sc">{</span><span class="st">', '</span><span class="sc">.</span>join(topic_words[i][:<span class="dv">10</span>])<span class="sc">}</span><span class="ss">"</span>)  <span class="co"># Show top 10 words</span></span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a><span class="co"># Get topic for a specific document</span></span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a>doc_topics, doc_scores, doc_ids <span class="op">=</span> model.search_documents_by_topic(topic_num<span class="op">=</span><span class="dv">0</span>, num_docs<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Sample documents for Topic 0:"</span>)</span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, doc_id <span class="kw">in</span> <span class="bu">enumerate</span>(doc_ids):</span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Doc </span><span class="sc">{</span>doc_id<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>documents[doc_id][:<span class="dv">100</span>]<span class="sc">}</span><span class="ss">..."</span>)</span>
<span id="cb2-39"><a href="#cb2-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-40"><a href="#cb2-40" aria-hidden="true" tabindex="-1"></a><span class="co"># Search for similar documents using keywords</span></span>
<span id="cb2-41"><a href="#cb2-41" aria-hidden="true" tabindex="-1"></a><span class="cf">try</span>:</span>
<span id="cb2-42"><a href="#cb2-42" aria-hidden="true" tabindex="-1"></a>    doc_topics, doc_scores, doc_ids <span class="op">=</span> model.search_documents_by_keywords(keywords<span class="op">=</span>[<span class="st">"space"</span>, <span class="st">"NASA"</span>], num_docs<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb2-43"><a href="#cb2-43" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Documents similar to 'space, NASA':"</span>)</span>
<span id="cb2-44"><a href="#cb2-44" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, doc_id <span class="kw">in</span> <span class="bu">enumerate</span>(doc_ids):</span>
<span id="cb2-45"><a href="#cb2-45" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Score: </span><span class="sc">{</span>doc_scores[i]<span class="sc">:.3f}</span><span class="ss"> - </span><span class="sc">{</span>documents[doc_id][:<span class="dv">100</span>]<span class="sc">}</span><span class="ss">..."</span>)</span>
<span id="cb2-46"><a href="#cb2-46" aria-hidden="true" tabindex="-1"></a><span class="cf">except</span>:</span>
<span id="cb2-47"><a href="#cb2-47" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"No documents found for these keywords"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<hr>
</section>
</section>
<section id="bertopic" class="level3" data-number="9.3.2">
<h3 data-number="9.3.2" class="anchored" data-anchor-id="bertopic"><span class="header-section-number">9.3.2</span> BERTopic</h3>
<p><strong>BERTopic</strong> is another modern topic modeling technique inspired by Top2Vec, but leverages transformer-based embeddings (e.g., BERT).</p>
<section id="key-features" class="level4" data-number="9.3.2.1">
<h4 data-number="9.3.2.1" class="anchored" data-anchor-id="key-features"><span class="header-section-number">9.3.2.1</span> Key Features</h4>
<ul>
<li><strong>No centroid calculation:</strong><br>
Each document in a cluster is treated as unique.</li>
<li><strong>Topic extraction:</strong><br>
Uses class-based TF-IDF to extract topic words from clusters.</li>
<li><strong>Transformer-based:</strong><br>
Utilizes contextual embeddings for improved topic quality, especially in nuanced or short texts.</li>
<li><strong>Computational cost:</strong><br>
More resource-intensive due to transformer models.</li>
</ul>
<hr>
</section>
</section>
<section id="k-means-clustering-with-word2vec" class="level3" data-number="9.3.3">
<h3 data-number="9.3.3" class="anchored" data-anchor-id="k-means-clustering-with-word2vec"><span class="header-section-number">9.3.3</span> K-means Clustering with Word2Vec</h3>
<ul>
<li><strong>Embed documents</strong> using Word2Vec.</li>
<li><strong>Cluster</strong> the embeddings using k-means.</li>
<li><strong>Best for short texts</strong> (e.g., tweets, headlines).</li>
</ul>
<hr>
<p><strong>Summary Table</strong></p>
<table class="caption-top table">
<colgroup>
<col style="width: 11%">
<col style="width: 15%">
<col style="width: 12%">
<col style="width: 18%">
<col style="width: 16%">
<col style="width: 25%">
</colgroup>
<thead>
<tr class="header">
<th>Method</th>
<th>Needs # Topics?</th>
<th>Preprocessing</th>
<th>Handles Short Texts</th>
<th>Computational Cost</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>LDA</td>
<td>Yes</td>
<td>Yes</td>
<td>No</td>
<td>Moderate</td>
<td>Classic, interpretable</td>
</tr>
<tr class="even">
<td>Top2Vec</td>
<td>No</td>
<td>No</td>
<td>Yes</td>
<td>High</td>
<td>Finds topics automatically</td>
</tr>
<tr class="odd">
<td>BERTopic</td>
<td>No</td>
<td>No</td>
<td>Yes</td>
<td>Very High</td>
<td>Transformer-based</td>
</tr>
<tr class="even">
<td>K-means+W2V</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Low/Moderate</td>
<td>Simple, fast</td>
</tr>
</tbody>
</table>
<hr>
<p><strong>Practical Tips:</strong> - Start with small datasets to experiment with Top2Vec or BERTopic. - Tune hyperparameters for your specific data and task. - Consider computational resources—transformer-based models can be slow. - Use visualizations to interpret and validate discovered topics.</p>
<hr>
</section>
</section>
<section id="the-revolution-in-nlp" class="level2" data-number="9.4">
<h2 data-number="9.4" class="anchored" data-anchor-id="the-revolution-in-nlp"><span class="header-section-number">9.4</span> The Revolution in NLP</h2>
<p>Natural Language Processing (NLP) has undergone a revolution in recent years, primarily due to the introduction of the <strong>transformer model</strong>. Transformers are trained on massive datasets using objectives like <em>masked language modeling</em> (predicting missing words in a sentence). Further improvements are achieved through <strong>Reinforcement Learning from Human Feedback (RLHF)</strong>, allowing models to better align with human preferences.</p>
<blockquote class="blockquote">
<p><strong>Example:</strong><br>
Given the sentence: “The cat sat on the ___,” a transformer model predicts the missing word, such as “mat”.</p>
</blockquote>
</section>
<section id="language-is-hard" class="level2" data-number="9.5">
<h2 data-number="9.5" class="anchored" data-anchor-id="language-is-hard"><span class="header-section-number">9.5</span> Language is Hard</h2>
<p>Despite impressive progress, language remains a challenging domain for AI.</p>
<section id="why-is-language-hard" class="level3" data-number="9.5.1">
<h3 data-number="9.5.1" class="anchored" data-anchor-id="why-is-language-hard"><span class="header-section-number">9.5.1</span> Why is Language Hard?</h3>
<ul>
<li><strong>Infinite Possibilities:</strong><br>
Most sentences you hear are unique—you’ve never heard them before and may never hear them again.</li>
<li><strong>Ambiguity:</strong>
<ul>
<li><em>Lexical Ambiguity:</em> Words can have multiple meanings.<br>
&gt; <em>Example:</em> “bank” (river bank vs.&nbsp;financial bank)<br>
</li>
<li><em>Structural Ambiguity:</em> Sentences can be interpreted in different ways.<br>
&gt; <em>Example:</em> “I saw the man with the telescope.” (Who has the telescope?)</li>
</ul></li>
</ul>
<p>Many thinkers have argued that true human-level language understanding may be impossible for machines. Current LLMs (Large Language Models) appear to process and reason with language at a high level, but is this really human-like understanding?</p>
<hr>
</section>
</section>
<section id="brief-history" class="level2" data-number="9.6">
<h2 data-number="9.6" class="anchored" data-anchor-id="brief-history"><span class="header-section-number">9.6</span> Brief History</h2>
<ul>
<li><strong>Descartes:</strong><br>
Argued that a machine could never truly imitate a human; there would always be a way to tell the difference.</li>
<li><strong>Turing Test:</strong><br>
Proposed by Alan Turing as a test of a machine’s ability to exhibit intelligent behavior indistinguishable from a human.
<ul>
<li>A human judge engages in a conversation with both a human and a machine.<br>
</li>
<li>If the judge cannot reliably tell which is which, the machine is said to have passed the test.</li>
</ul></li>
</ul>
</section>
<section id="nlp-the-basic-approach" class="level2" data-number="9.7">
<h2 data-number="9.7" class="anchored" data-anchor-id="nlp-the-basic-approach"><span class="header-section-number">9.7</span> NLP: The Basic Approach</h2>
<section id="background" class="level3" data-number="9.7.1">
<h3 data-number="9.7.1" class="anchored" data-anchor-id="background"><span class="header-section-number">9.7.1</span> Background</h3>
<ul>
<li><strong>Numerical Data:</strong> Numbers, measurements, etc.</li>
<li><strong>Categorical Data:</strong> Discrete categories or labels.</li>
<li><strong>Text Data:</strong> Unstructured, variable-length, and context-dependent (e.g., email content, headlines, political speeches).</li>
</ul>
<p>Text data is fundamentally different from structured data. Can we treat language as structured data for machine learning?</p>
<section id="making-language-into-structured-data" class="level4" data-number="9.7.1.1">
<h4 data-number="9.7.1.1" class="anchored" data-anchor-id="making-language-into-structured-data"><span class="header-section-number">9.7.1.1</span> Making Language into Structured Data</h4>
<ul>
<li><strong>Bag-of-Words Representation:</strong>
<ul>
<li>Assign a feature (column) for each word in the vocabulary.<br>
</li>
<li>For a given text, the value is 1 if the word occurs, 0 otherwise.<br>
</li>
<li>Alternative values: word counts or TF-IDF scores.<br>
</li>
<li>Most features are 0 for any given text (sparse representation).</li>
</ul></li>
</ul>
</section>
<section id="supervised-ml-for-text-processing" class="level4" data-number="9.7.1.2">
<h4 data-number="9.7.1.2" class="anchored" data-anchor-id="supervised-ml-for-text-processing"><span class="header-section-number">9.7.1.2</span> Supervised ML for Text Processing</h4>
<ul>
<li><strong>Labeled Text Data:</strong> Enables building classifiers for:
<ul>
<li>Spam Detection<br>
</li>
<li>Sentiment Analysis<br>
</li>
<li>Topic Detection<br>
</li>
<li>…and more<br>
</li>
</ul></li>
<li>Raises questions: Does this approach capture real understanding? How does it relate to the Turing Test?</li>
</ul>
</section>
<section id="bag-of-words-limitation" class="level4" data-number="9.7.1.3">
<h4 data-number="9.7.1.3" class="anchored" data-anchor-id="bag-of-words-limitation"><span class="header-section-number">9.7.1.3</span> Bag-of-Words Limitation</h4>
<ul>
<li>Ignores word order and context (“bag” of words).<br>
</li>
<li>Cannot distinguish between “dog bites man” and “man bites dog”.</li>
</ul>
<hr>
</section>
</section>
<section id="language-modeling-with-n-grams" class="level3" data-number="9.7.2">
<h3 data-number="9.7.2" class="anchored" data-anchor-id="language-modeling-with-n-grams"><span class="header-section-number">9.7.2</span> Language Modeling with N-grams</h3>
<ul>
<li><strong>Goal:</strong> Assign a probability to a sequence of words.</li>
<li><strong>Markov Assumption:</strong> The probability of a word depends only on the previous <em>n-1</em> words.</li>
</ul>
<section id="example-he-went-to-the-store" class="level4" data-number="9.7.2.1">
<h4 data-number="9.7.2.1" class="anchored" data-anchor-id="example-he-went-to-the-store"><span class="header-section-number">9.7.2.1</span> Example: “He went to the store”</h4>
<ul>
<li><strong>Unigrams (1-grams):</strong> He, went, to, the, store</li>
<li><strong>Bigrams (2-grams):</strong> He went, went to, to the, the store</li>
<li><strong>Trigrams (3-grams):</strong> He went to, went to the, to the store</li>
<li><strong>4-grams:</strong> He went to the, went to the store</li>
<li><strong>5-gram:</strong> He went to the store</li>
</ul>
</section>
<section id="n-gram-approximations" class="level4" data-number="9.7.2.2">
<h4 data-number="9.7.2.2" class="anchored" data-anchor-id="n-gram-approximations"><span class="header-section-number">9.7.2.2</span> N-gram Approximations</h4>
<ul>
<li><p><strong>Bigram Model:</strong><br>
( p() = p() p( ) p( ) p( ) p( ) )</p></li>
<li><p><strong>Trigram Model:</strong><br>
( p() = p() p( ) p( ) p( ) p( ) )</p></li>
<li><p><strong>Key Idea:</strong><br>
N-gram models capture some local word order, but struggle with long-range dependencies and rare phrases.</p></li>
</ul>
<hr>
</section>
</section>
<section id="training-an-llm" class="level3" data-number="9.7.3">
<h3 data-number="9.7.3" class="anchored" data-anchor-id="training-an-llm"><span class="header-section-number">9.7.3</span> Training an LLM</h3>
<blockquote class="blockquote">
<p>“A general language model (LM) should be able to compute the probability of (and also generate) any string.”<br>
<em>(Radford et al., 2019)</em></p>
</blockquote>
</section>
<section id="what-does-training-an-llm-look-like" class="level3" data-number="9.7.4">
<h3 data-number="9.7.4" class="anchored" data-anchor-id="what-does-training-an-llm-look-like"><span class="header-section-number">9.7.4</span> What Does Training an LLM Look Like?</h3>
<p>Consider how humans complete sentences:<br>
- *As Descartes said, “I think, therefore I <strong>.”*<br>
- </strong>For all intents and**<br>
- <em>I learned how to drive a ___</em></p>
<p>Or in dialogue:<br>
&gt; <strong>Monica:</strong> Okay, everybody relax. This is not even a ___<br>
&gt; <strong>Rachel:</strong> Oh God… well, it started about a half hour before the ___<br>
&gt; <strong>Ross:</strong> (squatting and reading the instructions) I’m supposed to attach a brackety ___</p>
<p>The core training objective for LLMs is <strong>next word prediction</strong>:<br>
Given a sequence of words, predict the most likely next word.</p>
<section id="neural-network-for-next-word-prediction" class="level4" data-number="9.7.4.1">
<h4 data-number="9.7.4.1" class="anchored" data-anchor-id="neural-network-for-next-word-prediction"><span class="header-section-number">9.7.4.1</span> Neural Network for Next Word Prediction</h4>
<ul>
<li>The model is a neural network that outputs a score for every word in the vocabulary.</li>
<li>These scores are converted into probabilities using the <strong>softmax</strong> function.</li>
<li>For example, with a vocabulary of 50,000 words, the output might look like: <code>[fish: 0.00002, help: 0.00002, ..., the: 0.00002, ..., aardvarks: 0.00002]</code></li>
</ul>
</section>
<section id="training-process" class="level4" data-number="9.7.4.2">
<h4 data-number="9.7.4.2" class="anchored" data-anchor-id="training-process"><span class="header-section-number">9.7.4.2</span> Training Process</h4>
<ol type="1">
<li><strong>Compute Loss:</strong>
<ul>
<li>The true next word is masked (hidden).<br>
</li>
<li>The model predicts probabilities for all words.<br>
</li>
<li>The loss function measures how well the model predicts the correct word (e.g., <em>Loss = 1 - prob(correct word)</em>).<br>
</li>
<li>If the model assigns a probability of 1 to the correct word, loss is 0 (best). If 0, loss is 1 (worst).</li>
</ul></li>
<li><strong>Update Weights:</strong>
<ul>
<li>The model adjusts its internal weights to increase the probability of the correct word.<br>
</li>
<li>This also slightly decreases the probability for all other words.<br>
</li>
<li>Each training example provides a small update—repeated millions or billions of times.</li>
</ul></li>
</ol>
</section>
<section id="example-weight-updates" class="level4" data-number="9.7.4.3">
<h4 data-number="9.7.4.3" class="anchored" data-anchor-id="example-weight-updates"><span class="header-section-number">9.7.4.3</span> Example: Weight Updates</h4>
<p>Suppose the correct next word is “fish”:<br>
- The model increases the weights leading to “fish”.<br>
- The probabilities for other words (e.g., “help”, “the”, “aardvarks”) are slightly reduced.</p>
<blockquote class="blockquote">
<p>Each example nudges the model to make the correct word more likely in context, and less likely for others.<br>
Over time, the model learns to predict words in a wide variety of contexts.</p>
</blockquote>
<hr>
<p><em>This is the fundamental process behind training large language models: predict the next word, compute the loss, update the weights, and repeat—at massive scale.</em></p>
</section>
</section>
<section id="probing-gpt" class="level3" data-number="9.7.5">
<h3 data-number="9.7.5" class="anchored" data-anchor-id="probing-gpt"><span class="header-section-number">9.7.5</span> Probing GPT</h3>
<p>Large Language Models (LLMs) today generate highly coherent, grammatical text that can be indistinguishable from human output. They demonstrate some understanding of hierarchical structure and abstract linguistic categories (Mahowald et al., 2024).</p>
<p>While these models are not perfect learners of abstract linguistic rules, neither are humans. LLMs are progressing toward acquiring formal linguistic competence and have already challenged claims about the impossibility of learning certain linguistic knowledge—such as hierarchical structure and abstract categories—from input alone (Mahowald et al., 2024).</p>
<hr>
</section>
</section>
<section id="ai-where-are-we-heading" class="level2" data-number="9.8">
<h2 data-number="9.8" class="anchored" data-anchor-id="ai-where-are-we-heading"><span class="header-section-number">9.8</span> AI: Where Are We Heading?</h2>
<p><strong>Artificial General Intelligence (AGI):</strong><br>
AGI is defined as AI that matches or surpasses human cognitive capabilities across a wide range of tasks.</p>
<p><strong>AGI Benchmarks:</strong></p>
<ul>
<li><p><strong>The Robot College Student Test (Goertzel):</strong><br>
A machine enrolls in a university, takes and passes the same classes as humans, and obtains a degree. LLMs can now pass university-level exams without attending classes.</p></li>
<li><p><strong>The Employment Test (Nilsson):</strong><br>
A machine performs an economically important job at least as well as humans. AI is already replacing humans in roles ranging from fast food to marketing.</p></li>
<li><p><strong>The Ikea Test (Marcus):</strong><br>
An AI views the parts and instructions of an Ikea flat-pack product, then controls a robot to assemble the furniture correctly.</p></li>
<li><p><strong>The Coffee Test (Wozniak):</strong><br>
A machine enters an average home and figures out how to make coffee: find the machine, coffee, water, mug, and brew coffee by pushing the right buttons. This remains unsolved.</p></li>
<li><p><strong>The Modern Turing Test (Suleyman):</strong><br>
An AI is given $100,000 and must turn it into $1 million.</p></li>
</ul>
<hr>
</section>
<section id="applications-of-llms" class="level2" data-number="9.9">
<h2 data-number="9.9" class="anchored" data-anchor-id="applications-of-llms"><span class="header-section-number">9.9</span> Applications of LLMs</h2>
<ul>
<li>AI interfaces for customer support and onboarding</li>
<li>Research portals with Retrieval-Augmented Generation (RAG)</li>
<li>Automated customer support (e.g., Zendesk)</li>
<li>Accessibility tools (e.g., BeMyAI)</li>
</ul>
<hr>
</section>
<section id="takeaways" class="level2" data-number="9.10">
<h2 data-number="9.10" class="anchored" data-anchor-id="takeaways"><span class="header-section-number">9.10</span> Takeaways</h2>
<ul>
<li><strong>Language is Hard:</strong>
<ul>
<li>Language is infinite and ambiguous (both lexically and structurally).</li>
</ul></li>
<li><strong>The Revolution in NLP:</strong>
<ul>
<li>LLMs now approach human-level language ability.</li>
</ul></li>
<li><strong>Exciting Research Directions:</strong>
<ul>
<li>Building applications with LLMs<br>
</li>
<li>Probing their abilities</li>
</ul></li>
<li><strong>Powerful AI is Coming:</strong>
<ul>
<li>The field is rapidly advancing, with significant societal impact on the horizon.</li>
</ul></li>
</ul>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./session6.html" class="pagination-link" aria-label="Chapter 6: Sentiment Analysis">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Chapter 6: Sentiment Analysis</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./session8.html" class="pagination-link" aria-label="Chapter 8: Lexical Semantics and Vector Embeddings">
        <span class="nav-page-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Chapter 8: Lexical Semantics and Vector Embeddings</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>