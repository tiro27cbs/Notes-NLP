[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "NLP Notes",
    "section": "",
    "text": "0.1 Lecture Outline\nThis document contains a collection of my notes on Natural Language Processing (NLP). It covers various topics, concepts, and techniques related to understanding and processing human language.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>NLP Notes</span>"
    ]
  },
  {
    "objectID": "index.html#lecture-outline",
    "href": "index.html#lecture-outline",
    "title": "NLP Notes",
    "section": "",
    "text": "Lecture 1 – Course introduction and introduction to Linguistics and the study of Language\n\nLecture 2 – Basic Text Processing: Regular expressions, Text Normalization, POS tagging, NER\n\nLecture 3 – Language Modeling with N-Grams\n\nLecture 4 – Naïve Bayes, Text Classification\n\nLecture 5 – Logistic Regression\n\nLecture 6 – Sentiment Analysis\n\nLecture 7 – Topic Modeling and Top2Vec\n\nLecture 8 – Vector Semantics and Embeddings\n\nLecture 9 – Neural Networks and Neural Language Models\n\nLecture 10 – The Transformer and Pre-trained language models\n\nLecture 11 – Question Answering and Dialog systems and Chatbots",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>NLP Notes</span>"
    ]
  },
  {
    "objectID": "session1.html",
    "href": "session1.html",
    "title": "2  Introduction to Natural Language Processing",
    "section": "",
    "text": "2.1 The Revolution in NLP\nNatural Language Processing (NLP) has undergone a revolution in recent years, primarily due to the introduction of the transformer model. Transformers are trained on massive datasets using objectives like masked language modeling (predicting missing words in a sentence). Further improvements are achieved through Reinforcement Learning from Human Feedback (RLHF), allowing models to better align with human preferences.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Natural Language Processing</span>"
    ]
  },
  {
    "objectID": "session1.html#the-revolution-in-nlp",
    "href": "session1.html#the-revolution-in-nlp",
    "title": "2  Introduction to Natural Language Processing",
    "section": "",
    "text": "Example:\nGiven the sentence: “The cat sat on the ___,” a transformer model predicts the missing word, such as “mat”.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Natural Language Processing</span>"
    ]
  },
  {
    "objectID": "session1.html#language-is-hard",
    "href": "session1.html#language-is-hard",
    "title": "2  Introduction to Natural Language Processing",
    "section": "2.2 Language is Hard",
    "text": "2.2 Language is Hard\nDespite impressive progress, language remains a challenging domain for AI.\n\n2.2.1 Why is Language Hard?\n\nInfinite Possibilities:\nMost sentences you hear are unique—you’ve never heard them before and may never hear them again.\nAmbiguity:\n\nLexical Ambiguity: Words can have multiple meanings.\n&gt; Example: “bank” (river bank vs. financial bank)\nStructural Ambiguity: Sentences can be interpreted in different ways.\n&gt; Example: “I saw the man with the telescope.” (Who has the telescope?)\n\n\nMany thinkers have argued that true human-level language understanding may be impossible for machines. Current LLMs (Large Language Models) appear to process and reason with language at a high level, but is this really human-like understanding?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Natural Language Processing</span>"
    ]
  },
  {
    "objectID": "session1.html#brief-history",
    "href": "session1.html#brief-history",
    "title": "2  Introduction to Natural Language Processing",
    "section": "2.3 Brief History",
    "text": "2.3 Brief History\n\nDescartes:\nArgued that a machine could never truly imitate a human; there would always be a way to tell the difference.\nTuring Test:\nProposed by Alan Turing as a test of a machine’s ability to exhibit intelligent behavior indistinguishable from a human.\n\nA human judge engages in a conversation with both a human and a machine.\nIf the judge cannot reliably tell which is which, the machine is said to have passed the test.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Natural Language Processing</span>"
    ]
  },
  {
    "objectID": "session1.html#nlp-the-basic-approach",
    "href": "session1.html#nlp-the-basic-approach",
    "title": "2  Introduction to Natural Language Processing",
    "section": "2.4 NLP: The Basic Approach",
    "text": "2.4 NLP: The Basic Approach\n\n2.4.1 Background\n\nNumerical Data: Numbers, measurements, etc.\nCategorical Data: Discrete categories or labels.\nText Data: Unstructured, variable-length, and context-dependent (e.g., email content, headlines, political speeches).\n\nText data is fundamentally different from structured data. Can we treat language as structured data for machine learning?\n\n2.4.1.1 Making Language into Structured Data\n\nBag-of-Words Representation:\n\nAssign a feature (column) for each word in the vocabulary.\nFor a given text, the value is 1 if the word occurs, 0 otherwise.\nAlternative values: word counts or TF-IDF scores.\nMost features are 0 for any given text (sparse representation).\n\n\n\n\n2.4.1.2 Supervised ML for Text Processing\n\nLabeled Text Data: Enables building classifiers for:\n\nSpam Detection\nSentiment Analysis\nTopic Detection\n…and more\n\nRaises questions: Does this approach capture real understanding? How does it relate to the Turing Test?\n\n\n\n2.4.1.3 Bag-of-Words Limitation\n\nIgnores word order and context (“bag” of words).\nCannot distinguish between “dog bites man” and “man bites dog”.\n\n\n\n\n\n2.4.2 Language Modeling with N-grams\n\nGoal: Assign a probability to a sequence of words.\nMarkov Assumption: The probability of a word depends only on the previous n-1 words.\n\n\n2.4.2.1 Example: “He went to the store”\n\nUnigrams (1-grams): He, went, to, the, store\nBigrams (2-grams): He went, went to, to the, the store\nTrigrams (3-grams): He went to, went to the, to the store\n4-grams: He went to the, went to the store\n5-gram: He went to the store\n\n\n\n2.4.2.2 N-gram Approximations\n\nBigram Model:\n\\[\n  p(\\text{He went to the store}) = p(\\text{He}) \\times p(\\text{went} \\mid \\text{He}) \\times p(\\text{to} \\mid \\text{went}) \\times p(\\text{the} \\mid \\text{to}) \\times p(\\text{store} \\mid \\text{the})\n  \\]\nTrigram Model:\n\\[\n  p(\\text{He went to the store}) = p(\\text{He}) \\times p(\\text{went} \\mid \\text{He}) \\times p(\\text{to} \\mid \\text{He\\ went}) \\times p(\\text{the} \\mid \\text{went\\ to}) \\times p(\\text{store} \\mid \\text{to\\ the})\n  \\]\nKey Idea:\nN-gram models capture some local word order, but struggle with long-range dependencies and rare phrases.\n\n\n\n\n\n2.4.3 Training an LLM\n\n“A general language model (LM) should be able to compute the probability of (and also generate) any string.”\n(Radford et al., 2019)\n\n\n\n2.4.4 What Does Training an LLM Look Like?\nConsider how humans complete sentences: - As Descartes said, “I think, therefore I .” - For all intents and - *I learned how to drive a ___*\nOr in dialogue: &gt; Monica: Okay, everybody relax. This is not even a ___\n&gt; Rachel: Oh God… well, it started about a half hour before the ___\n&gt; Ross: (squatting and reading the instructions) I’m supposed to attach a brackety ___\nThe core training objective for LLMs is next word prediction:\nGiven a sequence of words, predict the most likely next word.\n\n2.4.4.1 Neural Network for Next Word Prediction\n\nThe model is a neural network that outputs a score for every word in the vocabulary.\nThese scores are converted into probabilities using the softmax function.\nFor example, with a vocabulary of 50,000 words, the output might look like: [fish: 0.00002, help: 0.00002, ..., the: 0.00002, ..., aardvarks: 0.00002]\n\n\n\n2.4.4.2 Training Process\n\nCompute Loss:\n\nThe true next word is masked (hidden).\nThe model predicts probabilities for all words.\nThe loss function measures how well the model predicts the correct word (e.g., Loss = 1 - prob(correct word)).\nIf the model assigns a probability of 1 to the correct word, loss is 0 (best). If 0, loss is 1 (worst).\n\nUpdate Weights:\n\nThe model adjusts its internal weights to increase the probability of the correct word.\nThis also slightly decreases the probability for all other words.\nEach training example provides a small update—repeated millions or billions of times.\n\n\n\n\n2.4.4.3 Example: Weight Updates\nSuppose the correct next word is “fish”: - The model increases the weights leading to “fish”. - The probabilities for other words (e.g., “help”, “the”, “aardvarks”) are slightly reduced.\n\nEach example nudges the model to make the correct word more likely in context, and less likely for others.\nOver time, the model learns to predict words in a wide variety of contexts.\n\n\nThis is the fundamental process behind training large language models: predict the next word, compute the loss, update the weights, and repeat—at massive scale.\n\n\n\n2.4.5 Probing GPT\nLarge Language Models (LLMs) today generate highly coherent, grammatical text that can be indistinguishable from human output. They demonstrate some understanding of hierarchical structure and abstract linguistic categories (Mahowald et al., 2024).\nWhile these models are not perfect learners of abstract linguistic rules, neither are humans. LLMs are progressing toward acquiring formal linguistic competence and have already challenged claims about the impossibility of learning certain linguistic knowledge—such as hierarchical structure and abstract categories—from input alone (Mahowald et al., 2024).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Natural Language Processing</span>"
    ]
  },
  {
    "objectID": "session1.html#ai-where-are-we-heading",
    "href": "session1.html#ai-where-are-we-heading",
    "title": "2  Introduction to Natural Language Processing",
    "section": "2.5 AI: Where Are We Heading?",
    "text": "2.5 AI: Where Are We Heading?\nArtificial General Intelligence (AGI):\nAGI is defined as AI that matches or surpasses human cognitive capabilities across a wide range of tasks.\nAGI Benchmarks:\n\nThe Robot College Student Test (Goertzel):\nA machine enrolls in a university, takes and passes the same classes as humans, and obtains a degree. LLMs can now pass university-level exams without attending classes.\nThe Employment Test (Nilsson):\nA machine performs an economically important job at least as well as humans. AI is already replacing humans in roles ranging from fast food to marketing.\nThe Ikea Test (Marcus):\nAn AI views the parts and instructions of an Ikea flat-pack product, then controls a robot to assemble the furniture correctly.\nThe Coffee Test (Wozniak):\nA machine enters an average home and figures out how to make coffee: find the machine, coffee, water, mug, and brew coffee by pushing the right buttons. This remains unsolved.\nThe Modern Turing Test (Suleyman):\nAn AI is given $100,000 and must turn it into $1 million.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Natural Language Processing</span>"
    ]
  },
  {
    "objectID": "session1.html#applications-of-llms",
    "href": "session1.html#applications-of-llms",
    "title": "2  Introduction to Natural Language Processing",
    "section": "2.6 Applications of LLMs",
    "text": "2.6 Applications of LLMs\n\nAI interfaces for customer support and onboarding\nResearch portals with Retrieval-Augmented Generation (RAG)\nAutomated customer support (e.g., Zendesk)\nAccessibility tools (e.g., BeMyAI)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Natural Language Processing</span>"
    ]
  },
  {
    "objectID": "session1.html#takeaways",
    "href": "session1.html#takeaways",
    "title": "2  Introduction to Natural Language Processing",
    "section": "2.7 Takeaways",
    "text": "2.7 Takeaways\n\nLanguage is Hard:\n\nLanguage is infinite and ambiguous (both lexically and structurally).\n\nThe Revolution in NLP:\n\nLLMs now approach human-level language ability.\n\nExciting Research Directions:\n\nBuilding applications with LLMs\nProbing their abilities\n\nPowerful AI is Coming:\n\nThe field is rapidly advancing, with significant societal impact on the horizon.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Introduction to Natural Language Processing</span>"
    ]
  },
  {
    "objectID": "session2.html",
    "href": "session2.html",
    "title": "3  NLP Libraries and Text Normalization",
    "section": "",
    "text": "3.1 Libraries",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>NLP Libraries and Text Normalization</span>"
    ]
  },
  {
    "objectID": "session2.html#libraries",
    "href": "session2.html#libraries",
    "title": "3  NLP Libraries and Text Normalization",
    "section": "",
    "text": "NLTK\nspaCy\n\n\n\n\nMethods\nstring processing library\nobject-oriented approach - it parses a text, returns document object whose words and sentences are objects themselves\n\n\nTokenization\nuses regular expression-based methods which are not always accurate\nuses a rule-based approach to tokenization which is more accurate\n\n\nPOS Tagging\nprovides wide range of POS taggers (ranging from rule-based to machine learning-based)\nuses a deep learning-based POS tagger which is more accurate (also offers pre-trained models in multiple languages)\n\n\nNamed Entity Recognition (NER)\nprovides multiple NER taggers (ranging from rule-based to machine learning-based)\nuses a highly efficient deep learning-based NER tagger for detecting entities such as names, organizations, locations, etc.\n\n\nPerformance\nslower than spaCy\nfaster than NLTK (due to its optimized implementation in Cython)\n\n\n\n\nTextblob is another popular library for NLP in Python. It is built on top of NLTK and provides a simple API for common NLP tasks such as tokenization, POS tagging, and sentiment analysis.\n\nIt is not good for large scale production use",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>NLP Libraries and Text Normalization</span>"
    ]
  },
  {
    "objectID": "session2.html#normalization",
    "href": "session2.html#normalization",
    "title": "3  NLP Libraries and Text Normalization",
    "section": "3.2 Normalization",
    "text": "3.2 Normalization\nNormalization is the process of converting a text into a standard form. This involves removing any characters that are not part of the standard English alphabet, converting all characters to lowercase, and removing any extra spaces.\nTasks involved in normalization include\n\nTokenization: Breaking a text into words, phrases, symbols, or other meaningful elements.\nCase Folding: Converting all characters to lowercase.\nRemoving Punctuation: Removing any non-alphanumeric characters.\nRemoving Stopwords: Removing common words that do not carry much meaning (e.g., “the”, “is”, “and”, “in”, “to”, “of”).\nStemming or Lemmatization: Reducing words to their base or root form.\nRemoving Extra Spaces: Removing any extra spaces between words.\nExpanding Contractions: Expanding contractions such as “don’t” to “do not”.\nRemoving URLs and Emails: Removing URLs, email addresses, and other web-related content.\nRemoving HTML Tags: Removing HTML tags from web pages.\nRemoving Emojis and Special Characters: Removing emojis, emoticons, and other special characters.\n\nRemoval of Stopwords in Python using NLTK\n\ntext = \"NLTK and spaCy are popular NLP libraries used for text processing.\"\ntokens = word_tokenize(text)\nstop_words = set(stopwords.words(\"english\"))\nfiltered_tokens = [word for word in tokens if word.lower() not in stop_words]\nprint(f\"NLTK stopwords removal: {' '.join(filtered_tokens)}\")\n\nNLTK stopwords removal: NLTK spaCy popular NLP libraries used text processing .\n\n\nRemoval of Stopwords in Python using spaCy\n\nnlp = spacy.load(\"en_core_web_sm\") # Load English language model\ntext = \"NLTK and spaCy are popular NLP libraries used for text processing.\"\ndoc = nlp(text)\nfiltered_tokens = [token.text for token in doc if not token.is_stop]\nprint(f\"spaCy stopwords removal: {' '.join(filtered_tokens)}\")\n\nspaCy stopwords removal: NLTK spaCy popular NLP libraries text processing .\n\n\n\n3.2.0.1 Morphological Normalization\n\nMorphological normalization is the process of reducing a word to its base or root form\nThis can involve stemming or lemmatization\nRoots are the base forms of words eg. “run” is the root of “running”, “ran”, “runs”\nAffixes are the prefixes and suffixes that are added to roots to create different forms of words eg. “ing”, “ed”, “s”\n\nPrefixes are added to the beginning of a word eg. “un” in “undo”\nSuffixes are added to the end of a word eg. “ly” in “quickly”\n\nInflectional affixes are added to a word to change its grammatical form eg. “s” for plural nouns, “ed” for past tense verbs eg. “dogs”, “walked”\n\n\n\n3.2.1 Tokenization\n\nTokenization is the process of breaking a text into words, phrases, symbols, or other meaningful elements.\nThe tokens are the words, sentences, characters, or subwords that are produced by the tokenization process.\nSpace based token is used to prepare the text for further processing such as stemming, lemmatization, and POS tagging.\nType is number of unique tokens in a text\nToken is a single instance of a token in a text\n\n\ntokens = word_tokenize(text)\ntotal_tokens = len(tokens)\ntotal_types = len(set(tokens))\n\n\nCorpus is a collection of text documents\n\n\n3.2.1.0.1 Tokenization of raw text in Python\n\nword.isalnum() - returns True if all characters in the word are alphanumeric\n\n\nshakespeare_url = \"https://ocw.mit.edu/ans7870/6/6.006/s08/lecturenotes/files/t8.shakespeare.txt\"\ncrime_punishment_url = \"http://www.gutenberg.org/files/2554/2554-0.txt\"\n\n# Tokenize Raw Text\n\nshakespeare_text = request.urlopen(shakespeare_url).read().decode(\"utf8\")\ncrime_punishment_text = request.urlopen(crime_punishment_url).read().decode(\"utf8\")\n\nshakespeare_tokenized = [word for word in word_tokenize(shakespeare_text.lower()) if word.isalnum()]\ncrime_punishment_tokenized = [\n    word for word in word_tokenize(crime_punishment_text.lower()) if word.isalnum()\n]\n\n# FreqDist (from nltk) to produce a frequency distribution (listing top 20 most common words)\n\nshakespeare_freq = FreqDist(shakespeare_tokenized)\ncrime_freq = FreqDist(crime_punishment_tokenized)\n\nshakespeare_common = shakespeare_freq.most_common(20)\ncrime_common = crime_freq.most_common(20)\n\nprint(\"Top 20 words in Shakespeare\")\nprint(shakespeare_common)\n\nprint(\"\\nTop 20 words in Crime and Punishment:\")\nprint(crime_common)\n\n\n\n3.2.1.0.2 NLTK Tokenizers\n\nword_tokenize(): Tokenizes a text into words using a regular expression-based tokenizer. A versatile tokenizer that handles contractions, abbreviations, and punctuation marks effectively. It is suitable for most general-purpose tokenization tasks.\nWordPunctTokenizer(): Tokenizes a text into words using a regular expression that matches punctuation as separate tokens. A specialized tokenizer that separates words and punctuation explicitly. It is useful when you need to distinguish between alphabetic and non-alphabetic characters.\n\n\n\n\n3.2.2 Stemming\n\nStemming is the process of reducing a word to its root or base form. For example, the word “running” would be reduced to “run” by a stemming algorithm.\nStemmers remove word suffixes by running input word tokens against a pre-defined list of common suffixes.\nStemming is a heuristic process that does not always produce a valid root word, but it can be useful for text processing tasks such as search indexing and information retrieval.\nPorter Stemmer is a popular (rule-based) stemming algorithm that is widely used in text processing applications.\nSnowball Stemmer is an improved version of the Porter Stemmer that supports multiple languages.\n\n\n3.2.2.1 Example of stemming in Python using NLTK\n\nsbs = SnowballStemmer(\"english\")\ntext=\"The stars twinkled in the dark, illuminating the night sky.\"\nmethod = TreebankWordTokenizer()\nword_tokens = method.tokenize(text)\nstemmed = [sbs.stem(token) for token in word_tokens]\n\nfor i in range(len(word_tokens)):\n    print(f\"{word_tokens[i]} | {stemmed[i]}\")\n\nThe | the\nstars | star\ntwinkled | twinkl\nin | in\nthe | the\ndark | dark\n, | ,\nilluminating | illumin\nthe | the\nnight | night\nsky | sky\n. | .\n\n\n\n\n\n3.2.3 Lemmatization\n\nThe process of reducing a word to its base or root form, known as a lemma.\nIs more sophisticated than stemming because it uses a dictionary to map words to their base forms.\nPOS tagging is used to determine the correct lemma for a word based on its part of speech.\nLemmatization is more accurate than stemming but can be slower and more computationally intensive.\nWordNet Lemmatizer is a popular lemmatization algorithm that is available in NLTK.\n\n\nNote:\nNLTK vs spaCy for lemmatization: NLTK is more traditional and relies on WordNet, while spaCy uses a more modern approach. spaCy is generally more accurate.\n\n\n3.2.3.1 Example of stemming in Python using NLTK\n\nsbs = SnowballStemmer(\"english\")\ntext=\"The stars twinkled in the dark, illuminating the night sky.\"\nmethod = TreebankWordTokenizer()\nword_tokens = method.tokenize(text)\nlemma = WordNetLemmatizer()\nlemmatized = [lemma.lemmatize(token) for token in word_tokens]\n\nfor i in range(len(word_tokens)):\n    print(f\"{word_tokens[i]} | {lemmatized[i]}\")\n\nThe | The\nstars | star\ntwinkled | twinkled\nin | in\nthe | the\ndark | dark\n, | ,\nilluminating | illuminating\nthe | the\nnight | night\nsky | sky\n. | .\n\n\nComparison table of stemming and lemmatization\n\n\n\n\n\n\n\nStemming\nLemmatization\n\n\n\n\nFaster\nSlower (Resource Intensive)\n\n\nLess accurate\nMore accurate\n\n\nUses heuristics (eg choppping off endings)\nUses a dictionary-based lookup\n\n\nProduces root words\nProduces base words\n\n\nRemoves word suffixes\nMaps words to base forms\n\n\nDoes not require POS tagging\nRequires POS tagging",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>NLP Libraries and Text Normalization</span>"
    ]
  },
  {
    "objectID": "session2.html#regular-expressions",
    "href": "session2.html#regular-expressions",
    "title": "3  NLP Libraries and Text Normalization",
    "section": "3.3 Regular Expressions",
    "text": "3.3 Regular Expressions\n\nRegular expressions are a powerful tool for pattern matching and text processing.\nThey allow you to search for patterns in text, extract specific information, and perform complex text transformations.\n\nRegular Expression (Disjunction) Table\n\n\n\n\n\n\n\n\nPattern\nMatches\nExample\n\n\n\n\n.\nAny character except newline\n“a.b” matches “axb”, “a1b”, “a@b”\n\n\n^\nStart of string\n“^abc” matches “abc”, “abcd”, “abc123”\n\n\n$\nEnd of string\n“abc$” matches “abc”, “123abc”, “xyzabc”\n\n\n*\nZero or more occurrences\n“ab*c” matches “ac”, “abc”, “abbc”\n\n\n+\nOne or more occurrences\n“ab+c” matches “abc”, “abbc”, “abbbc”\n\n\n?\nZero or one occurrence\n“ab?c” matches “ac”, “abc”\n\n\n{n}\nExactly n occurrences\n“ab{2}c” matches “abbc”\n\n\n{n,}\nn or more occurrences\n“ab{2,}c” matches “abbc”, “abbbc”\n\n\n{n,m}\nBetween n and m occurrences\n“ab{2,3}c” matches “abbc”, “abbbc”\n\n\n[abc]\nAny character in the set\n“[abc]” matches “a”, “b”, “c”\n\n\n[^abc]\nAny character not in the set\n“[^abc]” matches “d”, “e”, “f”\n\n\n[A-Z]\nAny character in the range\n“[A-Z]” matches “A”, “B”, “C”\n\n\n[a-z]\nAny character in the range\n“[a-z]” matches “a”, “b”, “c”\n\n\n[0-9]\nAny digit\n“[0-9]” matches “0”, “1”, “2”\n\n\n\\d\nAny digit\n“\\d” matches “0”, “1”, “2”\n\n\n\\D\nAny non-digit\n“\\D” matches “a”, “b”, “c”\n\n\n\\w\nAny word character\n“\\w” matches “a”, “b”, “c”, “0”, “1”, “2”\n\n\n\\W\nAny non-word character\n“\\W” matches “@”, “#”, “$”\n\n\n\\s\nAny whitespace character\n“\\s” matches ” “,”, “”\n\n\n\\S\nAny non-whitespace character\n“\\S” matches “a”, “b”, “c”\n\n\n\n\n#(1.1)\n\nEmployee_Record = ['Rajani Singh: rs.digi@cbs.dk: +918750602050',\n                   'Sine Zambach: sz.itm@cbs.dk: +4538153815',\n                    'Daniel Hardt: dha.msc@cbs.dk: +41852024']\nprint('1.1 Created List\\n''Employee Name:',' Email address:',' Telephone Number')\nfor d in Employee_Record:\n    print(d)\n\n#(1.2)\n#\\w is short hand for [a-zA-Z_]\n#\\d is short hand for [0-9]\n\nimport re\nmy_pattern =r'(\\w+\\s\\w+): (\\w+\\.(\\w+)@\\w+\\.\\w+)'\nprint('\\n1.2 Extracted Information \\nEmployee Name:',' Email address:','Department')\nfor d in Employee_Record:\n    match = re.search(my_pattern ,d)\n    print(match.group(1)+': ' + match.group(2) + ': ' + match.group(3))\n\n#(1.3)\n#In tel. number we select last 8 digits, after the prefix (which is set to optional using ? sign)\n\nnew_pattern =r'(\\w+\\s\\w+): \\w+\\.\\w+@\\w+\\.\\w+: \\+?\\d?\\d?(\\d{8,10}$)'\nprint('\\n1.3 Extracted Information \\nEmployee Name:','Telephone Number')\nfor d in Employee_Record:\n    match = re.search(new_pattern ,d)\n    print(match.group(1)+': ' + match.group(2))\n\n    #notice all the match groups - and how they work - in the first one we have 3, in the second one only two - always use them for the info you want to retrieve\n\n1.1 Created List\nEmployee Name:  Email address:  Telephone Number\nRajani Singh: rs.digi@cbs.dk: +918750602050\nSine Zambach: sz.itm@cbs.dk: +4538153815\nDaniel Hardt: dha.msc@cbs.dk: +41852024\n\n1.2 Extracted Information \nEmployee Name:  Email address: Department\nRajani Singh: rs.digi@cbs.dk: digi\nSine Zambach: sz.itm@cbs.dk: itm\nDaniel Hardt: dha.msc@cbs.dk: msc\n\n1.3 Extracted Information \nEmployee Name: Telephone Number\nRajani Singh: 8750602050\nSine Zambach: 38153815\nDaniel Hardt: 41852024\n\n\nRegular Expression Method Table\n\n\n\nMethod\nDescription\n\n\n\n\nre.match()\nMatches a pattern at the beginning of a string\n\n\nre.search()\nSearches for a pattern in a string\n\n\nre.findall()\nFinds all occurrences of a pattern in a string\n\n\nre.split()\nSplits a string based on a pattern\n\n\nre.sub()\nReplaces a pattern in a string with another pattern\n\n\n\nExample of Regular Expressions in Python\n\ntext = \"The quick brown fox jumps over the lazy dog. The dog barks at the fox.\"\nsentences = sent_tokenize(text)\nwords = word_tokenize(text)\n\n# Find all words that start with \"b\"\npattern = r\"\\b[bB]\\w+\"  # \\b is a word boundary, \\w+ is one or more word characters\nfor word in words:\n    if re.match(pattern, word):\n        print(f\" All words that start with 'b': {word}\")\n\n# Find all sentences that contain the word \"dog\"\npattern = r\"\\b[dD]ogs?\\b\"  # \\b is a word boundary, ? is zero or one occurrence of previous character, s? matches \"dog\" or \"dogs\"\nfor sentence in sentences:\n    if re.search(pattern, sentence):\n        print(f\" All sentences containing word 'dog': {sentence}\")\n\n All words that start with 'b': brown\n All words that start with 'b': barks\n All sentences containing word 'dog': The quick brown fox jumps over the lazy dog.\n All sentences containing word 'dog': The dog barks at the fox.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>NLP Libraries and Text Normalization</span>"
    ]
  },
  {
    "objectID": "session2.html#pos-tagging",
    "href": "session2.html#pos-tagging",
    "title": "3  NLP Libraries and Text Normalization",
    "section": "3.4 POS Tagging",
    "text": "3.4 POS Tagging\n\nPart-of-speech (POS) tagging is the process of assigning a part of speech to each word in a text. The part of speech indicates the grammatical category of the word, such as noun, verb, adjective, etc.\nThe goal of POS tagging is to identify the syntactic structure of a sentence and extract useful information about the text.\nTypes of POS Tagging\n\nRule-based POS Tagging:\n\nUses hand-crafted rules to assign POS tags to words based on their context.\nRelies on a predefined set of rules and patterns to determine the correct POS tag for a word.\nPros:\n\nSimple and easy to implement, but may not be as accurate as other methods.\nDoesnt require a lot of computational resources or training data.\nCan be easily customized and adapted to different languages and domains.\nCons:\n\nMay not be as accurate as other methods, especially for complex or ambiguous cases.\nRequires manual effort to define rules and patterns for different languages and domains.\nLimited to the rules and patterns defined by the developer, which may not cover all cases.\n\nExample: Rule-based POS taggers in NLTK such as the DefaultTagger and RegexpTagger and pos_tag method.  \n\n\nStatistical POS Tagging:\nUses statistical models (such as Hidden Markov Models) to assign POS tags to words based on their context and probability.\nLearns the patterns and relationships between words and their POS tags from a large corpus of annotated text.\n\nPros:\n\nMore accurate than rule-based methods, especially for complex or ambiguous cases.\nCan handle a wide range of languages and domains without manual intervention.\nCan be trained on large datasets to improve accuracy and performance.\n\nCons:\n\nRequires a large amount of annotated training data to train the statistical model.\nCan be computationally intensive and require significant resources for training and inference.\nMay not be as interpretable or transparent as rule-based methods, making it difficult to understand the model’s decisions.\n\nExamples: Machine learning-based POS taggers in spaCy such as the PerceptronTagger and CNNTagger.\n\n\n\nMost Common POS Tags (NLTK)\n\n\n\nTag\nDescription\nExample\n\n\n\n\nCC\nCoordinating conjunction\nand, but, or\n\n\nCD\nCardinal number\n1, 2, 3\n\n\nDT\nDeterminer\nthe, a, an\n\n\nEX\nExistential there\nthere\n\n\nFW\nForeign word\nbonjour\n\n\nIN\nPreposition or subordinating conjunction\nin, of, on\n\n\nJJ\nAdjective\nbig, green, happy\n\n\nJJR\nAdjective, comparative\nbigger, greener, happier\n\n\nJJS\nAdjective, superlative\nbiggest, greenest, happiest\n\n\nLS\nList item marker\n1, 2, 3\n\n\nMD\nModal\ncan, could, might\n\n\nNN\nNoun, singular or mass\ndog, cat, house\n\n\n\n \nMost Common POS Tags (SpaCy)\n\n\n\nTag\nDescription\nExample\n\n\n\n\nADJ\nAdjective\nbig, green, happy\n\n\nADP\nAdposition\nin, to, during\n\n\nADV\nAdverb\nvery, tomorrow, down\n\n\nAUX\nAuxiliary\nis, has (done), will\n\n\nCONJ\nConjunction\nand, or, but\n\n\nCCONJ\nCoordinating conjunction\nand, or, but\n\n\nDET\nDeterminer\na, an, the\n\n\nINTJ\nInterjection\npsst, ouch, bravo\n\n\nNOUN\nN\n\n\n\nNUM\nNumeral\n42, forty-two\n\n\n\n\n3.4.0.1 Example of POS Tagging in Python using NLTK\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk import pos_tag\n\ntext = \"The quick brown fox jumps over the lazy dog.\"\nwords = word_tokenize(text)\ntags = pos_tag(words)\n\nfor word, tag in tags:\n    print(f\"{word} | {tag}\")\n\n\n3.4.0.2 Example of POS Tagging in Python using spaCy\n\nMake sure to download the spaCy model using python -m spacy download en_core_web_sm\nThis model is a small English model trained on written web text (blogs, news, comments), which includes vocabulary, vectors, syntax, and entities.\nThe model is trained on the OntoNotes 5 corpus and supports POS tagging, dependency parsing, named entity recognition, and more.\n\n\nmodel = spacy.load(\"en_core_web_sm\")\nsample_text = \"The quick brown fox jumps over the lazy dog.\"\ndoc = model(sample_text)\n\nfor word in doc:\n    print(f\"{word.text} | {word.pos_}\")\n\nThe | DET\nquick | ADJ\nbrown | ADJ\nfox | NOUN\njumps | VERB\nover | ADP\nthe | DET\nlazy | ADJ\ndog | NOUN\n. | PUNCT",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>NLP Libraries and Text Normalization</span>"
    ]
  },
  {
    "objectID": "session2.html#named-entity-recognition-ner",
    "href": "session2.html#named-entity-recognition-ner",
    "title": "3  NLP Libraries and Text Normalization",
    "section": "3.5 Named Entity Recognition (NER)",
    "text": "3.5 Named Entity Recognition (NER)\n\nNamed Entity Recognition (NER) is the process of identifying and classifying named entities in a text.\nNamed entities are real-world objects such as persons, organizations, locations, dates, and more.\nNER is an important task in NLP because it helps extract useful information from unstructured text and enables downstream tasks such as information retrieval, question answering, and text summarization.\nNER models are trained on annotated datasets that contain labeled examples of named entities in text.\nCommon types of named entities:\n\nPerson: Names of people, such as “John Doe” or “Alice Smith”.\nOrganization: Names of companies, institutions, or groups, such as “Google” or “United Nations”.\nLocation: Names of places, such as “New York” or “Mount Everest”.\nDate: Dates and times, such as “January 1, 2022” or “10:30 AM”.\nNumber: Numerical quantities, such as “100” or “3.14”.\nMiscellaneous: Other named entities, such as “Apple” (the company) or “Python” (the programming language).\nEvent: Names of events, such as “World War II” or “Super Bowl”.\nProduct: Names of products, such as “iPhone” or “Coca-Cola”.\n\n\n\n3.5.0.1 Example of Named Entity Recognition in Python using NLTK\nIn NLP, a tree structure is often used to represent the syntactic structure of a sentence. Each node in the tree represents a linguistic unit, such as a word or a phrase, and the edges represent the relationships between these units. The Tree class provides various methods for creating, traversing, and modifying these tree structures.\ntext = \"Apple is a technology company based in Cupertino, California.\"\nwords = word_tokenize(text)\ntags = pos_tag(words)\ntree = ne_chunk(tags)\n\nfor subtree in tree:\n    if isinstance(subtree, Tree):\n        label = subtree.label()\n        words = \" \".join([word for word, tag in subtree.leaves()])\n        print(f\"{label}: {words}\")\n\nNote: NLTK’s named entity recognizer has identified “Apple”, “Cupertino”, and “California” as geopolitical entities (GPE).\nThe NER model uses context and patterns learned from training data to classify named entities, but it is not always perfect and can sometimes make mistakes, as seen with “Apple” in this case.\n\n\n\n3.5.0.2 Example of Named Entity Recognition in Python using spaCy\nmodel = spacy.load(\"en_core_web_sm\")\nsample_text = \"Apple is a technology company based in Cupertino, California.\"\ndoc = model(sample_text)\n\ndisplacy.render(doc, style=\"ent\", jupyter=True) # style=\"ent\" is used to display named entities",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>NLP Libraries and Text Normalization</span>"
    ]
  },
  {
    "objectID": "session3.html",
    "href": "session3.html",
    "title": "4  N-Gram Language Models",
    "section": "",
    "text": "4.1 N-Gram (Probabilistic LM)\nThis is model predicts the probability of a word given the previous n-1 words. It is based on the assumption that the probability of a word depends on the context provided by the previous n-1 words. N-grams are used in various NLP tasks such as speech recognition, machine translation, and text generation.\nnumber of n-grams = (total number of words) - (n - 1)\nGenerating different n-grams for a text:\nfrom nltk import ngrams\nfrom nltk.tokenize import word_tokenize\ntext = \"Italy is famous for its cuisine.\"\n\ndef generate_ngrams(text, n):\n    tokens = word_tokenize(text)\n    ngrams_list = list(ngrams(tokens, n))\n    return [\" \".join(ngram) for ngram in ngrams_list]\n\ntrigrams = generate_ngrams(text, 3)\n\nprint(\"Trigrams:\", trigrams)\n\nTrigrams: ['Italy is famous', 'is famous for', 'famous for its', 'for its cuisine', 'its cuisine .']",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>N-Gram Language Models</span>"
    ]
  },
  {
    "objectID": "session3.html#n-gram-probabilistic-lm",
    "href": "session3.html#n-gram-probabilistic-lm",
    "title": "4  N-Gram Language Models",
    "section": "",
    "text": "Unigram: A single word\nBigram: A pair of words\nTrigram: A triplet of words ect…\n\n\n\n\n\n4.1.1 Conditional Probability and Chain Rule\n\nJoint Probability: The probability of two or more events occurring together. It is denoted as P(A, B) and is calculated as the probability of both events A and B occurring.\n\nJoint Probability Formula: \\(P(A \\cap B) = P(A) \\times P(B)\\)\nis equivalent to \\(P(A \\cap B) = P(A) \\times P(B)\\)  \n\nConditional Probability: The probability of an event given that another event has occurred. It is denoted as P(A | B) which is the probability of event A occurring given that event B has occurred.\nIt is calculated as the probability of both events A and B occurring divided by the probability of event B occurring.\n\nConditional Probability Formula: \\(P(A \\mid B) = \\frac{P(A, B)}{P(B)}\\)  \n\nChain Rule of Probability: The probability of a sequence of events can be calculated by multiplying the conditional probabilities of each event given the previous events in the sequence.\n\nChain Rule Formula: \\[\n\\small\nP(w_1, w_2, ..., w_n) = P(w_1) * P(w_2 \\mid w_1) * P(w_3 \\mid w_1, w_2) * ... * P(w_n \\mid w_1, w_2, ..., w_{n-1})\n\\]\nExample: The probability of the sequence “The quick brown fox” can be calculated using the chain rule as follows: \\[\n\\small\n\\begin{aligned}\nP(\\text{The quick brown fox}) &= P(\\text{The}) \\times P(\\text{quick} \\mid \\text{The}) \\\\\n&\\times P(\\text{brown} \\mid \\text{The quick}) \\\\\n&\\times P(\\text{fox} \\mid \\text{The quick brown})\n\\end{aligned}\n\\]\n\n“What is the probability of the 10th word given the previous 9 words?”\n\nNote: The chain rule is used to calculate the probability of a sequence of words in an N-gram model.\n\n\n\n\n4.1.2 Markov Assumption\nTo make things manageable, N-gram models approximate the chain rule by making a simplifying assumption called the Markov assumption. This assumption states that the probability of a word depends only on the previous n-1 words, not on all the previous words in the sequence. This simplifies the calculation of the conditional probability of a word given the previous n-1 words.\n\nFirst-Order Markov Model: The probability of a word depends only on the previous word.\n\n\\[P(w_n | w_1, w_2, ..., w_{n-1}) = P(w_n | w_{n-1})\\]\n“The probability of the 10th word depends only on the 9th word, not on the previous words.”\nExample: \\(P(\\text{fox} \\mid \\text{The quick brown}) = P(\\text{fox} \\mid \\text{brown})\\)\nNote: The first-order Markov model is a special case of the N-gram model where n=2.  \n\nSecond-Order Markov Model: The probability of a word depends only on the previous two words.\n\n\\[P(w_n | w_1, w_2, ..., w_{n-1}) = P(w_n | w_{n-2}, w_{n-1})\\]\n“The probability of the 10th word depends only on the 8th and 9th words, not on the previous words.”\nExample: \\(P(\\text{fox} \\mid \\text{The quick brown}) = P(\\text{fox} \\mid \\text{quick brown})\\)\nNote: The second-order Markov model is a special case of the N-gram model where n=3.\n\n\nN-Gram Probability Calculation\n\nMathematically, for a sequence of words \\(w_1, w_2, ..., w_n\\), the probability of the sequence can be calculated using the chain rule of probability:\n\n\\[\\small P(w_1, w_2, ..., w_n) = P(w_1) * P(w_2 \\mid w_1) * P(w_3 \\mid w_1, w_2) * ... * P(w_n \\mid w_1, w_2, ..., w_{n-1})\\]\n\nThe probability of a word given the previous n-1 words can be calculated using the formula:\n\n\\(P(w_n | w_1, w_2, ..., w_{n-1}) = \\frac{P(w_1, w_2, ..., w_n)}{P(w_1, w_2, ..., w_{n-1})}\\) \n\nThe probability of a sequence of words can be calculated by counting the occurrences of the n-gram in a corpus and dividing by the total number of n-grams in the corpus. \nThe Maximum Likelihood Estimation (MLE) of an N-gram is calculated as:\n\n\\(P(w_n | w_1, w_2, ..., w_{n-1}) = \\frac{\\text{Count(n-gram)}}{\\text{Count(of previous n-1 words)}}\\)  \nIssue: The MLE can assign zero probability to unseen n-grams, leading to sparsity and poor generalization.\n\n\n\n\n4.1.3 Padding\nIs a technique used to handle the boundary conditions in N-gram models where the context words are not available. - Start-of-Sentence (BOS) Padding: A special token that represents the beginning of a sentence. It is used to handle the first word in a sentence where the context words are not available. - Example: &lt;s&gt; The quick brown fox jumps over the lazy dog. Note &lt;s&gt; is equivalent to &lt;start&gt;  \n\nEnd-of-Sentence (EOS) Padding: A special token that represents the end of a sentence. It is used to handle the last word in a sentence where the context words are not available.\n\nExample: The fox jumps over the lazy dog. &lt;/s&gt;\n\n\nNotes: the choice of how many padding tokens to use depends on the order of the N-gram model. For a bigram model, you would use one padding token at the beginning of the sentence. For a trigram model, you would use two padding tokens at the beginning of the sentence. The padding tokens are not part of the vocabulary and are used only for modeling purposes.\nPadding Example in Python\n\nfrom nltk.tokenize import sent_tokenize, word_tokenize\ntext = \"I like NLP. NLP is fun. NLP in python is fun.\"\nsentences = sent_tokenize(text)\n\n# Pad each sentence individually with &lt;s&gt; and &lt;/s&gt; tokens and tokenize into words\npadded_sentences = []\nfor sentence in sentences:\n    words = word_tokenize(sentence)\n    padded = [\"&lt;s&gt;\"] + words + [\"&lt;/s&gt;\"]\n    padded_sentences.append(padded)\n\nfor sentence in padded_sentences:\n    print(sentence)\n\n['&lt;s&gt;', 'I', 'like', 'NLP', '.', '&lt;/s&gt;']\n['&lt;s&gt;', 'NLP', 'is', 'fun', '.', '&lt;/s&gt;']\n['&lt;s&gt;', 'NLP', 'in', 'python', 'is', 'fun', '.', '&lt;/s&gt;']\n\n\n\n\n4.1.4 Issue of Underflow\n\nUnderflow: A numerical issue that occurs when multiplying many small probabilities together, leading to a very small probability that may be rounded to zero.\nSolution: Use log probabilities to avoid underflow by converting the product of probabilities to the sum of log probabilities.\n\n\n\n4.1.5 Smoothing Techniques\n\nThis is a technique used to address the issue of zero probabilities in N-gram models. It assigns a small non-zero probability to unseen n-grams to improve the generalization of the model. You “test” these by comparing smoothed probabilities to unsmoothed ones.  \nLaplace (Add-One) Smoothing: A simple smoothing technique that adds one to the count of each n-gram to avoid zero probabilities.\n\nFormula: \\(P(w_n | w_1, w_2, ..., w_{n-1}) = \\frac{(\\text{Count(n-gram)} + 1)}{(\\text{Count(of previous n-1 words)} + V)}\\)\n\nV: The vocabulary size, which is the total number of unique words in the corpus.\nExample: \\[P(\\text{fox} \\mid \\text{quick brown}) = \\frac{\\text{Count}(\\text{quick brown fox}) + 1}{\\text{Count}(\\text{quick brown}) + V}\\]\n\n\nAdd-k Smoothing: A generalization of Laplace smoothing that adds a smaller constant k to the count of each n-gram to avoid zero probabilities.\n\nFormula: \\(P(w_n | w_1, w_2, ..., w_{n-1}) = \\frac{(\\text{Count(n-gram)} + k)}{(\\text{Count(of previous n-1 words)} + kV)}\\)\n\nk: A constant value that determines the amount of smoothing.\nExample: \\[P(\\text{fox} \\mid \\text{quick brown}) = \\frac{\\text{Count}(\\text{quick brown fox}) + k}{\\text{Count}(\\text{quick brown}) + kV}\\]\nTest: Tune k and compare probabilities for rare vs. frequent trigrams.\n\n\nGood-Turing Smoothing: A more sophisticated smoothing technique that estimates the probability of unseen n-grams based on the frequency of seen n-grams.\n\nadjusts the counts of n-grams based on the frequency of n-grams with similar counts.\nTest: compute adjusted probabilities for low-frequency n-grams and validate against held-out data.\n\nBackoff and Interpolation: A technique that combines n-gram models of different orders to improve the generalization of the model.\n\nBackoff: Uses lower-order n-grams when higher-order n-grams have zero probabilities.\nInterpolation: Combines probabilities from different n-gram models using linear interpolation.\nTest: Compare performance of backoff and interpolation on different datasets.\n\nKneser-Ney Smoothing: A state-of-the-art smoothing technique that estimates the probability of unseen n-grams based on the frequency of n-grams in the context of the n-gram.\n\nIt considers how often a word appears in a novel context, rather than just how often it appears overall.\nTest: Compare Kneser-Ney smoothing to other smoothing techniques on large datasets.\n\n\n\n\n4.1.6 Perplexity\n\nPerplexity: A measure of how well a language model predicts a given text. It is the inverse probability of the test set, normalized by the number of words.\nEvaluation Metric: “How well the model predicts the next word in a sequence.”\n\nPerplexity Formula: \\(PP(W) = P(w_1, w_2, ..., w_N)^{-\\frac{1}{N}}\\)\n\nwhich can be calculated as \\(PP(W) = \\left(\\frac{1}{P(w_1, w_2, ..., w_N)}\\right)^{\\frac{1}{N}}\\)\nwhere:\n\nP(w_1, w_2, …, w_N): Probability of the test set under the language model.\nN: Number of words in the test set.\nExample: If the perplexity of a language model is 100, it means that the model is as confused as if it had to choose uniformly among 100 words for each word in the test set.\n\nN: The number of words in the test set.\nExample: If the perplexity of a language model is 100, it means that the model is as confused as if it had to choose uniformly among 100 words for each word in the test set.\n\nLower Perplexity: better language model that predicts the test set more accurately.\nHigher Perplexity: worse language model that predicts the test set less accurately.\n\n\n\n\n4.1.7 Example of Trigram LM in Python\n\nSteps:\n\nTokenize text\nAdd padding tokens\nGenerate trigrams\nCount unique trigrams\nCalculate trigram probabilities (MLE)\nCalculate perplexity\n\n\nSTEP 1: tokenize text and add padding tokens\n\ntext = \"I like NLP. NLP is fun. NLP in python is fun. I like coding in python. NLP is cool.\"\ntokens = sent_tokenize(text)\npadded_sentences = []\nfor token in tokens:\n    words = word_tokenize(token)\n    padded = [\"&lt;s&gt;\"] + words + [\"&lt;/s&gt;\"]\n    padded_sentences.append(padded)\n\nprint(\"Padded Sentences:\")\nfor sent in padded_sentences:\n    print(sent)\n\nPadded Sentences:\n['&lt;s&gt;', 'I', 'like', 'NLP', '.', '&lt;/s&gt;']\n['&lt;s&gt;', 'NLP', 'is', 'fun', '.', '&lt;/s&gt;']\n['&lt;s&gt;', 'NLP', 'in', 'python', 'is', 'fun', '.', '&lt;/s&gt;']\n['&lt;s&gt;', 'I', 'like', 'coding', 'in', 'python', '.', '&lt;/s&gt;']\n['&lt;s&gt;', 'NLP', 'is', 'cool', '.', '&lt;/s&gt;']\n\n\nSTEP 2: Generate trigrams (and bigrams for MLE calculation)\n\ntrigrams = []\nfor sent in padded_sentences:\n    sent_trigrams = list(ngrams(sent, 3))\n    # sent_trigrams = list(ngrams(sent, 3, pad_left=False, pad_right=False, left_pad_symbol=\"&lt;s&gt;\", right_pad_symbol=\"&lt;/s&gt;\"))\n    # can use the above line to avoid padding tokens in trigrams, but its less flexible if you need to reuse the padded data.\n    trigrams.extend(sent_trigrams)\n\nbigrams = []\nfor sent in padded_sentences:\n    sent_bigrams = list(ngrams(sent, 2))\n    bigrams.extend(sent_bigrams)\n\nprint(\"\\nTrigrams:\")\nfor trigram in trigrams:\n    print(trigram)\n\n\nTrigrams:\n('&lt;s&gt;', 'I', 'like')\n('I', 'like', 'NLP')\n('like', 'NLP', '.')\n('NLP', '.', '&lt;/s&gt;')\n('&lt;s&gt;', 'NLP', 'is')\n('NLP', 'is', 'fun')\n('is', 'fun', '.')\n('fun', '.', '&lt;/s&gt;')\n('&lt;s&gt;', 'NLP', 'in')\n('NLP', 'in', 'python')\n('in', 'python', 'is')\n('python', 'is', 'fun')\n('is', 'fun', '.')\n('fun', '.', '&lt;/s&gt;')\n('&lt;s&gt;', 'I', 'like')\n('I', 'like', 'coding')\n('like', 'coding', 'in')\n('coding', 'in', 'python')\n('in', 'python', '.')\n('python', '.', '&lt;/s&gt;')\n('&lt;s&gt;', 'NLP', 'is')\n('NLP', 'is', 'cool')\n('is', 'cool', '.')\n('cool', '.', '&lt;/s&gt;')\n\n\nSTEP 3: Count unique trigrams and bigrams\n\nfrom collections import Counter\nfrom prettytable import PrettyTable\ntrigram_counts = Counter(trigrams)\nbigrams_counts = Counter(bigrams)\nunique_bigrams = len(bigrams_counts)\nunique_trigrams = len(trigram_counts)\nprint(\"\\nUnique Trigrams:\", unique_trigrams)\nprint(\"Unique Bigrams:\", unique_bigrams)\n\nc_tri_tab = PrettyTable([\"Index\", \"Unique Trigram\", \"Count\"])\nfor i, (trigram, count) in enumerate(trigram_counts.items()):\n    c_tri_tab.add_row([i, trigram, count])\nprint(c_tri_tab)\n\n\nUnique Trigrams: 20\nUnique Bigrams: 17\n+-------+----------------------------+-------+\n| Index |       Unique Trigram       | Count |\n+-------+----------------------------+-------+\n|   0   |    ('&lt;s&gt;', 'I', 'like')    |   2   |\n|   1   |    ('I', 'like', 'NLP')    |   1   |\n|   2   |    ('like', 'NLP', '.')    |   1   |\n|   3   |    ('NLP', '.', '&lt;/s&gt;')    |   1   |\n|   4   |    ('&lt;s&gt;', 'NLP', 'is')    |   2   |\n|   5   |    ('NLP', 'is', 'fun')    |   1   |\n|   6   |     ('is', 'fun', '.')     |   2   |\n|   7   |    ('fun', '.', '&lt;/s&gt;')    |   2   |\n|   8   |    ('&lt;s&gt;', 'NLP', 'in')    |   1   |\n|   9   |  ('NLP', 'in', 'python')   |   1   |\n|   10  |   ('in', 'python', 'is')   |   1   |\n|   11  |  ('python', 'is', 'fun')   |   1   |\n|   12  |  ('I', 'like', 'coding')   |   1   |\n|   13  |  ('like', 'coding', 'in')  |   1   |\n|   14  | ('coding', 'in', 'python') |   1   |\n|   15  |   ('in', 'python', '.')    |   1   |\n|   16  |  ('python', '.', '&lt;/s&gt;')   |   1   |\n|   17  |   ('NLP', 'is', 'cool')    |   1   |\n|   18  |    ('is', 'cool', '.')     |   1   |\n|   19  |   ('cool', '.', '&lt;/s&gt;')    |   1   |\n+-------+----------------------------+-------+\n\n\nSTEP 4: Calculate trigram probabilities (MLE)\n\ntri_mle = {}\nfor (w1, w2, w3), count in trigram_counts.items():\n    tri_mle[(w1, w2, w3)] = round(count / bigrams_counts[(w1, w2)], 3)\n\nprint(\"\\nTrigram MLE:\")\ntri_mle_tab = PrettyTable([\"Word 1\", \"Word 2\", \"Word 3\", \"MLE\"])\nfor (w1, w2, w3), mle in tri_mle.items():\n    tri_mle_tab.add_row([w1, w2, w3, mle])\nprint(tri_mle_tab)\n\n\nTrigram MLE:\n+--------+--------+--------+-------+\n| Word 1 | Word 2 | Word 3 |  MLE  |\n+--------+--------+--------+-------+\n|  &lt;s&gt;   |   I    |  like  |  1.0  |\n|   I    |  like  |  NLP   |  0.5  |\n|  like  |  NLP   |   .    |  1.0  |\n|  NLP   |   .    |  &lt;/s&gt;  |  1.0  |\n|  &lt;s&gt;   |  NLP   |   is   | 0.667 |\n|  NLP   |   is   |  fun   |  0.5  |\n|   is   |  fun   |   .    |  1.0  |\n|  fun   |   .    |  &lt;/s&gt;  |  1.0  |\n|  &lt;s&gt;   |  NLP   |   in   | 0.333 |\n|  NLP   |   in   | python |  1.0  |\n|   in   | python |   is   |  0.5  |\n| python |   is   |  fun   |  1.0  |\n|   I    |  like  | coding |  0.5  |\n|  like  | coding |   in   |  1.0  |\n| coding |   in   | python |  1.0  |\n|   in   | python |   .    |  0.5  |\n| python |   .    |  &lt;/s&gt;  |  1.0  |\n|  NLP   |   is   |  cool  |  0.5  |\n|   is   |  cool  |   .    |  1.0  |\n|  cool  |   .    |  &lt;/s&gt;  |  1.0  |\n+--------+--------+--------+-------+\n\n\n(Calculating MLE with Laplace Smoothing)\n\nV = len(trigram_counts)\nk = 1  # Laplace smoothing constant\ntri_mle_laplace = {}\n\nfor (w1, w2, w3), count in trigram_counts.items():\n    tri_mle_laplace[(w1, w2, w3)] = round((count + k) / (bigrams_counts[(w1, w2)] + k * V), 3)\n\nprint(\"\\nTrigram MLE with Laplace Smoothing:\")\ntri_mle_laplace_tab = PrettyTable([\"Word 1\", \"Word 2\", \"Word 3\", \"MLE (Laplace)\"])\nfor (w1, w2, w3), mle in tri_mle_laplace.items():\n    tri_mle_laplace_tab.add_row([w1, w2, w3, mle])\nprint(tri_mle_laplace_tab)\n\n\nTrigram MLE with Laplace Smoothing:\n+--------+--------+--------+---------------+\n| Word 1 | Word 2 | Word 3 | MLE (Laplace) |\n+--------+--------+--------+---------------+\n|  &lt;s&gt;   |   I    |  like  |     0.136     |\n|   I    |  like  |  NLP   |     0.091     |\n|  like  |  NLP   |   .    |     0.095     |\n|  NLP   |   .    |  &lt;/s&gt;  |     0.095     |\n|  &lt;s&gt;   |  NLP   |   is   |      0.13     |\n|  NLP   |   is   |  fun   |     0.091     |\n|   is   |  fun   |   .    |     0.136     |\n|  fun   |   .    |  &lt;/s&gt;  |     0.136     |\n|  &lt;s&gt;   |  NLP   |   in   |     0.087     |\n|  NLP   |   in   | python |     0.095     |\n|   in   | python |   is   |     0.091     |\n| python |   is   |  fun   |     0.095     |\n|   I    |  like  | coding |     0.091     |\n|  like  | coding |   in   |     0.095     |\n| coding |   in   | python |     0.095     |\n|   in   | python |   .    |     0.091     |\n| python |   .    |  &lt;/s&gt;  |     0.095     |\n|  NLP   |   is   |  cool  |     0.091     |\n|   is   |  cool  |   .    |     0.095     |\n|  cool  |   .    |  &lt;/s&gt;  |     0.095     |\n+--------+--------+--------+---------------+\n\n\nSTEP 5: Calculate Perplexity\n\nimport math\n\ntest_trigrams = trigrams  # Reusing the training trigrams for simplicity\n# Calculate the sum of log probabilities\nlog_prob_sum = 0\nN = len(test_trigrams)  # Number of trigrams in the test set\nfor trigram in test_trigrams:\n    prob = tri_mle.get(trigram, 0)  # Get MLE probability, default to 0 if unseen\n    if prob &gt; 0:  # Avoid log(0)\n        log_prob_sum += math.log2(prob) # use log to avoid underflow issues (A numerical issue that occurs when multiplying many small probabilities together, leading to a very small probability that may be rounded to zero)\n    else:\n        print(f\"Warning: Trigram {trigram} has zero probability (unseen in training)\")\n        log_prob_sum = float(\"-inf\")  # This will lead to infinite perplexity\n        break\n\n# Calculate perplexity\nif log_prob_sum != float(\"-inf\"):\n    avg_log_prob = log_prob_sum / N\n    perplexity = 2 ** (-avg_log_prob)\nelse:\n    perplexity = float(\"inf\")\n\nprint(f\"Number of test trigrams (N): {N}\")\nprint(f\"Sum of log probabilities: {log_prob_sum:.3f}\")\nprint(f\"Average log probability: {avg_log_prob:.3f}\")\nprint(f\"Perplexity: {perplexity:.3f}\")\n\nNumber of test trigrams (N): 24\nSum of log probabilities: -8.755\nAverage log probability: -0.365\nPerplexity: 1.288\n\n\n\n\n4.1.8 Example of Trigram LM with NLTK ABC Corpus\n\nprint(type(list))\n\n&lt;class 'type'&gt;\n\n\n\nimport time\nfrom nltk.util import trigrams, bigrams\n\nimport nltk\nnltk.download('abc')\n\n\nstart_time = time.time()\nfrom nltk.corpus import abc\n\n# Load the ABC corpus\nabc_text = abc.raw(\"rural.txt\")\n\n# Step 1 get sentences from corpus\nsentences = abc.sents()[0:2000]\nprint(\"Number of sentences:\", len(sentences))\n\n# Step 2: Tokenize text and add padding tokens\ntokens = []\nfor sentence in sentences:\n    padded_sentence = [\"&lt;s&gt;\"] + [word.lower() for word in sentence] + [\"&lt;/s&gt;\"]\n    tokens.extend(padded_sentence)\nprint(\"Example tokens:\", tokens[:10])\n\n# Step 3: Generate trigrams\ntrigram_list = list(trigrams(tokens))\nbigram_list = list(bigrams(tokens))\n\nprint(\"Example trigrams:\", trigram_list[:5])\nprint(\"Example bigrams:\", bigram_list[:5])\n\n# Step 4: Count unique trigrams\ntrigram_counts = Counter(trigram_list)\nbigram_counts = Counter(bigram_list)\nunique_trigrams = len(trigram_counts)\nunique_bigrams = len(bigram_counts)\nprint(\"Unique Trigrams:\", unique_trigrams)\nprint(\"Unique Bigrams:\", unique_bigrams)\n\n# Step 5: Calculate trigram probabilities (MLE) with Laplace smoothing and k=1\nV = len(trigram_counts)\nk = 0.01\ntrigram_mle_laplace = {}\nfor (w1, w2, w3), count in trigram_counts.items():\n    trigram_mle_laplace[(w1, w2, w3)] = (count + k) / (bigram_counts[(w1, w2)] + k * V)\n\n\n# Function to predict next word based on conditional probability\ndef predict_next_word(w1, w2, trigram_mle):\n    candidates = {w3: prob for (x1, x2, w3), prob in trigram_mle.items() if x1 == w1 and x2 == w2}\n    predicted = max(candidates, key=candidates.get) if candidates else None\n    prob = candidates.get(predicted, 0.0) if predicted else 0.0\n    return predicted, prob\n\n\n# Test prediction\nw1, w2 = \"the\", \"prime\"\npredicted_word, probability = predict_next_word(w1, w2, trigram_mle_laplace)\nprint(f\"Predicted next word after '{w1} {w2}': {predicted_word}\")\nprint(f\"Probability of the next word occurring: {probability:.5f}\")\n\nend_time = time.time()\nprint(f\"Execution time: {end_time - start_time:.5f} seconds\")\n\n[nltk_data] Downloading package abc to\n[nltk_data]     C:\\Users\\roess\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package abc is already up-to-date!\n\n\nNumber of sentences: 2000\nExample tokens: ['&lt;s&gt;', 'pm', 'denies', 'knowledge', 'of', 'awb', 'kickbacks', 'the', 'prime', 'minister']\nExample trigrams: [('&lt;s&gt;', 'pm', 'denies'), ('pm', 'denies', 'knowledge'), ('denies', 'knowledge', 'of'), ('knowledge', 'of', 'awb'), ('of', 'awb', 'kickbacks')]\nExample bigrams: [('&lt;s&gt;', 'pm'), ('pm', 'denies'), ('denies', 'knowledge'), ('knowledge', 'of'), ('of', 'awb')]\nUnique Trigrams: 45447\nUnique Bigrams: 30474\nPredicted next word after 'the prime': minister\nProbability of the next word occurring: 0.02978\nExecution time: 0.24279 seconds\n\n\n\n\n4.1.9 Example of Sentence Generation with Trigram LM\n\nimport random\n\nstart_time = time.time()\n\n\ndef generate_sentence(model, max_length):\n    current_bigram = random.choice(list(model.keys()))  # Pick a random starting bigram\n    get_text = list(current_bigram)  # Initialize with the two words from the bigram\n\n    for _ in range(max_length - 2):  # Start from the third word\n        w_next_prob = model.get(tuple(get_text[-2:]), {})  # Get trigram probabilities\n        if not w_next_prob:  # If no next word, break\n            break\n        w_next = random.choices(list(w_next_prob.keys()), weights=list(w_next_prob.values()))[0]\n        get_text.append(w_next)  # Append next word\n\n    return \" \".join(get_text)  # Return generated sentence as a string\n\n\n# Example usage\ngenerated_text = generate_sentence(trigram_mle_laplace, 15)\nprint(f\"Generated sentence: {generated_text}\")\nend_time = time.time()\nprint(f\"Execution time: {end_time - start_time:.5f} seconds\")\n\nGenerated sentence: john is this\nExecution time: 0.00099 seconds",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>N-Gram Language Models</span>"
    ]
  },
  {
    "objectID": "session4.html",
    "href": "session4.html",
    "title": "5  Text Classification and Naive Bayes",
    "section": "",
    "text": "5.1 Text Classification\nThere are three main types of text classification techniques: - Supervised Learning: The model is trained on labeled data, where each document is associated with a predefined category. The model learns to map the input features to the corresponding labels. - Naive Bayes, Logistic Regression - Unsupervised Learning: The model is trained on unlabeled data, where the goal is to discover hidden patterns or clusters in the data. The model learns to group similar documents together without any predefined labels. - Latent Dirichlet Allocation (LDA), K-means clustering - Deep Learning: The model is trained on large amounts of data using deep neural networks. Deep learning models can automatically learn complex features and representations from the data, making them suitable for text classification tasks. - Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), Transformers (BERT, GPT-3)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Text Classification and Naive Bayes</span>"
    ]
  },
  {
    "objectID": "session4.html#text-classification",
    "href": "session4.html#text-classification",
    "title": "5  Text Classification and Naive Bayes",
    "section": "",
    "text": "Text classification is the process of assigning predefined categories or labels to text documents based on their content.\nIt is a supervised learning task where a model is trained on labeled data to learn the relationship between the text and its corresponding labels.\nText classification is used in various applications such as spam detection, sentiment analysis, topic categorization, and more.\nThe goal of text classification is to build a model that can accurately predict the category of unseen text documents based on their content.\nThe process of text classification involves several steps, including data preprocessing, feature extraction, model training, and evaluation.\nThe choice of features and the classification algorithm can significantly impact the performance of the model.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Text Classification and Naive Bayes</span>"
    ]
  },
  {
    "objectID": "session4.html#bayes-theorem",
    "href": "session4.html#bayes-theorem",
    "title": "5  Text Classification and Naive Bayes",
    "section": "5.2 Bayes Theorem",
    "text": "5.2 Bayes Theorem\n\nBayes theorem is a fundamental concept in probability theory that describes the relationship between conditional probabilities. It is used to update the probability of a hypothesis based on new evidence.\nThe equation is given by: \\[P(H \\mid E) = \\frac{P(E \\mid H) \\cdot P(H)}{P(E)}\\]\nWhere:\n\nP(H|E): The probability of hypothesis H given evidence E (posterior probability).\nP(E|H): The probability of evidence E given hypothesis H (likelihood).\nP(H): The prior probability of hypothesis H.\nP(E): The total probability of evidence E.\n\nNaive Assumption: The naive assumption in Naive Bayes classifiers is that the features are conditionally independent given the class label. This simplifies the computation of the posterior probability.\nThe naive assumption allows us to calculate the joint probability of the features given the class label as the product of the individual probabilities of each feature.\nAn exmple of the naive assumption is:\n\\[P(X_1, X_2, ..., X_n \\mid C) = P(X_1 \\mid C) \\cdot P(X_2 \\mid C) \\cdot ... \\cdot P(X_n \\mid C)\\]\nWhere:\n\nP(X1, X2, …, Xn | C): The joint probability of features X1, X2, …, Xn given class C.\nP(Xi | C): The probability of feature Xi given class C.\n\nThe naive assumption is a simplification that allows Naive Bayes classifiers to work well in practice, even when the features are not truly independent.\n\nTypes of Naive Bayes Classifiers: - Gaussian Naive Bayes: Assumes that the features follow a Gaussian (normal) distribution. It is suitable for continuous features. - Multinomial Naive Bayes: Assumes that the features are counts or frequencies. It is suitable for discrete features, such as word counts in text classification. - Bernoulli Naive Bayes: Assumes that the features are binary (0 or 1). It is suitable for binary features, such as presence or absence of words in text classification.\nImplementing Naive Bayes - Step 1: Load the dataset, clean and preprocess the text data. - Step 2: Split the dataset into training and testing sets. - Step 3: Extract features from the text data using techniques such as Bag of Words (BoW) - Step 4: Train the Naive Bayes classifier on the training set. - Step 5: Evaluate the classifier on the testing set using metrics such as accuracy, precision, recall, and F1-score.\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.datasets import fetch_20newsgroups\nfrom nltk.corpus import stopwords\nstop_words = set(stopwords.words(\"english\"))\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import PorterStemmer\nstemmer = PorterStemmer()\nfrom nltk import download\nfrom nltk import pos_tag\nfrom nltk import ne_chunk\nfrom nltk.tree import Tree\nfrom nltk.tokenize import word_tokenize\n\n\n# Load the dataset\nnewsgroups = fetch_20newsgroups(\n    subset=\"all\", categories=[\"alt.atheism\", \"sci.space\"], remove=(\"headers\", \"footers\", \"quotes\")\n)\ndf = pd.DataFrame({\"text\": newsgroups.data, \"label\": newsgroups.target})\n# Preprocess the text data\n\n\ndef preprocess_text(text):\n    # Tokenize the text\n    tokens = word_tokenize(text)\n    # Remove stop words\n    tokens = [word for word in tokens if word.lower() not in stop_words]\n    # Stem the words\n    tokens = [stemmer.stem(word) for word in tokens]\n    return \" \".join(tokens)\n\n\ndf[\"text\"] = df[\"text\"].apply(preprocess_text)\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(\n    df[\"text\"], df[\"label\"], test_size=0.2, random_state=42\n)\n\n# Create a pipeline with CountVectorizer and MultinomialNB\npipeline = make_pipeline(CountVectorizer(), MultinomialNB())\n\n# Train the Naive Bayes classifier\npipeline.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred = pipeline.predict(X_test)\n\n# Evaluate the classifier\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy: {accuracy:.4f}\")\n\nprint(\"Classification Report:\")\nprint(classification_report(y_test, y_pred, target_names=newsgroups.target_names))\n\nAccuracy: 0.9358\nClassification Report:\n              precision    recall  f1-score   support\n\n alt.atheism       0.90      0.96      0.93       157\n   sci.space       0.96      0.92      0.94       201\n\n    accuracy                           0.94       358\n   macro avg       0.93      0.94      0.94       358\nweighted avg       0.94      0.94      0.94       358",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Text Classification and Naive Bayes</span>"
    ]
  },
  {
    "objectID": "session4.html#evaluation-metrics",
    "href": "session4.html#evaluation-metrics",
    "title": "5  Text Classification and Naive Bayes",
    "section": "5.3 Evaluation Metrics",
    "text": "5.3 Evaluation Metrics\n\nAccuracy: The proportion of correctly classified instances out of the total instances.\n\nFormula: \\(Accuracy = \\frac{TP + TN}{TP + TN + FP + FN}\\)\nWhere:\n\nTP: True Positives\nTN: True Negatives\nFP: False Positives\nFN: False Negatives\n\n\nPrecision: The proportion of true positive predictions out of all positive predictions.\n\nFormula: \\(Precision = \\frac{TP}{TP + FP}\\)\nIndicates how many of the predicted positive instances are actually positive.\nHigh precision means fewer false positives.\n\nRecall: The proportion of true positive predictions out of all actual positive instances.\n\nFormula: \\(Recall = \\frac{TP}{TP + FN}\\)\nIndicates how many of the actual positive instances were correctly predicted.\nHigh recall means fewer false negatives.\nAlso known as Sensitivity or True Positive Rate.\nHigh precision and low recall means the model is conservative in making positive predictions.\n\nF1-Score: The harmonic mean of precision and recall. It is a single metric that balances both precision and recall.\n\nFormula: \\(F1 = 2 \\cdot \\frac{Precision \\cdot Recall}{Precision + Recall}\\)\nA high F1-score indicates a good balance between precision and recall.\nUseful when the class distribution is imbalanced, because accuracy can be misleading.\nClassification Report: A summary of precision, recall, and F1-score for each class in a multi-class classification problem.\n\n\nDisadvatages of Naive Bayes:\n\nIndependence Assumption: The naive assumption of feature independence may not hold in real-world data, leading to suboptimal performance.\nZero Probability Problem: If a feature value is not present in the training data for a particular class, the model will assign a probability of zero to that class, which can be problematic.\nProbability Estimation Bias: Naive Bayes may produce biased probability estimates, especially for rare events or classes with limited training data.\nFeature Revelance: Naive Bayes may not perform well when features are highly correlated or when the feature space is large and sparse.\nLimited Expressiveness: Naive Bayes is a linear classifier and may not capture complex relationships between features and classes.\n\nIf you are using a Naive Bayes classifier for a dataset where 95% of the tweets are from positive class. Can you suggest any method to handle this imbalance, and briefly explain how it helps improve model performance?\nAnswer: To handle class imbalance in a dataset where 95% of the tweets are from the positive class, you can use the following methods:\n\nResampling Techniques:\n\nOversampling: Increase the number of instances in the minority class (negative class) by duplicating existing instances or generating synthetic samples (e.g., using SMOTE).\nUndersampling: Decrease the number of instances in the majority class (positive class) by randomly removing instances.\nCombination: Use a combination of oversampling and undersampling to balance the classes.\nHow it helps: Resampling techniques help to create a more balanced dataset, which allows the model to learn better from both classes and reduces the bias towards the majority class. This can lead to improved performance in terms of precision, recall, and F1-score for the minority class.\n\nClass Weights: Assign higher weights to the minority class during model training. This can be done by using the class_weight parameter in scikit-learn classifiers.\n\nHow it helps: By assigning higher weights to the minority class, the model is penalized more for misclassifying instances from that class, which encourages it to focus on learning from the minority class.\n\nEnsemble Methods: Use ensemble methods such as Random Forest or Gradient Boosting, which can handle class imbalance better than individual classifiers.\n\nHow it helps: Ensemble methods combine the predictions of multiple classifiers, which can improve overall performance and robustness against class imbalance.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Text Classification and Naive Bayes</span>"
    ]
  },
  {
    "objectID": "session5.html",
    "href": "session5.html",
    "title": "6  Logistic Regression and Text Representation",
    "section": "",
    "text": "6.1 Logistic Regression\nLog-Odds:",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Logistic Regression and Text Representation</span>"
    ]
  },
  {
    "objectID": "session5.html#logistic-regression",
    "href": "session5.html#logistic-regression",
    "title": "6  Logistic Regression and Text Representation",
    "section": "",
    "text": "Logistic regression is a statistical method used for binary classification problems. It models the relationship between a binary dependent variable and one or more independent variables by estimating the probability of the dependent variable being in a particular class.\nIt is a type of regression analysis that uses the logistic function to model the probability of a binary outcome.\nLogistic regression is widely used in various fields, including social sciences, healthcare, marketing, and finance, for tasks such as predicting customer churn, disease diagnosis, and credit risk assessment.\nLogistic regression is a linear model that uses the logistic function to map the linear combination of input features to a probability value between 0 and 1.\nThe logistic function is defined as:\n\n\\[f(x) = \\frac{1}{1 + e^{-x}}\\]\nWhere:\n\nx: The linear combination of input features and their corresponding weights.\ne: The base of the natural logarithm (approximately 2.71828).\nf(x): The output probability value between 0 and 1.\nThis is known as the sigmoid function.\n\n\nThe logistic regression model can be represented as:\n\\[P(Y=1|X) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_nX_n)}}\\]\nWhere:\n\nP(Y=1|X): The probability of the dependent variable Y being equal to 1 given the input features X.\nβ0: The intercept term (bias).\nβ1, β2, …, βn: The coefficients (weights) for each input feature X1, X2, …, Xn.\n\nThe coefficients are learned during the training process using maximum likelihood estimation (MLE), which finds the values of the coefficients that maximize the likelihood of the observed data given the model.\nThe bias teram helps the model to fit the data better by allowing it to shift the decision boundary. A bias &lt;0 means the model is more likely to predict class 0, while a bias &gt;0 means the model is more likely to predict class 1.\n\n\n\nThe log-odds (or logit) is the logarithm of the odds ratio, which is the ratio of the probability of an event occurring to the probability of it not occurring.\nThe log-odds can be calculated as:\n\n\\[\\text{log-odds} = \\log\\left(\\frac{P(Y=1|X)}{1 - P(Y=1|X)}\\right) = = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_nX_n\\]\n\nThe log-odds transformation is useful because it maps the probability values (0, 1) to the entire real line (-∞, +∞), allowing for a linear relationship between the input features and the log-odds.\nThe log-odds can be interpreted as the change in the log-odds of the dependent variable for a one-unit increase in the independent variable.\nLogg odds of the probability of the positive class can be modelled as a linear function of the input features\n\n\n6.1.1 Training Logistic Regression\n\nMaximum Likelihood Estimation (MLE) is used to estimate the coefficients of the logistic regression model.\n\nThe MLE finds the values of the coefficients that maximize the likelihood of the observed data given the model.\n\nL2 regularization (Ridge) is often used to prevent overfitting by adding a penalty term to the loss function.\n\nThe regularization term helps to constrain the coefficients, reducing their variance and improving the model’s generalization to unseen data.\nThe choice of regularization strength is crucial, as too much regularization can lead to underfitting, while too little can result in overfitting.\nCross-validation is commonly used to select the optimal regularization parameter.\nIt is important to evaluate the model’s performance on a validation set to ensure that it generalizes well to new data.\n\nL1 regularization (Lasso) can also be used to perform feature selection by driving some coefficients to zero, effectively removing those features from the model.\nElastic Net is a combination of L1 and L2 regularization, allowing for both feature selection and coefficient shrinkage.\nBayesian logistic regression is another approach that incorporates prior distributions on the coefficients, allowing for uncertainty quantification and regularization.\n\nScikit-Learn’s LogisticRegression\n\nthe most commonly used solver is L2-regularized MLE with the liblinear solver.\nThe available optimization solvers are:\n\nliblinear: A coordinate descent algorithm for L1 and L2 regularization. Better for small datasets of size &lt; 10,000.\nsaga: A stochastic average gradient descent algorithm for L1 and L2 regularization. Suitable for large datasets of size &gt; 10,000.\nnewton-cg: A Newton’s method algorithm for L2 regularization.\nlbfgs: An optimization algorithm based on the limited-memory Broyden-Fletcher-Goldfarb-Shanno (BFGS) method for L2 regularization. Suitable for small to medium-sized datasets\n\n\n\n\n6.1.2 Bag of Words vs. TF-IDF\n\nBag of Words (BoW): A simple representation of text data where each document is represented as a vector of word counts. It ignores the order of words and focuses on the frequency of each word in the document.\n\nPros: Simple to implement, easy to interpret, works well for many applications.\nCons: Ignores word order, can lead to high-dimensional sparse vectors, may not capture semantic meaning.\n\nTerm Frequency-Inverse Document Frequency (TF-IDF): A more sophisticated representation of text data that considers both the frequency of words in a document and their importance across the entire corpus. It assigns higher weights to words that are frequent in a document but rare in the corpus.\nTF-IDF is calculated as:\n\\[\\text{TF-IDF}(w, d) = \\text{TF}(w, d) \\cdot \\text{IDF}(w)\\]\n\nTF(w, d): The term frequency of word w in document d.\nIDF(w): The inverse document frequency of word w, calculated as:\n\n\\[\\text{IDF}(w) = \\log\\left(\\frac{N}{DF(w)}\\right)\\]\nWhere:\n\nN: The total number of documents in the corpus.\nDF(w): The number of documents containing word w.\nIn this context documents is the number of tokens in the corpus.\n\n\nTF-IDF captures the importance of words in a document relative to the entire corpus, making it more effective for tasks such as text classification and information retrieval.\nPros: Captures word importance, reduces the impact of common words, better for semantic understanding.\nCons: More complex to implement, may still lead to high-dimensional sparse vectors.\n\nExample: In a corpus of 100 documents, the word “the” appears in 90 documents, while the word “NLP” appears in 10 documents. The TF-IDF score for “the” will be lower than that for “NLP” because “the” is common across many documents, while “NLP” is more specific to certain documents.\n\nMake table of columns: Feature | BoW | TF-IDF\n\n\n\n\n\n\n\n\nFeature\nBag of Words (BoW)\nTF-IDF\n\n\n\n\nConcept\nCounts word frequencies\nWeights word importance\n\n\nDimensionality\nHigh-dimensional sparse vectors\nHigh-dimensional sparse vectors, but weights may reduce freature noise\n\n\nComplexity\nSimple to implement\nMore complex to implement, due to IDF calculation\n\n\nWord Importance\nIgnores word importance\nConsiders word importance relative to the corpus\n\n\nContext Sensitivity\nIgnores word order and context\nIgnores word order, but captures context through IDF\n\n\nPeformance\nWorks well for many applications\nBetter for semantic understanding and information retrieval\n\n\nUse Cases\nText classification, sentiment analysis\nText classification, information retrieval, search engines\n\n\nLimitations\nIgnores word order, may lead to high-dimensional vectors\nMay still lead to high-dimensional vectors, sensitive to document length, Does not capture word order\n\n\n\n\n\n6.1.3 LR Model Assumptions\n\nLinearity: The relationship between the independent variables and the log-odds of the dependent variable is linear. This means that a one-unit change in an independent variable will result in a constant change in the log-odds of the dependent variable.\nIndependence: The observations are independent of each other. This means that the outcome of one observation does not influence the outcome of another observation.\nNo Multicollinearity: The independent variables are not highly correlated with each other. This means that the model should not include highly correlated features, as this can lead to unstable coefficient estimates and difficulty in interpreting the results.\nNo Outliers: The model assumes that there are no extreme outliers in the data that could disproportionately influence the results. Outliers can affect the estimates of the coefficients and the overall fit of the model.\n\n\n\n6.1.4 LR Limitations\n\nLinearity Assumption: Logistic regression assumes a linear relationship between the independent variables and the log-odds of the dependent variable. This may not hold true for all datasets, leading to poor performance.\nFeature Independence: Logistic regression assumes that the independent variables are independent of each other. In practice, this assumption may not hold, leading to multicollinearity issues.\nSensitivity to Outliers: Logistic regression can be sensitive to outliers, which can disproportionately influence the model’s coefficients and predictions.\nInability to Capture Complex Relationships: Logistic regression is a linear model and may not capture complex relationships between features and the target variable. Non-linear relationships may require more advanced models, such as decision trees or neural networks.\n-Requires Large Sample Size: Logistic regression requires a sufficient amount of data to produce reliable estimates. Small sample sizes may lead to overfitting and unreliable predictions. Sample size should be at least 10 times the number of features. eg if you have 5 features, you should have at least 50 samples.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Logistic Regression and Text Representation</span>"
    ]
  },
  {
    "objectID": "session6.html",
    "href": "session6.html",
    "title": "7  Sentiment Analysis",
    "section": "",
    "text": "7.1 Sentiment Analysis (SA)\nEkman’s Six Basic Emotions are: - Happiness - Sadness - Anger - Fear - Surprise - Disgust",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Sentiment Analysis</span>"
    ]
  },
  {
    "objectID": "session6.html#sentiment-analysis-sa",
    "href": "session6.html#sentiment-analysis-sa",
    "title": "7  Sentiment Analysis",
    "section": "",
    "text": "Sentiment analysis is the process of determining the sentiment or opinion expressed in a piece of text, such as positive, negative, or neutral.\nA lexicon\n\nA lexicon is a collection of words and phrases that are associated with specific sentiments, which can be used to analyze the sentiment of a given text.\nA sentiment lexicon can include words like “happy,” “sad,” “angry,” and “excited,” each associated with a specific sentiment score.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Sentiment Analysis</span>"
    ]
  },
  {
    "objectID": "session6.html#resources-for-sentiment-lexicons",
    "href": "session6.html#resources-for-sentiment-lexicons",
    "title": "7  Sentiment Analysis",
    "section": "7.2 Resources for Sentiment Lexicons",
    "text": "7.2 Resources for Sentiment Lexicons\n\nVADER (Valence Aware Dictionary and sEntiment Reasoner): A lexicon specifically designed for sentiment analysis in social media. It provides sentiment scores for words and phrases, along with rules for handling negations and intensifiers.\n\nVADER is particularly effective for analyzing short texts, such as tweets and product reviews, and is widely used in natural language processing tasks.\nVADER uses a combination of lexical features, syntactic rules, and sentiment intensity scores to determine the sentiment of a given text.\nIt is available in the NLTK library and can be easily integrated into Python applications.\nUses valence score (-1 to 1) to represent the sentiment of a word or phrase.\n\nAFINN (Affective Norms for English Words): A lexicon that provides affective ratings for English words, including valence, arousal, and dominance scores.\n\nIt is a list of 3300+ English words rated for valence (positive or negative sentiment) on a scale from -5 to +5.\nAFINN is useful for sentiment analysis tasks, particularly in social media and online reviews.\n\nSentiWordNet: A lexical resource that assigns sentiment scores to WordNet synsets. It provides positive, negative, and objective scores for each synset (group of synonymous words which are words that have similar/same meanings eg Happy and Joyful), allowing for more nuanced sentiment analysis.\n\nIt is beneficial for tasks that require understanding the sentiment of words in context.\n\nLIWC (Linguistic Inquiry and Word Count): A text analysis tool that categorizes words into psychological and linguistic categories. It provides insights into emotional, cognitive, and structural aspects of text.\n\nLIWC is widely used in psychology and social sciences for analyzing the emotional content of text data.\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel\nApproach\nNegation Handling\nSarcasm Handling\nBest Use Case\nExplanation\n\n\n\n\nTextBlob\nLexicon-based\nPoor\nVery Poor\nBasic sentiment analysis (formal text)\nUses a predefined dictionary; fails with ‘not good’ or sarcasm.\n\n\nVADER\nLexicon-based + Rules\nGood\nPartial (punctuation, caps)\nSocial media, informal text\nRecognizes negation and uses punctuation/capitalization cues for sarcasm.\n\n\nLSTM\nRNN-based Model\nVery Good\nSomewhat effective\nSentiment analysis with learned context\nLearns negation from training data but struggles with sarcasm without explicit examples.\n\n\nBERT\nTransformer-based\nExcellent\nBest\nSarcasm, nuanced sentiment, advanced NLP tasks\nContext-aware, understands negation and detects sarcasm using deep contextual learning.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Sentiment Analysis</span>"
    ]
  },
  {
    "objectID": "session6.html#how-to-do-sentiment-analysis",
    "href": "session6.html#how-to-do-sentiment-analysis",
    "title": "7  Sentiment Analysis",
    "section": "7.3 How to do Sentiment Analysis?",
    "text": "7.3 How to do Sentiment Analysis?\n\nTwo main approaches to sentiment analysis:\nRule-based: Uses predefined rules and lexicons to determine sentiment. It relies on linguistic patterns, such as negations, intensifiers, and sentiment-bearing words.\n\nExample: “I love this product!” would be classified as positive based on the presence of the word “love.”\nPros: Simple to implement, interpretable results.\nCons: Limited flexibility, may not capture complex sentiments or sarcasm.\n\nScoring in Lexicon-based sentiment analysis:\n\nPrepare a sentiment lexicon with words and their corresponding sentiment scores. Scoring can be binary, where words are classified as positive, negative, or neutral. or categorical, where words are classified into multiple categories (e.g., happy, sad, angry). or scale based, where words are assigned a score on a scale (e.g., -1 to 1).\nText preprocessing: Clean and preprocess the text data by removing stop words, punctuation, and special characters. Tokenize the text into words or phrases.\nMatch and Score: For each word in the text, check if it exists in the sentiment lexicon. If it does, retrieve its sentiment score and add it to the overall sentiment score for the text.\nCalculate the overall sentiment score by summing the individual word scores. The final sentiment score can be positive, negative, or neutral based on a predefined threshold.\nExample: If the sentiment score is greater than 0, classify the text as positive; if less than 0, classify it as negative; and if equal to 0, classify it as neutral.\n\nMachine learning-based: Uses supervised learning algorithms to train a model on labeled data. It learns to classify text based on features extracted from the text, such as word frequencies or TF-IDF scores.\nUses supervised learning algorithms, such as Naive Bayes, Logistic Regression, Support Vector Machines (SVMs), Neural Networks, to classify text into sentiment categories.\n\nExample: A model trained on positive and negative movie reviews can classify new reviews as positive or negative based on learned patterns.\nUses deep learning models, such as Recurrent Neural Networks (RNNs) or Transformers, to capture complex relationships in the text data.\n\nExample: A model trained on positive and negative movie reviews can classify new reviews as positive or negative based on learned patterns.\n\nPros: More flexible, can capture complex sentiments and context.\nCons: Requires labeled data, may be less interpretable.\n\n\n\n\n\n\n\n\n\n\nFeature\nRule-Based Sentiment Analysis\nMachine Learning-based Sentiment Analysis\n\n\n\n\nMethodology\nUses predefined rules and lexicons\nUses supervised learning algorithms\n\n\nExamples\nVADER, AFINN, LIWC\nNaive Bayes, Logistic Regression, SVMs\n\n\nAdaptability\nLimited flexibility, may not capture complex sentiments\nMore flexible, can capture complex sentiments and context\n\n\nData Requirements\nLow. Requires a sentiment lexicon\nHigh. Requires labeled training data\n\n\nContext Sensitivity\nLimited context sensitivity\nCan capture context and nuances\n\n\nLanguage evolution\nMay not adapt well to new language trends\nCan learn from new data and adapt to language evolution\n\n\nApplication\nSuitable for simple tasks\nSuitable for complex tasks and large datasets\n\n\nPrecision and Recall\nMay have lower precision and recall\nCan achieve higher precision and recall with sufficient data\n\n\n\n\nHybrid Approach: Combines rule-based and machine learning-based methods to leverage the strengths of both approaches. It uses predefined rules for initial sentiment classification and then refines the results using machine learning models.\n\nExample: A hybrid model may use VADER for initial sentiment scoring and then apply a machine learning model to improve accuracy.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Sentiment Analysis</span>"
    ]
  },
  {
    "objectID": "session6.html#python-libraries-and-tools-for-sa",
    "href": "session6.html#python-libraries-and-tools-for-sa",
    "title": "7  Sentiment Analysis",
    "section": "7.4 Python Libraries and Tools for SA",
    "text": "7.4 Python Libraries and Tools for SA\n\nNLTK (Natural Language Toolkit): A popular library for natural language processing in Python. It provides tools for text preprocessing, tokenization, stemming, and sentiment analysis using VADER and WordNet.\nTextBlob: A simple library for processing textual data. It provides a user-friendly API for common natural language processing tasks, including sentiment analysis, part-of-speech tagging, and noun phrase extraction.\nTensorFlow: An open-source machine learning library developed by Google. It provides tools for building and training deep learning models, including recurrent neural networks (RNNs) and transformers for sentiment analysis. Suited for large-scale applications and production environments.\nPyTorch: An open-source machine learning library developed by Facebook. It is widely used for deep learning applications and provides a flexible platform for building and training neural networks.\nHugging Face Transformers: A library that provides pre-trained models for natural language processing tasks, including sentiment analysis, text generation, and translation. It supports various architectures like BERT, GPT-2, and T5, making it easy to implement state-of-the-art models.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Sentiment Analysis</span>"
    ]
  },
  {
    "objectID": "session6.html#sa-challenges",
    "href": "session6.html#sa-challenges",
    "title": "7  Sentiment Analysis",
    "section": "7.5 SA Challenges",
    "text": "7.5 SA Challenges\n\nBiggest challenges in lexicon-based sentiment analysis?\nThe biggest challenge in lexicon-based sentiment analysis is the inability to capture context and sarcasm. Lexicon-based methods rely on predefined sentiment lexicons, which may not account for the nuances of language, such as:\nContextual Meaning: Words can have different meanings depending on the context in which they are used. For example, the word “sick” can be positive (e.g., “That movie was sick!”) or negative (e.g., “I feel sick”). Lexicon-based methods may misinterpret such words without considering context.\nSarcasm and Irony: Lexicon-based methods may struggle to identify sarcasm or irony, where the intended sentiment is opposite to the literal meaning of the words. For example, “Great job!” can be sarcastic in a negative context.\nAmbiguity: Some words may have multiple meanings or sentiments, leading to ambiguity in sentiment classification. For example, the word “bark” can refer to the sound a dog makes or the outer covering of a tree.\nCultural and Domain-Specific Language: Different cultures and domains may use language differently, leading to variations in sentiment expression. Lexicon-based methods may not generalize well across different contexts.\nNegation Handling: Lexicon-based methods may struggle to handle negations effectively. For example, “not good” should be classified as negative, but a simple lexicon lookup may misinterpret it as positive.\n\nAdd your notes and code examples below as you progress through the chapter.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Sentiment Analysis</span>"
    ]
  },
  {
    "objectID": "session7.html",
    "href": "session7.html",
    "title": "8  Topic Modeling",
    "section": "",
    "text": "8.1 Topic Modeling\nAs the volume of information grows, it becomes increasingly difficult to locate relevant content. Topic modeling is a technique used to uncover the hidden thematic structure in a collection of documents.\nWhen might you need to find hidden topics in text?",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Topic Modeling</span>"
    ]
  },
  {
    "objectID": "session7.html#topic-modeling",
    "href": "session7.html#topic-modeling",
    "title": "8  Topic Modeling",
    "section": "",
    "text": "Organizing large collections of documents\nSummarizing unstructured text data\nImproving information retrieval and search\nFeature selection for downstream machine learning tasks\n\n\n8.1.1 What is Topic Modeling?\n\nTopic modeling refers to a set of unsupervised machine learning techniques (such as clustering) for discovering the abstract “topics” that occur in a collection of documents.\nIt provides methods for automatically organizing, understanding, searching, and summarizing large electronic archives.\nCommon applications include document clustering, organizing large blocks of textual data, information retrieval from unstructured text, and feature selection.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Topic Modeling</span>"
    ]
  },
  {
    "objectID": "session7.html#latent-dirichlet-allocation-lda",
    "href": "session7.html#latent-dirichlet-allocation-lda",
    "title": "8  Topic Modeling",
    "section": "8.2 Latent Dirichlet Allocation (LDA)",
    "text": "8.2 Latent Dirichlet Allocation (LDA)\nLatent Dirichlet Allocation (LDA) is a foundational topic modeling technique. Let’s break down the name:\n\nLatent: Hidden or underlying (the topics are not directly observed).\nDirichlet: Refers to the Dirichlet distribution, which models the distribution of topics in documents and words in topics.\nAllocation: Assigning topics to words in documents.\n\n\n\n8.2.1 LDA Intuition\nLDA is a generative model: it assumes that documents are created by mixing topics, and each topic is a collection of words.\n\nEach topic is a distribution over words.\nEach document is a mixture of topics.\nEach word in a document is drawn from one of the document’s topics.\n\nIn short:\nDocuments are treated as bags of words. Each document is generated by a mixture of topics, and each topic is a mixture of words. The goal of LDA is to uncover these hidden topics from the observed words.\n\n\n\n8.2.2 How LDA Works\n\nFor each document, choose a distribution over topics.\nFor each word in the document:\n\nPick a topic from the document’s topic distribution.\nPick a word from the topic’s word distribution.\n\n\nLDA tries to reverse-engineer this process: given the words, it infers the topics.\n\n\n\n8.2.3 Key Components\n\nWord distribution per topic (β_k): Which words are likely in each topic.\nTopic distribution per document (θ_d): Which topics are likely in each document.\nTopic assignment for each word (z_{d,n}): Which topic generated each word.\n\n\n\n\n\n\n\nFigure 8.1: LDA as a graphical model\n\n\n\n\n\n\n8.2.4 The Trade-off: Document vs. Topic Sparsity\nLDA balances two goals:\n\nDocument sparsity: Each document should use as few topics as possible.\nTopic sparsity: Each topic should use as few words as possible.\n\nThese are in tension:\n- If all words in a document come from one topic, that topic must cover many words (less topic sparsity). - If each topic uses only a few words, documents must use more topics (less document sparsity).\nLDA finds a balance, resulting in topics that best explain the documents.\n\n\n\n8.2.5 LDA Parameters\n\nDocument density factor (α): Controls how many topics are expected per document.\nTopic word density factor (β): Controls how many words are expected per topic.\nNumber of topics (K): How many topics to extract.\n\n\n\n\n8.2.6 Parameter Estimation in LDA\nLDA uses Bayes’ Theorem to estimate the hidden variables (topics):\n\\[\nP(\\text{hypothesis} \\mid \\text{observations}) = \\frac{P(\\text{observations} \\mid \\text{hypothesis}) \\cdot P(\\text{hypothesis})}{P(\\text{observations})}\n\\]\n\nPrior: What we believe before seeing the data.\nPosterior: What we believe after seeing the data.\n\nKey challenge:\nComputing the exact posterior is intractable.\nCommon solutions: - Direct methods: Expectation Maximization, Variational Inference, Expectation Propagation. - Indirect methods: Estimate the posterior using sampling (e.g., Gibbs sampling, a type of Markov Chain Monte Carlo).\n# Python Example: LDA with scikit-learn and gensim\nimport numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation\nfrom sklearn.datasets import fetch_20newsgroups\nimport gensim\nfrom gensim import corpora\nimport pyLDAvis.gensim_models\nimport pyLDAvis\n\n# Example 1: LDA with scikit-learn\n# Load sample data\nnewsgroups = fetch_20newsgroups(subset='train', \n                               categories=['alt.atheism', 'sci.space', 'rec.sport.hockey'],\n                               remove=('headers', 'footers', 'quotes'))\n\n# Preprocess and vectorize\nvectorizer = CountVectorizer(max_features=1000, stop_words='english', \n                           min_df=2, max_df=0.95)\ndoc_term_matrix = vectorizer.fit_transform(newsgroups.data)\n\n# Fit LDA model\nlda = LatentDirichletAllocation(n_components=3, random_state=42, \n                              max_iter=10, learning_method='online')\nlda.fit(doc_term_matrix)\n\n# Display top words per topic\nfeature_names = vectorizer.get_feature_names_out()\nfor topic_idx, topic in enumerate(lda.components_):\n    top_words = [feature_names[i] for i in topic.argsort()[-10:]]\n    print(f\"Topic {topic_idx}: {', '.join(top_words)}\")\n\n# Example 2: LDA with gensim (more advanced)\n# Prepare documents\ntexts = [doc.split() for doc in newsgroups.data[:100]]  # Simple tokenization\ndictionary = corpora.Dictionary(texts)\ncorpus = [dictionary.doc2bow(text) for text in texts]\n\n# Train LDA model\nlda_model = gensim.models.LdaModel(corpus=corpus, id2word=dictionary, \n                                 num_topics=3, random_state=42,\n                                 alpha='auto', per_word_topics=True)\n\n# Print topics\nfor idx, topic in lda_model.print_topics(-1):\n    print(f'Topic {idx}: {topic}')\n\n# Get document-topic probabilities\ndoc_topics = lda_model[corpus[0]]\nprint(f\"Document 0 topic distribution: {doc_topics[0]}\")\n# Visualize (requires pyLDAvis)\nvis = pyLDAvis.gensim_models.prepare(lda_model, corpus, dictionary)\npyLDAvis.display(vis)\n\n\n\n8.2.7 Gibbs Sampling in LDA\nGibbs sampling is a Markov Chain Monte Carlo (MCMC) method used to estimate the posterior distribution of the hidden variables in LDA, specifically the topic assignments for each word.\n\n8.2.7.1 What does Gibbs sampling do in LDA?\n\nIt generates samples from the joint probability distribution of the topic assignments for all words in all documents.\nFor each word, it samples a new topic assignment based on the current state of all other assignments.\nThis iterative process gradually approximates the true posterior distribution.\n\n\n\n8.2.7.2 The Gibbs Sampling Algorithm\n\nInitialization:\nRandomly assign each word in each document to one of the K topics.\nIterative Sampling:\nFor each word ( w_{d,n} ) in document ( d ):\n\nRemove the current topic assignment for ( w_{d,n} ).\nCompute the probability of assigning topic ( k ) to ( w_{d,n} ) as: [ P(z_{d,n} = k z_{-}, w) ] where:\n\n( n_{d,k}^{-n} ): Number of words in document ( d ) assigned to topic ( k ), excluding the current word.\n( n_{k,w}^{-n} ): Number of times word ( w ) is assigned to topic ( k ), excluding the current word.\n( ), ( ): Dirichlet priors.\n( K ): Number of topics.\n( V ): Vocabulary size.\n\nSample a new topic for ( w_{d,n} ) from this distribution.\nUpdate the counts accordingly.\n\nRepeat:\nIterate over all words multiple times (epochs) until the topic assignments stabilize.\n\n\n\n8.2.7.3 Intuition\n\nDocument-topic counts encourage each document to use as few topics as possible (document sparsity).\nTopic-word counts encourage each topic to use as few words as possible (topic sparsity).\nOver many iterations, the assignments converge to a good approximation of the true topic structure.\n\n\n\n8.2.7.4 Summary\nGibbs sampling provides an efficient way to estimate the hidden topic structure in LDA by iteratively updating topic assignments for each word based on the current state of the model.\n\n\n\n\n8.2.8 Practical Considerations for LDA\n\nSimilar to k-means clustering: LDA is unsupervised and requires the number of topics to be specified in advance.\nBag-of-words assumption: Ignores word order and semantic context.\nPreprocessing is crucial: Good results require cleaning (stop words, stemming, lemmatization, etc.).\nNot ideal for short texts: LDA struggles with very short documents (e.g., tweets, headlines).\n\n\n\n\n8.2.9 Visualizing and Using LDA Results\n\nDocument-topic probabilities: Each document can be represented by its topic probability vector.\nWord-topic assignments: Each word in a document can be colored or labeled by its assigned topic.\nDocument similarity: Documents are similar if they have similar topic distributions (e.g., using Euclidean distance).",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Topic Modeling</span>"
    ]
  },
  {
    "objectID": "session7.html#top2vec-and-bertopic",
    "href": "session7.html#top2vec-and-bertopic",
    "title": "8  Topic Modeling",
    "section": "8.3 Top2Vec and BERTopic",
    "text": "8.3 Top2Vec and BERTopic\n\n8.3.1 Top2Vec: Distributed Topic Representations\nTop2Vec is a modern topic modeling algorithm (arXiv 2020, GitHub: ddangelov/Top2Vec) that automatically discovers topics in text data without requiring you to specify the number of topics in advance. It also works without extensive preprocessing (e.g., stop word removal, stemming, or lemmatization).\n\n8.3.1.1 Key Ideas\n\nVector Space Representation:\n\nEach document and word is embedded into a high-dimensional vector space.\nTerms are axes; documents and words are points or vectors.\n\nDimensionality Reduction:\n\nUMAP is used to reduce the dimensionality of the embeddings, preserving semantic relationships (using cosine similarity).\n\nClustering:\n\nHDBSCAN clusters the reduced vectors to identify topic groups.\n\nTopic Vectors:\n\nFor each cluster, a topic vector is calculated.\nThe closest words to each topic vector define the topic.\n\n\n\n\n8.3.1.2 Top2Vec Workflow\n\nEmbed documents and words (e.g., using doc2vec or word2vec).\nReduce dimensionality (UMAP).\nCluster embeddings (HDBSCAN).\nCalculate topic vectors for each cluster.\nFind closest words to each topic vector to define topics.\n\n\n\n\n\n\n\nFigure 8.2: Top2Vec\n\n\n\n\n\n8.3.1.3 Hyperparameters\n\nmin_count: Minimum word frequency to include in the model.\nembedding_model: Embedding method (e.g., ‘doc2vec’, ‘word2vec’).\numap_args: UMAP settings for dimensionality reduction.\nhdbscan_args: HDBSCAN clustering settings.\nvector_size: Size of embedding vectors.\n\n\n\n8.3.1.4 Pros and Cons\nPros: - Automatically determines the number of topics. - Preserves semantic meaning better than traditional models. - Minimal preprocessing required.\nCons: - Computationally intensive—use small datasets for initial testing. - Many hyperparameters to tune; results can vary. - Requires experimentation to achieve optimal results.\n# Python Example: Top2Vec\n\nfrom top2vec import Top2Vec\nfrom sklearn.datasets import fetch_20newsgroups\nimport numpy as np\n\n# Load sample data\nnewsgroups = fetch_20newsgroups(subset='train', \n                               categories=['alt.atheism', 'sci.space', 'rec.sport.hockey'],\n                               remove=('headers', 'footers', 'quotes'))\n\n# Take a smaller subset for faster processing\ndocuments = newsgroups.data[:200]  # Use only 200 documents for demo\n\n# Create Top2Vec model\n# Note: This may take several minutes to run\nmodel = Top2Vec(documents, \n                embedding_model='universal-sentence-encoder',  # Easier to use, no C compiler needed\n                speed='fast',  # Options: 'fast', 'deep', 'learn'\n                workers=1)\n\n# Get number of topics found\nnum_topics = model.get_num_topics()\nprint(f\"Number of topics found: {num_topics}\")\n\n# Get topic words for each topic\ntopic_words, word_scores, topic_nums = model.get_topics()\n\n# Display topics\nfor i, topic_num in enumerate(topic_nums):\n    print(f\"\\nTopic {topic_num}:\")\n    print(f\"Words: {', '.join(topic_words[i][:10])}\")  # Show top 10 words\n\n# Get topic for a specific document\ndoc_topics, doc_scores, doc_ids = model.search_documents_by_topic(topic_num=0, num_docs=3)\nprint(f\"\\nSample documents for Topic 0:\")\nfor i, doc_id in enumerate(doc_ids):\n    print(f\"Doc {doc_id}: {documents[doc_id][:100]}...\")\n\n# Search for similar documents using keywords\ntry:\n    doc_topics, doc_scores, doc_ids = model.search_documents_by_keywords(keywords=[\"space\", \"NASA\"], num_docs=3)\n    print(f\"\\nDocuments similar to 'space, NASA':\")\n    for i, doc_id in enumerate(doc_ids):\n        print(f\"Score: {doc_scores[i]:.3f} - {documents[doc_id][:100]}...\")\nexcept:\n    print(\"No documents found for these keywords\")\n\n\n\n\n8.3.2 BERTopic\nBERTopic is another modern topic modeling technique inspired by Top2Vec, but leverages transformer-based embeddings (e.g., BERT).\n\n8.3.2.1 Key Features\n\nNo centroid calculation:\nEach document in a cluster is treated as unique.\nTopic extraction:\nUses class-based TF-IDF to extract topic words from clusters.\nTransformer-based:\nUtilizes contextual embeddings for improved topic quality, especially in nuanced or short texts.\nComputational cost:\nMore resource-intensive due to transformer models.\n\n\n\n\n\n8.3.3 K-means Clustering with Word2Vec\n\nEmbed documents using Word2Vec.\nCluster the embeddings using k-means.\nBest for short texts (e.g., tweets, headlines).\n\n\nSummary Table\n\n\n\n\n\n\n\n\n\n\n\nMethod\nNeeds # Topics?\nPreprocessing\nHandles Short Texts\nComputational Cost\nNotes\n\n\n\n\nLDA\nYes\nYes\nNo\nModerate\nClassic, interpretable\n\n\nTop2Vec\nNo\nNo\nYes\nHigh\nFinds topics automatically\n\n\nBERTopic\nNo\nNo\nYes\nVery High\nTransformer-based\n\n\nK-means+W2V\nYes\nYes\nYes\nLow/Moderate\nSimple, fast\n\n\n\n\nPractical Tips: - Start with small datasets to experiment with Top2Vec or BERTopic. - Tune hyperparameters for your specific data and task. - Consider computational resources—transformer-based models can be slow. - Use visualizations to interpret and validate discovered topics.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Topic Modeling</span>"
    ]
  },
  {
    "objectID": "session8.html",
    "href": "session8.html",
    "title": "9  Lexical Semantics and Vector Embeddings",
    "section": "",
    "text": "9.1 Word Meanings\nUnderstanding word meanings is fundamental in NLP. Here are some key points:",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Lexical Semantics and Vector Embeddings</span>"
    ]
  },
  {
    "objectID": "session8.html#word-meanings",
    "href": "session8.html#word-meanings",
    "title": "9  Lexical Semantics and Vector Embeddings",
    "section": "",
    "text": "Words have meanings: But meanings can be complex and context-dependent.\nSome words are similar to others: For example, “car” and “automobile”.\n\n\n9.1.1 Challenges in Defining Word Meanings\n\nHomonymy: Words with multiple unrelated meanings (e.g., “bank” as a financial institution vs. “bank” of a river).\nPolysemy: Words with multiple related senses (e.g., “paper” as material vs. “paper” as an academic article).\nMany-to-many mapping: Concepts can be associated with multiple words, and words can represent multiple concepts.\n\n\n\n9.1.2 Word Relations\n\nSynonymy: Words with similar meanings (e.g., “big” and “large”).\nAntonymy: Words with opposite meanings (e.g., “hot” and “cold”).\nSimilarity: Degree to which words are alike (e.g., “cup” and “mug”).\nRelatedness: Words that are related but not necessarily similar (e.g., “doctor” and “hospital”).\nConnotation: Emotional or cultural association (e.g., “childish” vs. “youthful”).",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Lexical Semantics and Vector Embeddings</span>"
    ]
  },
  {
    "objectID": "session8.html#vector-semantics",
    "href": "session8.html#vector-semantics",
    "title": "9  Lexical Semantics and Vector Embeddings",
    "section": "9.2 Vector Semantics",
    "text": "9.2 Vector Semantics\nVector semantics represents words and documents as points in a multidimensional space.\n\n9.2.1 Idea 1: Defining Meaning by Linguistic Distribution\n\n“You shall know a word by the company it keeps.”\n— J.R. Firth\n\nWords that appear in similar contexts tend to have similar meanings.\n\n\n9.2.2 Idea 2: Meaning as a Point in Multidimensional Space\nEach word or document is represented as a vector of numbers.\n\n9.2.2.1 Example: Term-Document Matrix\n\n\n\n\nDoc1\nDoc2\nDoc3\n\n\n\n\napple\n2\n0\n1\n\n\norange\n1\n1\n0\n\n\nbank\n0\n2\n1\n\n\n\nEach row is a word, each column is a document, and the numbers are word counts.\n\n\n\nVisualizing Document Vectors\n\n\n\n\n9.2.2.2 Comparing Document Vectors\nSuppose we have two document vectors:\n- Doc1: [2, 1, 0]\n- Doc2: [0, 1, 2]\nWe can measure their similarity using various metrics.\n\n\n9.2.2.3 Example: Word Context Matrix\n\n\n\n\ncontext1\ncontext2\ncontext3\n\n\n\n\nbank\n3\n0\n2\n\n\nriver\n0\n2\n3\n\n\nmoney\n2\n3\n0\n\n\n\n\n\n\n\n9.2.3 Comparing Word Vectors\n\nCosine Similarity: Measures the angle between two vectors (e.g., “king” and “queen” have high cosine similarity).\n\n\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport numpy as np\n\nvec_king = np.array([0.5, 0.8, 0.1])\nvec_queen = np.array([0.51, 0.79, 0.12])\n\ncos_sim = cosine_similarity([vec_king], [vec_queen])\nprint(\"Cosine Similarity:\", cos_sim[0][0])\n\nCosine Similarity: 0.9996667100448761\n\n\n\nEuclidean Distance: Measures the straight-line distance between vectors.\n\n\nfrom scipy.spatial.distance import euclidean\nimport numpy as np\n\nvec_king = np.array([0.5, 0.8, 0.1])\nvec_queen = np.array([0.51, 0.79, 0.12])\n\ndist = euclidean(vec_king, vec_queen)\nprint(\"Euclidean Distance:\", dist)\n\nEuclidean Distance: 0.02449489742783178\n\n\n\nDot Product: Measures similarity based on direction and magnitude.\n\n\nimport numpy as np\n\nvec_king = np.array([0.5, 0.8, 0.1])\nvec_queen = np.array([0.51, 0.79, 0.12])\n\ndot = np.dot(vec_king, vec_queen)\nprint(\"Dot Product:\", dot)\n\nDot Product: 0.8990000000000001\n\n\nThese metrics help quantify how similar or related two words or documents are in vector space.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Lexical Semantics and Vector Embeddings</span>"
    ]
  },
  {
    "objectID": "session8.html#word2vec",
    "href": "session8.html#word2vec",
    "title": "9  Lexical Semantics and Vector Embeddings",
    "section": "9.3 Word2Vec",
    "text": "9.3 Word2Vec\nWord2Vec is a popular algorithm for learning word embeddings—compact, dense vectors that capture the meanings and relationships of words.\n\n9.3.1 From Sparse to Dense Vectors\n\nSparse Vectors: Traditional representations like one-hot or bag-of-words use high-dimensional vectors with mostly zeros.\n\nExample: One-hot for “apple” in a 10,000-word vocabulary: [0, 0, ..., 1, ..., 0].\nExample: Bag-of-words for a document: [2, 0, 0, 1, 0, ..., 0].\n\nDense Vectors: Word2Vec learns low-dimensional vectors (e.g., 50–300 dimensions) where most values are nonzero.\n\nExample: Word2Vec for “apple”: [0.12, -0.08, 0.44, ..., 0.03].\n\n\nDense vectors place similar words close together in space, making it easier to capture semantic similarity.\n\n\n\n9.3.2 How Word2Vec Works\nWord2Vec can use the skip-gram approach to learn word relationships from text:\n\nSkip-Gram Objective: For each word (the target) in a sentence, predict its nearby words (the context). - Example sentence:\n\"The bank of the river was flooded.\" - If the target is \"bank\", and the window size is 2, the context words are: \"the\", \"of\", \"river\", \"was\".\nCreating Training Pairs: The model creates (target, context) pairs from the text. - Example pairs: | Target | Context | |——–|———| | bank | the | | bank | of | | bank | river | | bank | was |\nPositive and Negative Examples: - Positive pairs: Real (target, context) pairs from the text. - Negative pairs: Randomly combine the target with words that do not appear in its context window.\nLearning Process: - The model uses a simple neural network/logistic regression to distinguish positive pairs from negative ones. - The goal is to maximize the probability of positive pairs while minimizing the probability of negative pairs. - The model adjusts the word vectors so that words appearing in similar contexts have similar vectors.\nResult: Each word is represented by a vector that reflects its meaning and relationships to other words.\n\n\nSelf-supervised: Word2Vec learns directly from raw text, without any manual labeling.\n\n\n\n\n9.3.3 Why Dense Embeddings Are Powerful\nDense vectors capture subtle relationships between words: - Words like \"king\" and \"queen\" have similar vectors. - You can do vector arithmetic to solve analogies:\nvec(\"king\") - vec(\"man\") + vec(\"woman\") ≈ vec(\"queen\")\nThese embeddings are much more effective for NLP tasks than sparse representations.\n\n\n\n9.3.4 Word2Vec at a Glance\n\n\n\n\n\n\n\nConcept\nDescription\n\n\n\n\nEmbedding\nEach word gets a dense vector\n\n\nSkip-Gram\nPredict context words from a target word\n\n\nPositive Example\n(target, context) from real text\n\n\nNegative Example\n(target, random word) not in context\n\n\nTraining\nMake real pairs similar, random pairs dissimilar\n\n\nResult\nVectors encode word meaning and relationships\n\n\n\n\n\n9.3.4.1 Analogy Reasoning with Embeddings\nWord2Vec embeddings can solve analogies using vector math. For example:",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Lexical Semantics and Vector Embeddings</span>"
    ]
  },
  {
    "objectID": "session8.html#takeaways",
    "href": "session8.html#takeaways",
    "title": "9  Lexical Semantics and Vector Embeddings",
    "section": "9.4 Takeaways",
    "text": "9.4 Takeaways\n\n9.4.1 Key Takeaways\n\nVector Semantics:\nSummarizes word contexts as dense vectors, enabling mathematical operations on word meanings.\nWord2Vec:\n\nProvides powerful word representations for many NLP tasks (e.g., translation, sentiment analysis, question answering).\nUses the skip-gram word prediction method to learn embeddings.\nEmbeddings capture abstract relationships (e.g., male–female, capital–city, comparative–superlative).\nNote: Word2Vec produces static embeddings—each word has a single vector, regardless of context.\n\n\n\n9.4.1.1 Practical Steps\n\nLoad pre-trained Word2Vec embeddings.\nExplore vector similarities to find related words.\nSolve analogies (e.g., “man” is to “king” as “woman” is to ?).\nCompute sentence embeddings (e.g., by averaging word vectors).\nUse embeddings for sentence classification and compare with Bag-of-Words (BoW) models.\n\nAdd your notes and code examples below as you progress through the chapter.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Lexical Semantics and Vector Embeddings</span>"
    ]
  },
  {
    "objectID": "session9.html",
    "href": "session9.html",
    "title": "10  Simple Neural Networks",
    "section": "",
    "text": "10.1 Ambiguity\nAmbiguity arises when a sentence can be interpreted in more than one way. For example:\nAmbiguity is a significant challenge in NLP, as it can lead to multiple interpretations of the same sentence.\nThere are several types of ambiguity, including:\nTo address ambiguity, we need models that capture sentence structure, such as:",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Simple Neural Networks</span>"
    ]
  },
  {
    "objectID": "session9.html#ambiguity",
    "href": "session9.html#ambiguity",
    "title": "10  Simple Neural Networks",
    "section": "",
    "text": "“I saw the man with the telescope.”\n\nWho has the telescope—the speaker or the man?\n\n\n\n\n\nLexical ambiguity: When a word has multiple meanings.\nStructural (syntactic) ambiguity: When a sentence can be parsed in different ways.\n\n\n\nParse trees\nDependency graphs\nOther structural representations",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Simple Neural Networks</span>"
    ]
  },
  {
    "objectID": "session9.html#simple-neural-networks-and-neural-language-models",
    "href": "session9.html#simple-neural-networks-and-neural-language-models",
    "title": "10  Simple Neural Networks",
    "section": "10.2 Simple Neural Networks and Neural Language Models",
    "text": "10.2 Simple Neural Networks and Neural Language Models\nNeural networks are powerful models that consist of layers of simple computational units. Each unit takes a vector of input values, applies weights and a bias, and produces a single output value through an activation function.\n\n10.2.1 Structure of a Simple Neural Network\nA typical neural network consists of:\n\nInput layer: Receives the input features.\nWeights: Each input is multiplied by a corresponding weight.\nWeighted sum: The weighted inputs are summed, and a bias term is added.\nActivation function: The sum is passed through a non-linear function.\nOutput layer: Produces the final prediction.\n\n\n10.2.1.1 Mathematical Formulation\nGiven input vector x = (x₁, x₂, …, xₙ), weight vector w = (w₁, w₂, …, wₙ), and bias b, the unit computes:\n\\[\nz = \\mathbf{w} \\cdot \\mathbf{x} + b\n\\]\nThe output y is then:\n\\[\ny = a = f(z)\n\\]\nwhere f is the activation function.\n\n\n\n10.2.2 Example Walkthrough\nSuppose we have a unit with weights w = [0.5, -0.3], bias b = 0.1, and input x = [2, 3].\n\nWeighted sum: \\[\nz = (0.5 \\times 2) + (-0.3 \\times 3) + 0.1 = 1.0 - 0.9 + 0.1 = 0.2\n\\]\nActivation function:\nUsing the sigmoid function: \\[\n\\text{sigmoid}(z) = \\frac{1}{1 + e^{-z}} = \\frac{1}{1 + e^{-0.2}} \\approx 0.55\n\\]\n\nSo, the output of this unit is approximately 0.55.\n\n\n10.2.3 Common Activation Functions\n\nSigmoid:\n\\[\n\\sigma(z) = \\frac{1}{1 + e^{-z}}\n\\] Maps output to (0, 1). Useful for probabilities, but can cause vanishing gradients.\nTanh:\n\\[\n\\tanh(z) = \\frac{e^{z} - e^{-z}}{e^{z} + e^{-z}}\n\\] Maps output to (-1, 1). Zero-centered and often preferred over sigmoid.\nReLU (Rectified Linear Unit):\n\\[\n\\text{ReLU}(z) = \\max(0, z)\n\\] Simple and effective. Avoids vanishing gradients for positive values.\n\n\n\n10.2.4 Why Activation Functions Matter\nActivation functions introduce non-linearity, allowing neural networks to model complex relationships. For example, ReLU is widely used because it is computationally efficient and helps mitigate the vanishing gradient problem, which can occur with sigmoid or tanh for large input values.\n\n\n\n\nNeural Network Unit\n\n\nFigure: A single neural network unit computes a weighted sum of its inputs, adds a bias, and applies an activation function to produce its output.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Simple Neural Networks</span>"
    ]
  },
  {
    "objectID": "session9.html#the-power-of-multi-layer-neural-networks-xor-example",
    "href": "session9.html#the-power-of-multi-layer-neural-networks-xor-example",
    "title": "10  Simple Neural Networks",
    "section": "10.3 The Power of Multi-Layer Neural Networks: XOR Example",
    "text": "10.3 The Power of Multi-Layer Neural Networks: XOR Example\nA single neural unit (perceptron) can compute simple logical functions like AND and OR, but not XOR. This is because XOR is not linearly separable—a single line cannot separate its positive and negative cases.\n\n10.3.1 Truth Tables\n\n\n\nx₁\nx₂\nAND\nOR\nXOR\n\n\n\n\n0\n0\n0\n0\n0\n\n\n0\n1\n0\n1\n1\n\n\n1\n0\n0\n1\n1\n\n\n1\n1\n1\n1\n0\n\n\n\n\n\n10.3.2 Perceptron as Linear Classifier\nA perceptron computes: \\[\ny = f(\\mathbf{w} \\cdot \\mathbf{x} + b)\n\\] where \\(f\\) is a step function. For AND and OR, weights and bias can be chosen so that the perceptron outputs the correct result. For XOR, no single line (decision boundary) can separate the outputs.\n\n\n10.3.3 Why XOR Needs Multiple Layers\n\nAND/OR: Can be separated by a line (linear).\nXOR: Requires a curve or multiple lines (non-linear).\n\n\n\n10.3.4 Multi-Layer Solution for XOR\nA two-layer network with ReLU activation can compute XOR. For example:\n\nHidden layer: 2 units\nOutput layer: 1 unit\n\nSuppose we choose weights and biases so that for input \\(\\mathbf{x} = [0, 0]\\), the hidden layer outputs \\([0, 0]\\) and the final output is 0. For other inputs, the network can be set up so that the output is 1 for \\([0, 1]\\) and \\([1, 0]\\), and 0 for \\([1, 1]\\).\nExercise: Try computing the outputs for all input pairs to see how the network solves XOR.\n\n\n10.3.5 Key Takeaway\nMulti-layer neural networks with non-linear activation functions can learn complex functions (like XOR) by forming new representations in hidden layers. This ability to learn useful representations is a major strength of neural networks.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Simple Neural Networks</span>"
    ]
  },
  {
    "objectID": "session9.html#feedforward-neural-networks",
    "href": "session9.html#feedforward-neural-networks",
    "title": "10  Simple Neural Networks",
    "section": "10.4 Feedforward Neural Networks",
    "text": "10.4 Feedforward Neural Networks\nFeedforward neural networks are the foundation of many modern neural architectures. While more complex models like RNNs and Transformers are widely used in NLP, the simple feedforward (or multilayer perceptron, MLP) network remains a crucial building block.\n\n10.4.1 What is a Feedforward Neural Network?\nA feedforward network consists of multiple layers of units (neurons) where information flows in one direction—from input to output—without cycles or loops. Each layer receives inputs from the previous layer and passes outputs to the next.\n\nInput layer: Receives the raw input features.\nHidden layers: Transform the input through learned weights and activation functions.\nOutput layer: Produces the final prediction (e.g., class probabilities).\n\n\nNote: Historically, “multilayer perceptron” (MLP) refers to these networks, though modern MLPs use activation functions like ReLU or tanh, not the original perceptron step function.\n\n\n\n10.4.2 Structure and Computation\nLet’s revisit logistic regression. Binary logistic regression can be viewed as a 1-layer network (not counting the input layer):\n\nInput: Vector \\(\\mathbf{x}\\)\nWeights: Vector \\(\\mathbf{w}\\)\nBias: Scalar \\(b\\)\nOutput: \\(y = \\sigma(\\mathbf{w} \\cdot \\mathbf{x} + b)\\), where \\(\\sigma\\) is the sigmoid function.\n\n\n10.4.2.1 Example: Logistic Regression as a Neural Network\nSuppose \\(\\mathbf{x} = [2, 3]\\), \\(\\mathbf{w} = [0.5, -0.3]\\), \\(b = 0.1\\):\n\\[\nz = (0.5 \\times 2) + (-0.3 \\times 3) + 0.1 = 0.2 \\\\\ny = \\sigma(0.2) \\approx 0.55\n\\]\n\n\n10.4.2.2 Adding a Hidden Layer\nA feedforward network with one hidden layer computes:\n\nHidden layer:\n\\[\n\\mathbf{h} = g(\\mathbf{W} \\mathbf{x} + \\mathbf{b})\n\\] where \\(g\\) is an activation function (e.g., ReLU, tanh), \\(\\mathbf{W}\\) is the weight matrix, and \\(\\mathbf{b}\\) is the bias vector.\nOutput layer:\n\\[\n\\mathbf{z} = \\mathbf{U} \\mathbf{h}\n\\] \\[\n\\mathbf{y} = \\text{softmax}(\\mathbf{z})\n\\] For binary classification, use sigmoid instead of softmax.\n\n\n\n10.4.2.3 Example: Feedforward Network for Classification\nSuppose we have:\n\nInput \\(\\mathbf{x} \\in \\mathbb{R}^3\\)\nHidden layer: 2 units, ReLU activation\nOutput layer: 3 units (for 3 classes), softmax activation\n\nLet: - \\(\\mathbf{W}\\) is \\(2 \\times 3\\) (hidden layer weights) - \\(\\mathbf{U}\\) is \\(3 \\times 2\\) (output layer weights) - \\(\\mathbf{b}\\) is \\(2 \\times 1\\) (hidden layer bias)\nThe computation is:\n\nHidden layer: \\[\n\\mathbf{h} = g(\\mathbf{W} \\mathbf{x} + \\mathbf{b})\n\\] where \\(g\\) is the ReLU activation function.\nOutput layer: \\[\n\\mathbf{z} = \\mathbf{U} \\mathbf{h}\n\\] \\[\n\\mathbf{y} = \\text{softmax}(\\mathbf{z})\n\\]\n\n\n\n\nFeedforward Neural Network",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Simple Neural Networks</span>"
    ]
  },
  {
    "objectID": "session9.html#applying-feedforward-networks-to-nlp-tasks",
    "href": "session9.html#applying-feedforward-networks-to-nlp-tasks",
    "title": "10  Simple Neural Networks",
    "section": "10.5 Applying Feedforward Networks to NLP Tasks",
    "text": "10.5 Applying Feedforward Networks to NLP Tasks\nFeedforward neural networks can be applied to a variety of NLP tasks. Let’s look at two common examples: text classification and language modeling.\n\n10.5.1 1. Text Classification\nSuppose we want to classify text (e.g., movie reviews as positive or negative). Traditionally, we might use logistic regression with binary features (e.g., word presence). With a feedforward network, we can do more:\n\nInput layer: Each word is represented as a vector (embedding), not just a binary feature.\nHidden layer: Allows the network to learn non-linear interactions between features.\nOutput layer: Produces a probability (e.g., positive or negative sentiment).\n\nWhy add a hidden layer?\nA hidden layer enables the network to capture complex patterns and interactions between words, which may improve performance over simple linear models.\nExample:\nSuppose our input is a review:\n\"The movie was surprisingly good.\"\nEach word is mapped to an embedding. The network can learn to focus on words like “surprisingly” and “good” to predict a positive sentiment.\n\n\n10.5.2 2. Learning Features Automatically\nA key strength of deep learning is the ability to learn features from data rather than relying on hand-crafted features. Hidden nodes can learn abstract representations by focusing on specific patterns in the embeddings.\n\nInstead of manually designing features, the network discovers useful patterns during training.\nThis is fundamental to the power of deep learning for NLP.\n\n\n\n10.5.3 3. Handling Variable-Length Input\nFeedforward networks expect fixed-size input. In NLP, sentences and documents vary in length. Some common solutions:\n\nPadding: Pad shorter inputs with zeros to match the length of the longest input.\nTruncation: Cut longer inputs to a fixed length.\nPooling: Create a single “sentence embedding” by combining word embeddings, e.g.:\n\nMean pooling: Take the average of all word embeddings.\nMax pooling: For each dimension, take the maximum value across all words.\n\n\n\n\n10.5.4 4. Multi-Class Classification\nIf you have more than two output classes (e.g., topic classification), add more output units—one for each class—and use a softmax layer to produce class probabilities.\n\n\n10.5.5 5. Language Modeling\nTask: Predict the next word \\(w_t\\) given previous words \\(w_{t-1}, w_{t-2}, \\ldots\\)\n\nTraditional approach: N-gram language models, which have limited ability to generalize.\nNeural approach: Feedforward neural language models use word embeddings and can generalize better.\n\nExample:\nSuppose the training data contains:\nI have to make sure that the cat gets fed.\nBut never:\ndog gets fed\nAt test time:\nI forgot to make sure that the dog gets ___\n\nAn N-gram model may not predict “fed” after “dog gets”.\nA neural language model can use the similarity between “cat” and “dog” embeddings to predict “fed” after “dog”.\n\nHandling sequences:\nFeedforward models use a sliding window of fixed length (e.g., previous 3 words) as input. More advanced models (like RNNs and Transformers) handle arbitrary-length sequences.\n\nSummary:\nFeedforward neural networks provide a foundation for many NLP tasks. They enable learning from data, handle non-linear patterns, and can generalize better than traditional models, especially when using learned word embeddings.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Simple Neural Networks</span>"
    ]
  },
  {
    "objectID": "session9.html#training-neural-networks",
    "href": "session9.html#training-neural-networks",
    "title": "10  Simple Neural Networks",
    "section": "10.6 Training Neural Networks",
    "text": "10.6 Training Neural Networks\nTraining a neural network involves adjusting its weights to minimize the difference between the predicted output and the true output. This is typically done using an algorithm called backpropagation combined with gradient descent.\n\n10.6.1 Training Steps\nFor each training example \\((\\mathbf{x}, y)\\):\n\nForward Pass:\nCompute the network’s prediction \\(\\hat{y}\\) by passing \\(\\mathbf{x}\\) through the network.\nCompute Loss:\nCalculate the loss \\(L(y, \\hat{y})\\), which measures how far the prediction is from the true value.\nBackward Pass (Backpropagation):\nCompute the gradients of the loss with respect to each weight in the network.\nUpdate Weights:\nAdjust each weight \\(w\\) using the gradients to reduce the loss (e.g., \\(w \\leftarrow w - \\eta \\frac{\\partial L}{\\partial w}\\), where \\(\\eta\\) is the learning rate).\n\n\n\n10.6.2 What Happens During Backpropagation?\n\nFor each output node, compute how much it contributed to the loss.\nFor each hidden node, determine how much it contributed to the output error (its “blame”).\nUse these contributions to update the weights from the hidden layer to the output layer, and from the input layer to the hidden layer.\n\nThis process is repeated for many epochs (passes through the training data) until the network’s predictions are sufficiently accurate.\nKey Points: - Training is iterative and data-driven. - Backpropagation efficiently computes gradients for all weights. - The learning rate controls how big each update step is.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Simple Neural Networks</span>"
    ]
  },
  {
    "objectID": "session10.html",
    "href": "session10.html",
    "title": "11  The Transformer and Pre-trained Language Models",
    "section": "",
    "text": "11.1 Background",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>The Transformer and Pre-trained Language Models</span>"
    ]
  },
  {
    "objectID": "session10.html#background",
    "href": "session10.html#background",
    "title": "11  The Transformer and Pre-trained Language Models",
    "section": "",
    "text": "11.1.1 Descartes and the Nature of Thought\nPhilosophers like Descartes have long debated what it means to “think.” In the context of artificial intelligence, this leads us to consider whether machines can truly understand or just simulate understanding.\n\n\n11.1.2 The Turing Test\nAlan Turing proposed a test to determine if a machine can exhibit intelligent behavior indistinguishable from a human. If a human evaluator cannot reliably tell the machine from a human based on their responses, the machine is said to have passed the Turing Test.\n\n\n11.1.3 Singularity and AGI\nThe concept of the Singularity refers to a hypothetical point where artificial intelligence surpasses human intelligence, leading to rapid technological growth. AGI (Artificial General Intelligence) is the idea of machines that can perform any intellectual task that a human can.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>The Transformer and Pre-trained Language Models</span>"
    ]
  },
  {
    "objectID": "session10.html#the-challenge-of-language",
    "href": "session10.html#the-challenge-of-language",
    "title": "11  The Transformer and Pre-trained Language Models",
    "section": "11.2 The Challenge of Language",
    "text": "11.2 The Challenge of Language\nLanguage is complex and powerful. It allows us to express an infinite number of ideas using a finite set of words and rules. Sentences can have unbounded dependencies, such as:\n\n“The cat that the dog chased ran away.”\n\nHere, “the cat” and “ran away” are connected, even though other words intervene.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>The Transformer and Pre-trained Language Models</span>"
    ]
  },
  {
    "objectID": "session10.html#the-miracle-of-llms-3-key-insights",
    "href": "session10.html#the-miracle-of-llms-3-key-insights",
    "title": "11  The Transformer and Pre-trained Language Models",
    "section": "11.3 The Miracle of LLMs – 3 Key Insights",
    "text": "11.3 The Miracle of LLMs – 3 Key Insights\n\nBag of Words / N-grams\nEarly models represented text as unordered collections of words (bag of words) or short sequences (n-grams). For example, the sentence “The quick brown fox” as a bag of words is just {“The”, “quick”, “brown”, “fox”}.\nWord Embeddings and Self-Learning\nWord embeddings map words to high-dimensional vectors that capture semantic relationships. For example:\n\n“king” - “man” + “woman” ≈ “queen”\n“Paris” - “France” + “Italy” ≈ “Rome” These vectors capture analogies and relationships between words.\n\nThe Transformer\nTransformers (covered in detail next) revolutionized NLP by allowing models to consider the entire context of a sentence, not just local word sequences.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>The Transformer and Pre-trained Language Models</span>"
    ]
  },
  {
    "objectID": "session10.html#word-embeddings",
    "href": "session10.html#word-embeddings",
    "title": "11  The Transformer and Pre-trained Language Models",
    "section": "11.4 Word Embeddings",
    "text": "11.4 Word Embeddings\nWord vectors are rich representations of word meaning and usage. They capture relationships such as:\n\nGender:\n“king” - “man” + “woman” ≈ “queen”\nCapital Cities:\n“Paris” - “France” + “Italy” ≈ “Rome”\nComparatives:\n“big” - “bigger” + “small” ≈ “smaller”\n\nThese embeddings allow models to understand and manipulate language in a way that reflects real-world relationships.\nWhat we need is a method to capture how words are combined to produce sentences and meaning—this is where models like the Transformer come in.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>The Transformer and Pre-trained Language Models</span>"
    ]
  },
  {
    "objectID": "session10.html#transformers",
    "href": "session10.html#transformers",
    "title": "11  The Transformer and Pre-trained Language Models",
    "section": "11.5 Transformers",
    "text": "11.5 Transformers\n\n11.5.1 The Problem with Static Embeddings\nTraditional embeddings like word2vec are static:\nA word always has the same vector, regardless of context.\n\nExample:\n“The chicken didn’t cross the road because it was too tired.”\n\nWhat does “it” refer to?\nA static embedding for “it” cannot capture the difference between “chicken” and “road” as possible referents.\nKey Insight:\nA word’s meaning should change depending on its context!\n\n\n11.5.2 Contextual Embeddings\n\nContextual embeddings assign a different vector to each word in each context.\nThe meaning of “it” in the above sentence depends on the surrounding words.\n\nHow can we compute contextual embeddings?\n→ Attention\n\n\n11.5.3 Attention Mechanism\nIntuition:\nTo build a contextual embedding for a word, we selectively integrate information from all other words in the sentence.\n\nEach word “attends to” other words, weighting them by relevance.\nThe embedding for a word is a weighted sum of the embeddings of all words in the sentence.\n\n\nExample 1:\n“The chicken didn’t cross the road because it was too tired.”\nHere, “it” likely refers to “chicken”.\n\n\nExample 2:\n“The chicken didn’t cross the road because it was too wide.”\nHere, “it” likely refers to “road”.\n\nAt the word “it”, the model uses attention to decide whether “it” refers to “chicken” or “road”, based on context.\nFormally:\nGiven token embeddings:\nx₁, x₂, x₃, …, xₙ\nFor each word:\naᵢ = weighted sum of x₁, x₂, …, xₙ\nWeights are based on similarity to xᵢ (the current word).\nResult:\n- Each word’s embedding is context-dependent. - Attention enables models to capture complex relationships and meanings in language.\n\n\n11.5.4 Position Embeddings\nTransformers have no inherent sense of word order.\nPosition embeddings solve this by assigning each position in the sequence a unique vector.\n\nFor a sequence of length N, learn a position embedding matrix Eₚₒₛ of shape [1 × N].\nEach position (e.g., 1, 2, 3, …) gets its own embedding, learned during training.\nThese are added to the word embeddings so the model knows the order of words.\n\n\nExample:\nThe embedding for “fish” at position 3 is different from “fish” at position 17.\n\n\n\n\nPosition Embeddings\n\n\n\n\n11.5.5 Output: Logits and Softmax\nAfter processing, the model produces a logit (score) for each word in the vocabulary.\n\nLogits: Vector of size [1 × |V|], where |V| is the vocabulary size.\nSoftmax: Converts logits into probabilities over the vocabulary.\n\n\n\n\nFinal Transformer Model\n\n\n\n\n11.5.6 BERT: Bidirectional Encoder Representations from Transformers\n\nBERT is an encoder-only Transformer.\nGoal: Produce a rich vector representation for each token in the input sequence.\nNot a chatbot; best for classification tasks (e.g., sentiment analysis, spam detection, ticket routing).\n\n\nExample:\nGiven a sentence, BERT can classify its sentiment as positive or negative.\n\n\nBERT adds a special [CLS] token to represent the entire sequence (sentence embedding).\nFor specific tasks, BERT is further trained (fine-tuned) with labeled data.\n\n\n\n11.5.7 Decoder-Only Models (e.g., GPT)\n\nGoal: Generate new output sequences from input sequences (e.g., text generation, chatbots).\nUnlike BERT, these models predict the next word in a sequence.\n\n\nExample:\nGiven “The weather is”, GPT might generate “sunny today in Berlin.”\n\nSummary Table: Encoder vs Decoder Transformers\n\n\n\n\n\n\n\n\n\nModel Type\nExample\nMain Use\nSpecial Token\n\n\n\n\nEncoder-only\nBERT\nClassification, embeddings\n[CLS]\n\n\nDecoder-only\nGPT\nText generation\nNone",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>The Transformer and Pre-trained Language Models</span>"
    ]
  },
  {
    "objectID": "session10.html#large-language-models-llms",
    "href": "session10.html#large-language-models-llms",
    "title": "11  The Transformer and Pre-trained Language Models",
    "section": "11.6 Large Language Models (LLMs)",
    "text": "11.6 Large Language Models (LLMs)\nLarge Language Models (LLMs) are advanced neural networks trained to predict the next word in a sequence, enabling them to generate coherent and contextually relevant text.\n\n11.6.1 From N-gram Models to LLMs\n\nN-gram Language Models:\nAssign probabilities to word sequences based on observed counts in large text corpora.\nExample:\nGiven “The cat sat on the”, an n-gram model predicts “mat” if that sequence is common.\nLLMs:\nAlso assign probabilities to word sequences, but learn these probabilities by training on massive datasets and using deep neural networks (Transformers).\nExample:\nGiven “The cat sat on the”, an LLM might generate “mat”, “sofa”, or “floor”, depending on context.\n\n\n\n11.6.2 Why LLMs Are Powerful\n\nEven though LLMs are trained only to predict the next word, they learn a lot about language, facts, and reasoning.\nMany tasks can be reframed as next-word prediction.\n\nExamples:\n\nSentiment Analysis\nPrompt:\n&gt; The sentiment of the sentence “I like Jackie Chan” is\nLLM likely predicts: “positive”\nSummarization\nPrompt:\n&gt; Summarize: “The movie was exciting and full of twists.”\nLLM might generate: “Exciting and unpredictable movie.”",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>The Transformer and Pre-trained Language Models</span>"
    ]
  },
  {
    "objectID": "session10.html#decoding-and-sampling",
    "href": "session10.html#decoding-and-sampling",
    "title": "11  The Transformer and Pre-trained Language Models",
    "section": "11.7 Decoding and Sampling",
    "text": "11.7 Decoding and Sampling\nWhen generating text, LLMs must choose the next word based on predicted probabilities. This process is called decoding.\n\n11.7.1 Random Sampling\n\nThe model samples the next word according to its probability.\nMost of the time, high-probability words are chosen, but occasionally, rare words are picked, which can lead to unexpected or odd sentences.\n\nExample:\nPrompt: “The sky is”\nPossible outputs: “blue”, “clear”, “falling”, “delicious” (the last is odd, but possible with low probability).\n\n\n11.7.2 Balancing Quality and Diversity\n\nEmphasize high-probability words:\n\nMore accurate, coherent, and factual\n– Can be repetitive or boring\n\nEmphasize middle-probability words:\n\nMore creative and diverse\n– May be less factual or coherent\n\n\n\n\n11.7.3 Top-k Sampling\n\nChoose a number k (e.g., 10).\nFor each prediction, keep only the top k most probable words.\nRandomly sample from these k words.\n\nExample:\nIf the top 3 words after “The sky is” are “blue”, “clear”, “cloudy”, only these are considered for sampling.\n\n\n11.7.4 Top-p (Nucleus) Sampling\n\nInstead of a fixed k, keep the smallest set of words whose total probability exceeds a threshold p (e.g., 0.9).\nThis set may be larger or smaller depending on the context.\n\nExample:\nIf “blue” has 0.6, “clear” 0.2, “cloudy” 0.1, “stormy” 0.05, “red” 0.05, then top-p with p=0.9 includes “blue”, “clear”, “cloudy”.\n\n\n11.7.5 Temperature Sampling\n\nAdjusts the “sharpness” of the probability distribution.\nLow temperature (τ &lt; 1):\nMakes the model more confident; high-probability words become even more likely.\nHigh temperature (τ &gt; 1):\nMakes the model more random; low-probability words are more likely to be chosen.\n\nExample:\nWith τ = 0.5, “blue” is much more likely than “cloudy”.\nWith τ = 2.0, “cloudy” or “red” might be chosen more often.\nHow it works:\nDivide the logits (raw scores) by τ before applying softmax.\n- τ = 1: No change\n- τ &lt; 1: Sharper distribution\n- τ &gt; 1: Flatter distribution\nSummary Table: Decoding Methods\n\n\n\n\n\n\n\n\nMethod\nHow it works\nEffect\n\n\n\n\nRandom\nSample from all words by probability\nMost diverse, least accurate\n\n\nTop-k\nSample from top k words\nBalances quality and diversity\n\n\nTop-p\nSample from smallest set covering p prob.\nAdaptive, flexible\n\n\nTemperature\nAdjusts probability sharpness\nControls randomness",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>The Transformer and Pre-trained Language Models</span>"
    ]
  },
  {
    "objectID": "session10.html#pretraining",
    "href": "session10.html#pretraining",
    "title": "11  The Transformer and Pre-trained Language Models",
    "section": "11.8 Pretraining",
    "text": "11.8 Pretraining\nThe big idea behind the success of language models:\n\nPretrain a transformer model on enormous amounts of text.\nFine-tune it for new tasks.\n\n\n11.8.1 Self-Supervised Training\n\nTrain models to predict the next word in a sequence.\nAt each time step t, ask the model to predict the next word.\nUse gradient descent to minimize the prediction error.\n\nWhy “self-supervised”?\nBecause the next word in the text itself serves as the label—no manual annotation needed.\n\n\n11.8.2 Language Model Training: Loss Function\n\nUse cross-entropy loss.\nWe want the model to assign a high probability to the true next word.\nIf the model assigns too low a probability, the loss is high.\nTraining adjusts weights to increase the probability of the correct word.\n\n\n\n11.8.3 Pretraining Data\n\nCommon Crawl: Billions of web pages.\nColossal Clean Crawled Corpus (C4): 156 billion tokens of English, filtered for quality.\n\nIncludes patent documents, Wikipedia, news sites, etc.\n\n\n\n\n11.8.4 What Does a Model Learn from Pretraining?\n\nFactual knowledge:\n\n“The author of ‘A Room of One’s Own’ is Virginia Woolf.”\n“The square root of 4 is 2.”\n\nLanguage patterns:\n\n“There are canines everywhere! One dog in the front room, and two dogs…”\n“It wasn’t just big, it was enormous.”\n\nCommonsense reasoning:\n\n“The doctor told me that he…”\n\n\nText contains enormous amounts of knowledge.\nPretraining on large, diverse text corpora gives language models their remarkable abilities.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>The Transformer and Pre-trained Language Models</span>"
    ]
  },
  {
    "objectID": "session10.html#working-with-large-language-models",
    "href": "session10.html#working-with-large-language-models",
    "title": "11  The Transformer and Pre-trained Language Models",
    "section": "11.9 Working with Large Language Models",
    "text": "11.9 Working with Large Language Models\n\n11.9.1 Model Types\n\nBase Model: Result of pre-training on large text corpora.\nInstruct Model: Base model fine-tuned to follow instructions.\nChat Model: Further tuned for dialogue and conversational tasks.\n\n\n\n11.9.2 Key Settings\n\nTemperature:\nControls randomness in output.\n\nLow temperature (e.g., 0.2): More deterministic, focused responses.\n\nHigh temperature (e.g., 1.0): More random, creative outputs.\n\nTop-p (Nucleus Sampling):\nOnly considers the smallest set of words whose cumulative probability exceeds p (e.g., 0.9).\n\nHigher p: More possible words, more diverse outputs.\n\nMax Length:\nMaximum number of tokens in the generated response.\nStop Sequences:\nSpecify tokens or phrases where generation should stop.\nFrequency Penalty:\nPenalizes repeated words to encourage variety.\nPresence Penalty:\nPenalizes new words to encourage sticking to the prompt context.\n\n\n\n11.9.3 Prompt Structure\nA good prompt often includes:\n\nInstruction: What you want the model to do.\nExample: “Summarize the following text.”\nContext: Extra information to guide the model.\nExample: “The text is a news article.”\nInput Data: The main content or question.\nExample: “The movie was exciting and full of twists.”\nOutput Indicator: Desired format or type of output.\nExample: “Summary:”\n\n\n\n11.9.4 Common Tasks\n\nText Summarization\nInformation Extraction\nQuestion Answering\nText Classification\nConversation\nCode Generation\nReasoning\n\n\n\n11.9.5 Prompting Techniques\n\nZero-shot Prompting:\nAsk the model to perform a task without examples.\nExample:\n&gt; “Translate to French: ‘Good morning.’”\nFew-shot Prompting:\nProvide a few examples to guide the model.\nExample:\n&gt; “Translate to French:\n&gt; English: ‘Good morning.’ → French: ‘Bonjour.’\n&gt; English: ‘How are you?’ → French:”\nChain-of-Thought Prompting:\nEncourage the model to explain its reasoning step by step.\nExample:\n&gt; “If there are 3 apples and you eat 1, how many are left? Let’s think step by step.”",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>The Transformer and Pre-trained Language Models</span>"
    ]
  },
  {
    "objectID": "session10.html#takeaways",
    "href": "session10.html#takeaways",
    "title": "11  The Transformer and Pre-trained Language Models",
    "section": "11.10 Takeaways",
    "text": "11.10 Takeaways\n\nTransformers use attention to create context-aware word representations.\nBERT: Encoder model, best for classification and embedding tasks.\nGPT: Decoder model, best for text generation and chat.\nPrompt engineering is key to getting useful outputs from LLMs.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>The Transformer and Pre-trained Language Models</span>"
    ]
  },
  {
    "objectID": "session11.html",
    "href": "session11.html",
    "title": "12  Question Answering and Information Retrieval",
    "section": "",
    "text": "12.1 Introduction\nInformation retrieval QA: Also called Open Domain QA find relevant passages, then use MRC to draw an answer directly from spans of text Knowledge-based QA build semantic representation of query, then use that to query a database LLMs for QA",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Question Answering and Information Retrieval</span>"
    ]
  },
  {
    "objectID": "session11.html#information-retrieval-ir",
    "href": "session11.html#information-retrieval-ir",
    "title": "12  Question Answering and Information Retrieval",
    "section": "12.2 Information Retrieval (IR)",
    "text": "12.2 Information Retrieval (IR)\nKey Concepts in Information Retrieval (IR):\n\nDocument: The unit of text that the system retrieves (e.g., web page, news article, paragraph).\nCollection: The set of documents being used to satisfy requests.\nTerm: A word or phrase in the query.\nQuery: The user’s information need, represented as a set of terms.\n\nVector Space Model:\nBoth queries and documents are mapped to vectors, typically using unigram word counts (bag-of-words approach). Documents are ranked by their cosine similarity to the query vector.\n\nExample:\nQuery: “machine learning applications”\nDocument 1: “Applications of machine learning in healthcare”\nDocument 2: “History of art and design”\nDocument 1 will have a higher cosine similarity to the query.\n\nTerm Frequency–Inverse Document Frequency (TF-IDF):\n\nTerm Frequency (tf): The more a term occurs in a document, the higher its weight.\nInverse Document Frequency (idf): Terms that appear in many documents get lower weight.\n\nTF-IDF Example:\nSuppose “machine” appears 3 times in Document 1 and in 100 out of 10,000 documents in the collection.\n- tf = 3\n- idf = log(10,000 / 100) = 2\n- tf-idf = 3 × 2 = 6\nScoring Documents:\nRepresent queries and documents as TF-IDF vectors. Score each document by its cosine similarity to the query:\n\\[\n\\text{score}(q, d) = \\frac{q \\cdot d}{\\|q\\| \\times \\|d\\|}\n\\]\nwhere\n- \\(q\\) is the query vector\n- \\(d\\) is the document vector\n- \\(\\|q\\|\\) and \\(\\|d\\|\\) are their vector lengths\nEvaluation Metrics:\n\nPrecision: Fraction of returned documents that are relevant.\nRecall: Fraction of all relevant documents that are returned.\n\n\n\n\nRank\nJudgment (Relevant?)\nPrecision\nRecall\n\n\n\n\n1\nYes\n1/1 = 1.0\n1/3 = 0.33\n\n\n2\nNo\n1/2 = 0.5\n1/3 = 0.33\n\n\n3\nYes\n2/3 = 0.67\n2/3 = 0.67\n\n\n4\nYes\n3/4 = 0.75\n3/3 = 1.0\n\n\n\nVocabulary Mismatch Problem:\nA search for “tragic love story” might not find documents containing “star-crossed lovers.”\nSolution: Use models that recognize synonyms and similar phrases.\nDense Retrieval with Neural Models:\nEncode queries and documents with models like BERT, then compare their embeddings to score relevance.\nExample:\nQuery: “famous tragic love story”\nDocument: “Romeo and Juliet are star-crossed lovers”\nBERT can help match these despite different wording.\n\nQuestion Answering Datasets:\n\nStanford Question Answering Dataset (SQuAD): Passages from Wikipedia with questions whose answers are spans from the passage.\nSQuAD 2.0: Adds questions that are unanswerable based on the passage.\n\nExtractive QA:\nThe answer is a span in the passage. For each token ( p_i ) in the passage, compute the probability it is the start or end of the answer span.\nBERT Implementation for Extractive QA:\n\nThe question is the first sequence, the passage is the second.\nA linear layer predicts the start and end positions of the answer span.\nTwo vectors are learned: a span-start embedding ( S ) and a span-end embedding ( E ).\nFor each output token ( p_i ):\n\nSpan-start probability: dot product of ( S ) and ( p_i )\nSpan-end probability: dot product of ( E ) and ( p_i )\n\n\nExample:\nPassage: “The capital of France is Paris.”\nQuestion: “What is the capital of France?”\nModel predicts start and end positions corresponding to “Paris”.\n\n\n12.2.1 IR-based Question Answering\n\n\n12.2.2 Entity Linking\nEntity linking is the process of associating a mention in text with a specific real-world entity in a knowledge base or ontology.\n\nPurpose: Disambiguate mentions (e.g., “Paris” as a city in France vs. Paris Hilton).\nApplications: Improves information retrieval, question answering, and knowledge base population.\n\nWikification:\nA common form of entity linking where mentions in text are mapped to their corresponding Wikipedia pages.\nSteps in Entity Linking: 1. Mention Detection: Identify candidate entity mentions in the text. 2. Candidate Generation: Generate possible entities from the knowledge base for each mention. 3. Entity Disambiguation: Select the most likely entity based on context.\nExample:\nText: “Apple was founded by Steve Jobs.”\n- “Apple” → Apple Inc. (Wikipedia) - “Steve Jobs” → Steve Jobs (Wikipedia)\nEntity linking enables more accurate retrieval and reasoning by grounding text to structured knowledge.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Question Answering and Information Retrieval</span>"
    ]
  },
  {
    "objectID": "session11.html#knowledge-based-question-answering",
    "href": "session11.html#knowledge-based-question-answering",
    "title": "12  Question Answering and Information Retrieval",
    "section": "12.3 Knowledge-based Question Answering",
    "text": "12.3 Knowledge-based Question Answering\nKnowledge-based question answering (KBQA) involves answering questions by mapping them to queries over structured databases or knowledge bases.\nTypes of Knowledge-based QA: - Graph-based QA: Represents the knowledge base as a graph, with entities as nodes and relations/propositions as edges. Answers are found by traversing or querying the graph. - Semantic Parsing QA: Maps natural language questions to logical forms (e.g., database queries or graph traversals) that can be executed against the knowledge base.\nRDF Triples: A common structure for knowledge bases is the RDF triple: (subject, predicate, object), expressing a simple fact or relation. - Example: (“Ada Lovelace”, “birth year”, “1815”)\nExample Questions: - “When was Ada Lovelace born?” → birth-year(Ada Lovelace, ?x) - “Who was born in 1815?” → birth-year(?x, 1815) - “What is the capital of England?” → capital-city(England, ?x)\nPopular Knowledge Bases: - DBpedia: Contains over 2 billion RDF triples extracted from Wikipedia. - Freebase/Wikidata: Large collaborative knowledge bases with structured facts.\nQuestion Datasets: - SimpleQuestions: 100K+ questions paired with Freebase triples, designed for evaluating KBQA systems.\nSupervision for Semantic Parsing: - Fully Supervised: Each question is paired with a hand-annotated logical form. - Weakly Supervised: Each question is paired only with the answer (denotation); the logical form is treated as a latent variable and learned indirectly.\nKnowledge-based QA enables precise, fact-based answering by leveraging structured representations of world knowledge.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Question Answering and Information Retrieval</span>"
    ]
  },
  {
    "objectID": "session11.html#llms-for-question-answering",
    "href": "session11.html#llms-for-question-answering",
    "title": "12  Question Answering and Information Retrieval",
    "section": "12.4 LLMs for Question Answering",
    "text": "12.4 LLMs for Question Answering\n\n12.4.1 Prompting Techniques\nSee prompting techniques in Session 10.\n\n\n\n12.4.2 Retrieve-and-Generate (RAG) for Question Answering\nRAG (Retrieval-Augmented Generation) combines information retrieval with large language models (LLMs) to answer questions more accurately and with up-to-date information.\nWorkflow: 1. Input: User submits a question. 2. Indexing: Documents are split into chunks, and embeddings are generated for each chunk and for the input question. 3. Retrieval: The system retrieves the most relevant chunks by comparing their embeddings to the question embedding. 4. Generation: The retrieved chunks are combined with the original question to form a prompt, which is passed to the LLM to generate an answer.\n\n\n\n12.4.3 Evaluation of RAG Systems\nRetrieval Evaluation: - Are the correct/relevant chunks retrieved for a given query? - With reference answers, precision and recall can be measured. - Reference-free evaluation (e.g., using LLMs as judges) can assess precision, but recall requires ground-truth relevant chunks.\nGeneration Evaluation: - Is the generated answer correct and relevant? - With references: Use metrics like BLEU, ROUGE, embedding similarity, or LLM-based judgments. - Reference-free: LLM-based evaluation of answer quality.\nKey Criteria: - Faithfulness: Does the answer only make claims supported by the retrieved context? - Answer Relevance: Does the answer directly address the question? - Context Relevance: Does the context contain only information needed to answer the question?\nIf the correct answer is not present in the retrieved data, the model should indicate this. Faithful answers should not hallucinate unsupported information.\nLinks Between Retrieval and Generation: - Poor retrieval typically leads to poor answers. - Evaluating answer quality can indirectly reflect retrieval effectiveness.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Question Answering and Information Retrieval</span>"
    ]
  },
  {
    "objectID": "session11.html#agents",
    "href": "session11.html#agents",
    "title": "12  Question Answering and Information Retrieval",
    "section": "12.5 Agents",
    "text": "12.5 Agents\n\nDefinition: Agents are systems capable of forming intentions, making plans, and acting to achieve goals. This implies the presence of internal mental states or representations.\nLLMs as Agents: Large Language Models (LLMs) possess richer internal representations than previous AI systems. In practice, an “agent” often refers to a system that uses an LLM to control the flow of an application (e.g., LangChain agents).\nModalities:\n\nText Input: LLMs natively process text, but implied meanings and context can be challenging. Instruction tuning improves performance.\nAudio Input: Requires external tools (e.g., speech-to-text). Modality fusion techniques can integrate audio embeddings with text.\nVisual Input: Involves external tools (e.g., image captioning) or modality fusion to combine visual and textual information.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Question Answering and Information Retrieval</span>"
    ]
  },
  {
    "objectID": "session11.html#formal-vs.-functional-abilities-of-llms",
    "href": "session11.html#formal-vs.-functional-abilities-of-llms",
    "title": "12  Question Answering and Information Retrieval",
    "section": "12.6 Formal vs. Functional Abilities of LLMs",
    "text": "12.6 Formal vs. Functional Abilities of LLMs\n\nHuman Assumptions: We often assume language is produced by rational, thinking agents (Turing Test).\nCompetence Types:\n\nFormal Linguistic Competence: Knowledge of grammar, rules, and patterns.\nFunctional Linguistic Competence: Using and understanding language in real-world contexts.\n\nCommon Fallacies (Mahowald et al., 2023):\n\n“Good at language → good at thought”\n“Bad at thought → bad at language”\n\nExamples:\n\nGrammaticality judgments:\n\n“Bert knows what many writers find.” (grammatical)\n“*Bert knows that many writers find.” (ungrammatical)\n“The truck has clearly tipped over.” (grammatical)\n“*The truck has ever tipped over.” (ungrammatical)\n\n\nOther Abilities:\n\nFormal reasoning (logic, math)\nWorld knowledge\nSituation and social reasoning\n\nTesting Issues:\n\nHas the model been fine-tuned for the task?\nIs test material present in training data?",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Question Answering and Information Retrieval</span>"
    ]
  },
  {
    "objectID": "session11.html#takeaways",
    "href": "session11.html#takeaways",
    "title": "12  Question Answering and Information Retrieval",
    "section": "12.7 Takeaways",
    "text": "12.7 Takeaways\n\nQuestion Answering Approaches:\n\nInformation Retrieval (IR): Encode queries and candidate answers, compare using dot product or cosine similarity. Encodings can be TF-IDF or dense vectors.\nKnowledge-based QA: Map questions to structured queries over knowledge bases.\n\nDatasets:\n\nSQuAD: Span-based QA from Wikipedia passages.\nEntity Linking/Wikification: Map mentions to Wikipedia pages.\nRDF Datasets: DBpedia (2B+ triples), Freebase/Wikidata.\nSimpleQuestions: 100K+ questions paired with Freebase triples.\n\nLLM Techniques:\n\nPrompt engineering (settings, structure, few-shot, chain-of-thought)\nRetrieval-Augmented Generation (RAG)\nPrompt chaining\nAgents\n\nCompetence:\n\nLLMs excel at formal linguistic competence.\nFunctional competence (real-world understanding and reasoning) remains challenging.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Question Answering and Information Retrieval</span>"
    ]
  },
  {
    "objectID": "summary_concise.html",
    "href": "summary_concise.html",
    "title": "13  Concepts and Explanations Short",
    "section": "",
    "text": "13.1 Chapter 1: Foundations of NLP",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Concepts and Explanations Short</span>"
    ]
  },
  {
    "objectID": "summary_concise.html#chapter-1-foundations-of-nlp",
    "href": "summary_concise.html#chapter-1-foundations-of-nlp",
    "title": "13  Concepts and Explanations Short",
    "section": "",
    "text": "13.1.1 The Revolution in NLP\n\nTransformer models have enabled NLP systems to learn from massive datasets, using objectives like masked language modeling.\nRLHF (Reinforcement Learning from Human Feedback) further aligns models with human preferences.\n\n\n\n13.1.2 Language is Hard\n\nLanguage is infinitely expressive and ambiguous, making it difficult for machines to achieve true understanding.\nEven advanced models may not fully replicate human-like comprehension.\n\n\n\n13.1.3 Brief History\n\nDescartes doubted machines could imitate humans; Turing proposed the Turing Test to evaluate machine intelligence via conversation.\nPassing the Turing Test means a machine’s responses are indistinguishable from a human’s.\n\n\n\n13.1.4 NLP: The Basic Approach\n\nText data is unstructured and context-dependent, unlike numerical data.\nBag-of-words (BoW) representations structure text but ignore word order and context.\n\n\n\n13.1.5 Supervised ML for Text Processing\n\nLabeled text enables tasks like spam detection and sentiment analysis.\nRaises questions about whether these approaches capture real language understanding.\n\n\n\n13.1.6 Bag-of-Words Limitation\n\nBoW ignores word order, so it cannot distinguish between sentences with the same words in different orders.\n\n\n\n13.1.7 Language Modeling with N-grams\n\nN-gram models assign probabilities to word sequences based on the Markov assumption (dependence on previous n-1 words).\nThey capture local word order but struggle with long-range dependencies and rare phrases.\n\n\n\n13.1.8 Training an LLM\n\nLLMs are trained to predict the next word using neural networks and the softmax function.\nTraining involves computing loss and updating model weights to improve predictions.\n\n\n\n13.1.9 Probing GPT\n\nLLMs generate coherent, grammatical text and show some understanding of linguistic structure.\nThey challenge previous claims about the impossibility of learning abstract linguistic knowledge from input alone.\n\n\n\n13.1.10 AI: Where Are We Heading?\n\nAGI aims for machines to match or surpass human cognitive abilities across tasks.\nBenchmarks include passing exams, performing jobs, and completing complex real-world tasks.\n\n\n\n13.1.11 Applications of LLMs\n\nLLMs are used in customer support, research portals, automated help desks, and accessibility tools.\n\n\n\n13.1.12 Takeaways\n\nLanguage is inherently difficult for machines due to its infinite and ambiguous nature.\nLLMs are rapidly advancing toward human-level language ability.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Concepts and Explanations Short</span>"
    ]
  },
  {
    "objectID": "summary_concise.html#chapter-2-core-nlp-tools-and-preprocessing",
    "href": "summary_concise.html#chapter-2-core-nlp-tools-and-preprocessing",
    "title": "13  Concepts and Explanations Short",
    "section": "13.2 Chapter 2: Core NLP Tools and Preprocessing",
    "text": "13.2 Chapter 2: Core NLP Tools and Preprocessing\n\n13.2.1 NLP Libraries\n\nNLTK: Flexible and feature-rich, but slower; supports POS tagging, NER, and more.\nspaCy: Fast, deep learning-based, suitable for production.\nTextBlob: Simple API for basic NLP tasks, not suitable for large-scale production.\n\n\n\n13.2.2 Normalization\n\nStandardizes text (lowercasing, removing punctuation/stopwords, extra spaces, special characters) to reduce noise.\n\n\n\n13.2.3 Morphological Normalization\n\nStemming: Reduces words to their root form by removing suffixes (fast, less accurate).\nLemmatization: Maps words to their dictionary form using vocabulary and POS information (more accurate, slower).\n\n\n\n13.2.4 Tokenization\n\nSplits text into tokens (words, sentences) for further processing.\nTokens are used for stemming, lemmatization, and POS tagging.\n\n\n\n13.2.5 Regular Expressions\n\nUsed for pattern matching and text manipulation (searching, extracting, replacing).\nPython’s re module provides methods for regex operations.\n\n\n\n13.2.6 POS Tagging\n\nAssigns grammatical categories (noun, verb, adjective, etc.) to each word.\nMethods: rule-based and statistical (machine learning).\n\n\n\n13.2.7 Named Entity Recognition (NER)\n\nIdentifies and classifies named entities (persons, organizations, locations, dates, etc.) in text.\nCrucial for extracting structured information from unstructured text.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Concepts and Explanations Short</span>"
    ]
  },
  {
    "objectID": "summary_concise.html#chapter-3-language-modeling-with-n-grams",
    "href": "summary_concise.html#chapter-3-language-modeling-with-n-grams",
    "title": "13  Concepts and Explanations Short",
    "section": "13.3 Chapter 3: Language Modeling with N-grams",
    "text": "13.3 Chapter 3: Language Modeling with N-grams\n\n13.3.1 N-Gram Language Models\n\nPredict the probability of a word based on the previous n-1 words.\nTypes: unigram, bigram, trigram.\n\n\n\n13.3.2 Conditional Probability and Chain Rule\n\nJoint probability measures the likelihood of multiple events together; conditional probability measures one event given another.\nThe chain rule allows calculation of a word sequence’s probability by multiplying conditional probabilities.\n\n\n\n13.3.3 Markov Assumption\n\nSimplifies modeling by assuming a word depends only on the previous n-1 words.\n\n\n\n13.3.4 N-Gram Probability Calculation & MLE\n\nProbability is estimated by counting n-gram occurrences and dividing by the count of the previous (n-1)-gram.\nMLE can assign zero probability to unseen n-grams.\n\n\n\n13.3.5 Padding\n\nUses special tokens at sentence boundaries to handle missing context.\n\n\n\n13.3.6 Underflow\n\nMultiplying many small probabilities can cause underflow; log probabilities convert multiplication into addition.\n\n\n\n13.3.7 Smoothing Techniques\n\nAssign small non-zero probabilities to unseen n-grams (Laplace, add-k, Good-Turing, backoff/interpolation, Kneser-Ney).\n\n\n\n13.3.8 Perplexity\n\nMeasures how well a model predicts text; lower perplexity indicates a better model.\n\n\n\n13.3.9 Practical Implementation\n\nSteps: tokenize, pad, generate n-grams, count, calculate probabilities, evaluate with perplexity.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Concepts and Explanations Short</span>"
    ]
  },
  {
    "objectID": "summary_concise.html#chapter-4-text-classification",
    "href": "summary_concise.html#chapter-4-text-classification",
    "title": "13  Concepts and Explanations Short",
    "section": "13.4 Chapter 4: Text Classification",
    "text": "13.4 Chapter 4: Text Classification\n\n13.4.1 Text Classification\n\nAssigns predefined categories to text using supervised, unsupervised, or deep learning methods.\nUsed in spam detection, sentiment analysis, and topic categorization.\n\n\n\n13.4.2 Types of Text Classification Techniques\n\nSupervised Learning: Uses labeled data (e.g., Naive Bayes, Logistic Regression).\nUnsupervised Learning: Finds patterns in unlabeled data (e.g., LDA, K-means).\nDeep Learning: Uses neural networks (e.g., CNNs, RNNs, Transformers).\n\n\n\n13.4.3 Bayes Theorem\n\nRelates conditional probabilities and is used to update the probability of a hypothesis given new evidence.\nNaive Bayes assumes features are conditionally independent given the class label.\n\n\n\n13.4.4 Types of Naive Bayes Classifiers\n\nGaussian: For continuous features.\nMultinomial: For discrete features like word counts.\nBernoulli: For binary features.\n\n\n\n13.4.5 Implementing Naive Bayes\n\nSteps: load and preprocess data, split into train/test, extract features, train classifier, evaluate.\n\n\n\n13.4.6 Evaluation Metrics\n\nAccuracy: Correct predictions / total predictions.\nPrecision: True positives / predicted positives.\nRecall: True positives / actual positives.\nF1-Score: Harmonic mean of precision and recall.\n\n\n\n13.4.7 Disadvantages of Naive Bayes\n\nIndependence assumption may not hold; zero probability for unseen features; struggles with correlated features.\n\n\n\n13.4.8 Handling Class Imbalance\n\nResampling: Oversample minority or undersample majority class.\nClass Weights: Penalize misclassification of minority class.\nEnsemble Methods: Improve robustness (e.g., Random Forest, Gradient Boosting).",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Concepts and Explanations Short</span>"
    ]
  },
  {
    "objectID": "summary_concise.html#chapter-5-logistic-regression",
    "href": "summary_concise.html#chapter-5-logistic-regression",
    "title": "13  Concepts and Explanations Short",
    "section": "13.5 Chapter 5: Logistic Regression",
    "text": "13.5 Chapter 5: Logistic Regression\n\n13.5.1 Logistic Regression\n\nLinear model for binary classification using the logistic (sigmoid) function.\nOutputs probabilities between 0 and 1.\n\n\n\n13.5.2 Log-Odds (Logit)\n\nThe log-odds is the logarithm of the odds ratio, modeled as a linear combination of input features.\n\n\n\n13.5.3 Training Logistic Regression\n\nCoefficients are estimated using Maximum Likelihood Estimation (MLE).\nRegularization (L1, L2, Elastic Net) helps prevent overfitting.\n\n\n\n13.5.4 Bag of Words vs. TF-IDF\n\nBoW: Represents text as word count vectors, ignoring word order and context.\nTF-IDF: Weighs words by their frequency in a document and rarity across the corpus.\n\n\n\n13.5.5 LR Model Assumptions\n\nAssumes linearity in log-odds, independence of observations, and no multicollinearity.\n\n\n\n13.5.6 LR Limitations\n\nSensitive to multicollinearity and outliers; cannot capture complex, non-linear relationships.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Concepts and Explanations Short</span>"
    ]
  },
  {
    "objectID": "summary_concise.html#chapter-6-sentiment-analysis",
    "href": "summary_concise.html#chapter-6-sentiment-analysis",
    "title": "13  Concepts and Explanations Short",
    "section": "13.6 Chapter 6: Sentiment Analysis",
    "text": "13.6 Chapter 6: Sentiment Analysis\n\n13.6.1 Sentiment Analysis (SA)\n\nDetermines the sentiment (positive, negative, neutral) expressed in text.\nUses sentiment lexicons or machine learning models.\n\n\n\n13.6.2 Resources for Sentiment Lexicons\n\nVADER: Social media, handles negations/intensifiers.\nAFINN: 3300+ English words, scores -5 to +5.\nSentiWordNet: Assigns positive, negative, objective scores to WordNet synsets.\nLIWC: Psychological/linguistic categories.\n\n\n\n13.6.3 Sentiment Analysis Approaches\n\nRule-based: Uses predefined rules and lexicons.\nMachine learning-based: Trains models on labeled data.\nHybrid: Combines both for improved performance.\n\n\n\n13.6.4 Model Comparison\n\nLexicon-based models are simple but struggle with negation/sarcasm.\nDeep learning models (LSTM, BERT) handle complex sentiment and context.\n\n\n\n13.6.5 Python Libraries and Tools for SA\n\nNLTK, TextBlob, TensorFlow, PyTorch, Hugging Face Transformers.\n\n\n\n13.6.6 SA Challenges\n\nContext, sarcasm, ambiguity, and domain-specific language make accurate classification difficult.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Concepts and Explanations Short</span>"
    ]
  },
  {
    "objectID": "summary_concise.html#chapter-7-topic-modeling",
    "href": "summary_concise.html#chapter-7-topic-modeling",
    "title": "13  Concepts and Explanations Short",
    "section": "13.7 Chapter 7: Topic Modeling",
    "text": "13.7 Chapter 7: Topic Modeling\n\n13.7.1 Topic Modeling\n\nUnsupervised technique for discovering abstract topics in a collection of documents.\n\n\n\n13.7.2 Latent Dirichlet Allocation (LDA)\n\nEach document is a mixture of topics, each topic is a distribution over words.\nUses Bayes’ Theorem for parameter estimation; methods include EM, Variational Inference, Gibbs sampling.\n\n\n\n13.7.3 LDA Parameters and Trade-offs\n\nKey parameters: document density (α), topic word density (β), number of topics (K).\n\n\n\n13.7.4 Practical Considerations for LDA\n\nRequires specifying the number of topics; works best with longer, preprocessed documents.\n\n\n\n13.7.5 Top2Vec\n\nEmbeds documents/words in vector space, clusters, finds topics automatically.\n\n\n\n13.7.6 BERTopic\n\nUses transformer-based embeddings for better topic quality, especially in nuanced/short texts.\n\n\n\n13.7.7 K-means Clustering with Word2Vec\n\nClusters Word2Vec embeddings, suitable for short texts.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Concepts and Explanations Short</span>"
    ]
  },
  {
    "objectID": "summary_concise.html#chapter-8-word-meanings-and-vector-semantics",
    "href": "summary_concise.html#chapter-8-word-meanings-and-vector-semantics",
    "title": "13  Concepts and Explanations Short",
    "section": "13.8 Chapter 8: Word Meanings and Vector Semantics",
    "text": "13.8 Chapter 8: Word Meanings and Vector Semantics\n\n13.8.1 Word Meanings\n\nMeanings are complex and context-dependent; words can have multiple unrelated meanings (homonymy) or related senses (polysemy).\n\n\n\n13.8.2 Word Relations\n\nSynonymy: Similar meanings.\nAntonymy: Opposite meanings.\nSimilarity/Relatedness: Words can be similar or just related.\nConnotation: Emotional/cultural associations.\n\n\n\n13.8.3 Vector Semantics\n\nRepresents words/documents as vectors in multidimensional space.\nDistributional hypothesis: words in similar contexts have similar meanings.\n\n\n13.8.3.1 Comparing Word Vectors\n\nCosine Similarity, Euclidean Distance, Dot Product: Metrics for quantifying semantic similarity.\n\n\n\n\n13.8.4 Word2Vec\n\nLearns dense word embeddings by distinguishing real context pairs from random pairs.\nEmbeddings capture abstract relationships and analogies.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Concepts and Explanations Short</span>"
    ]
  },
  {
    "objectID": "summary_concise.html#chapter-9-neural-networks-in-nlp",
    "href": "summary_concise.html#chapter-9-neural-networks-in-nlp",
    "title": "13  Concepts and Explanations Short",
    "section": "13.9 Chapter 9: Neural Networks in NLP",
    "text": "13.9 Chapter 9: Neural Networks in NLP\n\n13.9.1 Ambiguity in Language\n\nAmbiguity is a core challenge, requiring structural models (parse trees, dependency graphs) to clarify meaning.\n\n\n\n13.9.2 Simple Neural Networks\n\nLayers of units (neurons) compute weighted sums, add bias, and apply activation functions.\n\n\n\n13.9.3 Activation Functions\n\nSigmoid, Tanh, ReLU: Introduce non-linearity, enabling modeling of complex relationships.\n\n\n\n13.9.4 Multi-Layer Networks and XOR\n\nMulti-layer networks with non-linear activations can solve complex functions like XOR.\n\n\n\n13.9.5 Feedforward Neural Networks\n\nInformation flows from input to output without cycles; hidden layers enable hierarchical representations.\n\n\n\n13.9.6 Applying to NLP\n\nUsed for text classification, language modeling, and more; handle variable-length input via padding, truncation, or pooling.\n\n\n\n13.9.7 Training Neural Networks\n\nUses backpropagation and gradient descent to minimize prediction error.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Concepts and Explanations Short</span>"
    ]
  },
  {
    "objectID": "summary_concise.html#chapter-10-transformers-and-large-language-models",
    "href": "summary_concise.html#chapter-10-transformers-and-large-language-models",
    "title": "13  Concepts and Explanations Short",
    "section": "13.10 Chapter 10: Transformers and Large Language Models",
    "text": "13.10 Chapter 10: Transformers and Large Language Models\n\n13.10.1 Key Innovations\n\nBag of Words/N-grams: Early models missed deeper context.\nWord Embeddings: Capture semantic relationships.\nTransformers: Consider entire sentence context via attention.\n\n\n\n13.10.2 Transformers\n\nUse attention to create context-aware word representations.\nPosition embeddings encode word order.\n\n\n\n13.10.3 BERT and Decoder-Only Models\n\nBERT: Encoder-only, best for classification and embeddings.\nGPT: Decoder-only, best for text generation and chat.\n\n\n\n13.10.4 Large Language Models (LLMs)\n\nTrained to predict the next word, learning language, facts, and reasoning.\nDecoding methods (random, top-k, top-p, temperature) balance quality and diversity.\n\n\n\n13.10.5 Pretraining and Self-Supervised Learning\n\nLLMs are pretrained on large text corpora using self-supervised learning and cross-entropy loss.\n\n\n\n13.10.6 Working with LLMs\n\nPrompt engineering (zero-shot, few-shot, chain-of-thought) is key for effective use.\nModel types: base, instruct, chat.\nKey settings: temperature, top-p, max length, penalties.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Concepts and Explanations Short</span>"
    ]
  },
  {
    "objectID": "summary_concise.html#chapter-11-question-answering-and-information-retrieval",
    "href": "summary_concise.html#chapter-11-question-answering-and-information-retrieval",
    "title": "13  Concepts and Explanations Short",
    "section": "13.11 Chapter 11: Question Answering and Information Retrieval",
    "text": "13.11 Chapter 11: Question Answering and Information Retrieval\n\n13.11.1 Information Retrieval (IR)\n\nIR finds relevant documents from a large collection based on a user’s query.\nDocuments and queries are represented as vectors (e.g., BoW, TF-IDF), and similarity (often cosine similarity) is used to rank results.\nKey metrics: precision (fraction of retrieved documents that are relevant) and recall (fraction of relevant documents that are retrieved).\n\n\n\n13.11.2 Vector Space Model & TF-IDF\n\nRepresents documents and queries as vectors in a high-dimensional space.\nTF-IDF weighs terms by their importance: frequent in a document but rare in the collection.\n\n\n\n13.11.3 Dense Retrieval with Neural Models\n\nUses neural networks (e.g., BERT) to encode queries and documents into dense embeddings.\nAllows semantic matching beyond exact word overlap, addressing vocabulary mismatch.\n\n\n\n13.11.4 Question Answering Datasets\n\nDatasets like SQuAD provide passages and questions, with answers as text spans within the passage.\nSQuAD 2.0 introduces unanswerable questions to test model robustness.\n\n\n\n13.11.5 Extractive Question Answering\n\nInvolves selecting a span of text from a passage as the answer to a question.\nModels (e.g., BERT) predict the start and end positions of the answer span.\n\n\n\n13.11.6 Entity Linking (Wikification)\n\nMaps mentions in text to specific entities in a knowledge base (e.g., Wikipedia pages).\nInvolves mention detection, candidate generation, and disambiguation based on context.\n\n\n\n13.11.7 Knowledge-based Question Answering (KBQA)\n\nAnswers questions by mapping them to queries over structured knowledge bases (e.g., DBpedia, Wikidata).\nApproaches: graph-based QA (traversing entity-relation graphs), semantic parsing (mapping questions to logical forms).\nRDF triples (subject, predicate, object) are a common data structure.\n\n\n\n13.11.8 Supervision for Semantic Parsing\n\nFully supervised: annotated logical forms for each question.\nWeakly supervised: only the answer (denotation), logical form is latent.\n\n\n\n13.11.9 Retrieve-and-Generate (RAG)\n\nCombines retrieval of relevant documents or passages with generative LLMs to produce answers.\nRetrieves relevant chunks, combines them with the question, and generates an answer using an LLM.\n\n\n\n13.11.10 Evaluation of RAG Systems\n\nRetrieval evaluation: checks if relevant chunks are retrieved (precision, recall).\nGeneration evaluation: assesses if the answer is correct, relevant, and faithful to the retrieved context.\nFaithfulness and answer/context relevance are key; hallucinations should be avoided.\n\n\n\n13.11.11 Agents\n\nAgents are systems capable of forming intentions, making plans, and acting to achieve goals, often using internal representations.\nLLM-based agents control application flow (e.g., LangChain agents) and can process text, audio, or visual input via modality fusion.\n\n\n\n13.11.12 Formal vs. Functional Abilities of LLMs\n\nFormal linguistic competence: knowledge of grammar and language rules.\nFunctional competence: ability to use language in real-world contexts.\nLLMs excel at formal competence but may struggle with functional competence, such as real-world reasoning.\n\n\n\n13.11.13 Takeaways\n\nQuestion answering can be approached via IR, KBQA, or LLM-based methods (including RAG and agents).\nKey datasets: SQuAD, SimpleQuestions, DBpedia.\nLLMs are powerful for formal linguistic tasks, but functional, real-world understanding remains a challenge.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Concepts and Explanations Short</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "14  Concepts and Explanations Long",
    "section": "",
    "text": "14.1 Concepts and Explanations - Chapter 1",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Concepts and Explanations Long</span>"
    ]
  },
  {
    "objectID": "summary.html#concepts-and-explanations---chapter-1",
    "href": "summary.html#concepts-and-explanations---chapter-1",
    "title": "14  Concepts and Explanations Long",
    "section": "",
    "text": "14.1.1 The Revolution in NLP\n\nThe introduction of transformer models has dramatically advanced NLP, enabling models to learn from massive datasets using objectives like masked language modeling.\nReinforcement Learning from Human Feedback (RLHF) further improves model alignment with human preferences.\n\n\n\n14.1.2 Language is Hard\n\nLanguage is complex due to infinite possible sentences and inherent ambiguity (both lexical and structural).\nMachines struggle with true understanding, and even advanced models may not fully replicate human-like comprehension.\n\n\n\n14.1.3 Brief History\n\nDescartes argued machines could never truly imitate humans, while Turing proposed the Turing Test to evaluate machine intelligence through conversation.\nPassing the Turing Test means a machine’s responses are indistinguishable from a human’s.\n\n\n\n14.1.4 NLP: The Basic Approach\n\nText data is unstructured and context-dependent, unlike numerical or categorical data.\nBag-of-words representations convert text into structured data but ignore word order and context, limiting understanding.\n\n\n\n14.1.5 Supervised ML for Text Processing\n\nLabeled text data enables tasks like spam detection, sentiment analysis, and topic detection.\nRaises questions about whether these approaches capture real language understanding.\n\n\n\n14.1.6 Bag-of-Words Limitation\n\nBag-of-words ignores word order, so it cannot distinguish between sentences with the same words in different orders.\n\n\n\n14.1.7 Language Modeling with N-grams\n\nN-gram models assign probabilities to word sequences based on the Markov assumption (dependence on previous n-1 words).\nThey capture local word order but struggle with long-range dependencies and rare phrases.\n\n\n\n14.1.8 Training an LLM\n\nLLMs are trained to predict the next word in a sequence, using neural networks and the softmax function to assign probabilities.\nTraining involves computing loss based on prediction accuracy and updating model weights to improve future predictions.\n\n\n\n14.1.9 Probing GPT\n\nLLMs generate coherent, grammatical text and show some understanding of linguistic structure.\nThey challenge previous claims about the impossibility of learning abstract linguistic knowledge from input alone.\n\n\n\n14.1.10 AI: Where Are We Heading?\n\nArtificial General Intelligence (AGI) aims for machines to match or surpass human cognitive abilities across tasks.\nBenchmarks include passing university exams, performing jobs, assembling furniture, and completing complex real-world tasks.\n\n\n\n14.1.11 Applications of LLMs\n\nLLMs are used in customer support, research portals, automated help desks, and accessibility tools.\n\n\n\n14.1.12 Takeaways\n\nLanguage is inherently difficult for machines due to its infinite and ambiguous nature.\nLLMs are approaching human-level language ability, and the field is advancing rapidly with significant societal impact expected.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Concepts and Explanations Long</span>"
    ]
  },
  {
    "objectID": "summary.html#concepts-and-explanations---chapter-2",
    "href": "summary.html#concepts-and-explanations---chapter-2",
    "title": "14  Concepts and Explanations Long",
    "section": "14.2 Concepts and Explanations - Chapter 2",
    "text": "14.2 Concepts and Explanations - Chapter 2\n\n14.2.1 NLP Libraries\n\nNLTK and spaCy are two major Python libraries for NLP. NLTK is flexible and feature-rich but slower, while spaCy is faster and uses deep learning for tasks like POS tagging and NER.\nTextBlob is another library built on NLTK, offering a simple API for basic NLP tasks, but it is not suitable for large-scale production.\n\n\n\n14.2.2 Normalization\n\nNormalization standardizes text by converting to lowercase, removing punctuation, stopwords, extra spaces, and special characters.\nIt prepares text for further processing and analysis by reducing noise and inconsistencies.\n\n\n\n14.2.3 Morphological Normalization\n\nInvolves reducing words to their root or base form using stemming or lemmatization.\nRoots are the core part of words, while affixes (prefixes/suffixes) modify meaning or grammatical form.\n\n\n\n14.2.4 Tokenization\n\nTokenization splits text into smaller units (tokens) such as words or sentences.\nTokens are used for further processing like stemming, lemmatization, and POS tagging; a corpus is a collection of such texts.\n\n\n\n14.2.5 Stemming\n\nStemming reduces words to their root form by removing suffixes, often using rule-based algorithms like Porter or Snowball stemmers.\nIt is fast but may not always produce valid words, making it less accurate than lemmatization.\n\n\n\n14.2.6 Lemmatization\n\nLemmatization maps words to their base or dictionary form (lemma) using vocabulary and POS information.\nIt is more accurate than stemming but slower and requires more resources.\n\n\n\n14.2.7 Regular Expressions\n\nRegular expressions (regex) are used for pattern matching and text manipulation, such as searching, extracting, or replacing text patterns.\nPython’s re module provides methods like match, search, findall, split, and sub for regex operations.\n\n\n\n14.2.8 POS Tagging\n\nPart-of-speech (POS) tagging assigns grammatical categories (noun, verb, adjective, etc.) to each word in a text.\nMethods include rule-based (using hand-crafted rules) and statistical (using machine learning models); spaCy and NLTK both support POS tagging.\n\n\n\n14.2.9 Named Entity Recognition (NER)\n\nNER identifies and classifies named entities (persons, organizations, locations, dates, etc.) in text.\nIt is crucial for extracting structured information from unstructured text and is supported by both NLTK and spaCy.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Concepts and Explanations Long</span>"
    ]
  },
  {
    "objectID": "summary.html#concepts-and-explanations---chapter-3",
    "href": "summary.html#concepts-and-explanations---chapter-3",
    "title": "14  Concepts and Explanations Long",
    "section": "14.3 Concepts and Explanations - Chapter 3",
    "text": "14.3 Concepts and Explanations - Chapter 3\n\n14.3.1 N-Gram Language Models\n\nN-gram models predict the probability of a word based on the previous n-1 words, commonly used for tasks like speech recognition and text generation.\nTypes include unigram (single word), bigram (pair), and trigram (triplet), with the number of n-grams calculated as (total words) - (n - 1).\n\n\n\n14.3.2 Conditional Probability and Chain Rule\n\nJoint probability measures the likelihood of multiple events occurring together, while conditional probability measures the likelihood of one event given another.\nThe chain rule allows calculation of the probability of a word sequence by multiplying conditional probabilities for each word given its context.\n\n\n\n14.3.3 Markov Assumption\n\nThe Markov assumption simplifies language modeling by assuming a word depends only on the previous n-1 words, not the entire history.\nFirst-order (bigram) and second-order (trigram) Markov models are common, reducing computational complexity.\n\n\n\n14.3.4 N-Gram Probability Calculation & MLE\n\nThe probability of a word sequence is estimated by counting n-gram occurrences and dividing by the count of the previous (n-1)-gram.\nMaximum Likelihood Estimation (MLE) can assign zero probability to unseen n-grams, which is a limitation for generalization.\n\n\n\n14.3.5 Padding\n\nPadding uses special tokens (e.g., &lt;s&gt;, &lt;/s&gt;) at sentence boundaries to handle cases where context is missing at the start or end.\nThe number of padding tokens depends on the n-gram order (e.g., two for trigrams).\n\n\n\n14.3.6 Underflow\n\nUnderflow occurs when multiplying many small probabilities, resulting in values too small for computers to represent.\nUsing log probabilities converts multiplication into addition, preventing underflow.\n\n\n\n14.3.7 Smoothing Techniques\n\nSmoothing addresses zero probabilities for unseen n-grams by assigning them small non-zero values.\nTechniques include Laplace (add-one), add-k, Good-Turing, backoff/interpolation, and Kneser-Ney smoothing, each with different strategies for adjusting probabilities.\n\n\n\n14.3.8 Perplexity\n\nPerplexity measures how well a language model predicts a text; lower perplexity indicates a better model.\nIt is calculated as the inverse probability of the test set, normalized by the number of words.\n\n\n\n14.3.9 Practical Implementation\n\nSteps for building an n-gram model include tokenizing text, adding padding, generating n-grams, counting unique n-grams, calculating probabilities, and evaluating with perplexity.\nPython code examples demonstrate these steps, including smoothing and sentence generation.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Concepts and Explanations Long</span>"
    ]
  },
  {
    "objectID": "summary.html#concepts-and-explanations---chapter-4",
    "href": "summary.html#concepts-and-explanations---chapter-4",
    "title": "14  Concepts and Explanations Long",
    "section": "14.4 Concepts and Explanations - Chapter 4",
    "text": "14.4 Concepts and Explanations - Chapter 4\n\n14.4.1 Text Classification\n\nText classification assigns predefined categories to text documents using supervised, unsupervised, or deep learning methods.\nIt is widely used in applications like spam detection, sentiment analysis, and topic categorization.\nThe process involves data preprocessing, feature extraction, model training, and evaluation, with the choice of features and algorithms affecting performance.\n\n\n\n14.4.2 Types of Text Classification Techniques\n\nSupervised Learning: Uses labeled data to train models (e.g., Naive Bayes, Logistic Regression).\nUnsupervised Learning: Finds patterns or clusters in unlabeled data (e.g., LDA, K-means).\nDeep Learning: Employs neural networks to automatically learn complex features (e.g., CNNs, RNNs, Transformers).\n\n\n\n14.4.3 Bayes Theorem\n\nBayes theorem relates conditional probabilities and is used to update the probability of a hypothesis given new evidence.\nIn Naive Bayes, the “naive” assumption is that features are conditionally independent given the class label, simplifying probability calculations.\n\n\n\n14.4.4 Types of Naive Bayes Classifiers\n\nGaussian Naive Bayes: For continuous features, assumes a normal distribution.\nMultinomial Naive Bayes: For discrete features like word counts.\nBernoulli Naive Bayes: For binary features, such as word presence/absence.\n\n\n\n14.4.5 Implementing Naive Bayes\n\nSteps include loading and preprocessing data, splitting into train/test sets, extracting features (e.g., Bag of Words), training the classifier, and evaluating performance.\n\n\n\n14.4.6 Evaluation Metrics\n\nAccuracy: Proportion of correct predictions.\nPrecision: Proportion of true positives among predicted positives.\nRecall: Proportion of true positives among actual positives.\nF1-Score: Harmonic mean of precision and recall, useful for imbalanced datasets.\nClassification Report: Summarizes these metrics for each class.\n\n\n\n14.4.7 Disadvantages of Naive Bayes\n\nThe independence assumption may not hold, leading to suboptimal results.\nZero probability for unseen features can be problematic.\nProbability estimates may be biased, especially for rare events.\nPerformance drops with highly correlated features or large, sparse feature spaces.\nAs a linear classifier, it may not capture complex relationships.\n\n\n\n14.4.8 Handling Class Imbalance\n\nResampling: Oversample the minority class or undersample the majority class to balance the dataset.\nClass Weights: Assign higher weights to the minority class to penalize misclassification.\nEnsemble Methods: Use models like Random Forest or Gradient Boosting to improve robustness and performance on imbalanced data.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Concepts and Explanations Long</span>"
    ]
  },
  {
    "objectID": "summary.html#concepts-and-explanations---chapter-5",
    "href": "summary.html#concepts-and-explanations---chapter-5",
    "title": "14  Concepts and Explanations Long",
    "section": "14.5 Concepts and Explanations - Chapter 5",
    "text": "14.5 Concepts and Explanations - Chapter 5\n\n14.5.1 Logistic Regression\n\nLogistic regression is a linear model for binary classification that estimates the probability of a class using the logistic (sigmoid) function.\nIt outputs probabilities between 0 and 1, making it suitable for tasks like predicting customer churn or disease diagnosis.\nThe model learns coefficients for each feature and a bias term, which together determine the decision boundary.\n\n\n\n14.5.2 Log-Odds (Logit)\n\nThe log-odds (logit) is the logarithm of the odds ratio, representing the relationship between the probability of an event and its complement.\nIn logistic regression, the log-odds are modeled as a linear combination of input features and their coefficients.\nThis transformation allows for a linear relationship between features and the log-odds, even though the probability itself is non-linear.\n\n\n\n14.5.3 Training Logistic Regression\n\nCoefficients are estimated using Maximum Likelihood Estimation (MLE), which finds the values that maximize the likelihood of the observed data.\nRegularization techniques like L2 (Ridge), L1 (Lasso), and Elastic Net help prevent overfitting and can perform feature selection.\nScikit-learn offers several solvers (liblinear, saga, newton-cg, lbfgs) optimized for different dataset sizes and regularization types.\n\n\n\n14.5.4 Bag of Words vs. TF-IDF\n\nBag of Words (BoW) represents text as word count vectors, ignoring word order and context.\n\nSimple and effective for many tasks, but can result in high-dimensional, sparse data and does not capture word importance.\n\nTF-IDF weighs words by their frequency in a document and their rarity across the corpus, highlighting more informative words.\n\nBetter for capturing word importance and semantic relevance, but still ignores word order and can be complex to implement.\n\n\n\n\n14.5.5 LR Model Assumptions\n\nAssumes a linear relationship between features and the log-odds of the outcome.\nObservations should be independent, and features should not be highly correlated (no multicollinearity).\nAssumes absence of extreme outliers, as these can distort model estimates.\n\n\n\n14.5.6 LR Limitations\n\nThe linearity assumption may not hold for all datasets, limiting model performance.\nSensitive to multicollinearity and outliers, which can affect coefficient stability and predictions.\nCannot capture complex, non-linear relationships between features and the target variable.\nRequires a sufficiently large sample size (at least 10 times the number of features) to produce reliable results.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Concepts and Explanations Long</span>"
    ]
  },
  {
    "objectID": "summary.html#concepts-and-explanations---chapter-6",
    "href": "summary.html#concepts-and-explanations---chapter-6",
    "title": "14  Concepts and Explanations Long",
    "section": "14.6 Concepts and Explanations - Chapter 6",
    "text": "14.6 Concepts and Explanations - Chapter 6\n\n14.6.1 Sentiment Analysis (SA)\n\nSentiment analysis determines the sentiment or opinion expressed in text, classifying it as positive, negative, or neutral.\nA sentiment lexicon is a collection of words or phrases associated with specific sentiment scores, used to analyze the sentiment of text.\nEkman’s Six Basic Emotions (happiness, sadness, anger, fear, surprise, disgust) provide a foundation for categorizing emotional content.\n\n\n\n14.6.2 Resources for Sentiment Lexicons\n\nVADER: Designed for social media, VADER combines lexical features and rules to handle negations and intensifiers, providing sentiment scores from -1 to 1.\nAFINN: Contains 3300+ English words rated for sentiment on a scale from -5 to +5, useful for social media and online reviews.\nSentiWordNet: Assigns positive, negative, and objective scores to WordNet synsets, enabling nuanced sentiment analysis based on word context.\nLIWC: Categorizes words into psychological and linguistic categories, offering insights into emotional and cognitive aspects of text.\n\n\n\n14.6.3 Sentiment Analysis Approaches\n\nRule-based: Uses predefined rules and lexicons to assign sentiment, relying on word lists and linguistic patterns. Simple and interpretable, but limited in handling context, sarcasm, and complex expressions.\nMachine learning-based: Trains models on labeled data to classify sentiment, using features like word frequencies or embeddings. More flexible and context-aware, but requires labeled data and may be less interpretable.\nHybrid: Combines rule-based and machine learning methods to leverage the strengths of both, often using lexicons for initial scoring and ML models for refinement.\n\n\n\n14.6.4 Model Comparison\n\nLexicon-based models (TextBlob, VADER) are simple and interpretable but struggle with negation and sarcasm.\nDeep learning models (LSTM, BERT) learn context and can handle complex sentiment, with BERT excelling at sarcasm and nuanced language.\n\n\n\n14.6.5 Python Libraries and Tools for SA\n\nNLTK: Provides tools for preprocessing and sentiment analysis, including VADER and WordNet integration.\nTextBlob: Offers a user-friendly API for sentiment analysis and other NLP tasks.\nTensorFlow and PyTorch: Enable building and training deep learning models for sentiment analysis.\nHugging Face Transformers: Supplies pre-trained models (e.g., BERT) for advanced sentiment analysis and other NLP tasks.\n\n\n\n14.6.6 SA Challenges\n\nLexicon-based methods struggle with context, sarcasm, ambiguity, cultural/domain-specific language, and negation.\nWords can have different meanings depending on context, and sarcasm or irony can invert the intended sentiment, making accurate classification difficult.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Concepts and Explanations Long</span>"
    ]
  },
  {
    "objectID": "summary.html#concepts-and-explanations---chapter-7",
    "href": "summary.html#concepts-and-explanations---chapter-7",
    "title": "14  Concepts and Explanations Long",
    "section": "14.7 Concepts and Explanations - Chapter 7",
    "text": "14.7 Concepts and Explanations - Chapter 7\n\n14.7.1 Topic Modeling\n\nTopic modeling is an unsupervised machine learning technique for discovering abstract topics in a collection of documents.\nIt helps organize, summarize, and search large text corpora by uncovering hidden thematic structures.\n\n\n\n14.7.2 Latent Dirichlet Allocation (LDA)\n\nLDA is a generative probabilistic model where each document is a mixture of topics, and each topic is a distribution over words.\nThe model infers hidden topics by assuming documents are generated by mixing topics, and topics are generated by mixing words.\n\n\n\n14.7.3 LDA Intuition and Components\n\nEach document has a topic distribution, and each topic has a word distribution.\nThe model assigns each word in a document to a topic, aiming to uncover the latent structure that best explains the observed words.\n\n\n\n14.7.4 LDA Parameters and Trade-offs\n\nKey parameters: document density (α), topic word density (β), and number of topics (K).\nLDA balances document sparsity (few topics per document) and topic sparsity (few words per topic).\n\n\n\n14.7.5 Parameter Estimation in LDA\n\nLDA uses Bayes’ Theorem to estimate hidden variables, but exact inference is intractable.\nCommon estimation methods include Expectation Maximization, Variational Inference, and Gibbs sampling (a Markov Chain Monte Carlo method).\n\n\n\n14.7.6 Gibbs Sampling in LDA\n\nGibbs sampling iteratively updates topic assignments for each word, approximating the true topic structure.\nIt encourages sparsity by favoring few topics per document and few words per topic.\n\n\n\n14.7.7 Practical Considerations for LDA\n\nLDA requires specifying the number of topics and works best with well-preprocessed, longer documents.\nIt assumes a bag-of-words model, ignoring word order and semantic context.\n\n\n\n14.7.8 Visualizing and Using LDA Results\n\nLDA outputs document-topic and word-topic distributions, which can be used for document similarity, clustering, and visualization.\n\n\n\n14.7.9 Top2Vec\n\nTop2Vec is a modern topic modeling algorithm that embeds documents and words in a vector space, clusters them, and finds topics without needing to specify the number of topics.\nIt uses dimensionality reduction (UMAP) and clustering (HDBSCAN), requiring minimal preprocessing but is computationally intensive.\n\n\n\n14.7.10 BERTopic\n\nBERTopic builds on Top2Vec by using transformer-based embeddings (e.g., BERT) for better topic quality, especially in nuanced or short texts.\nIt uses class-based TF-IDF for topic extraction and is resource-intensive.\n\n\n\n14.7.11 K-means Clustering with Word2Vec\n\nDocuments are embedded using Word2Vec and clustered with k-means, making it suitable for short texts.\nThis approach is simple and fast but requires specifying the number of clusters.\n\n\n\n14.7.12 The Revolution in NLP: Transformers and LLMs\n\nThe transformer model has revolutionized NLP, enabling large language models (LLMs) trained on massive datasets with objectives like masked language modeling.\nLLMs are further improved with Reinforcement Learning from Human Feedback (RLHF), aligning models with human preferences.\n\n\n\n14.7.13 Language is Hard\n\nLanguage is challenging for AI due to infinite possibilities, ambiguity (lexical and structural), and context dependence.\nLLMs have made significant progress but true human-like understanding remains debated.\n\n\n\n14.7.14 Brief History: Turing Test and AGI Benchmarks\n\nThe Turing Test evaluates if a machine’s behavior is indistinguishable from a human.\nAGI benchmarks include tasks like passing university exams, performing jobs, assembling furniture, and adapting to new environments.\n\n\n\n14.7.15 NLP: The Basic Approach\n\nText data is converted into structured representations (e.g., bag-of-words, TF-IDF) for machine learning.\nSupervised ML enables tasks like spam detection and sentiment analysis, but bag-of-words ignores word order and context.\n\n\n\n14.7.16 Language Modeling with N-grams\n\nN-gram models assign probabilities to word sequences using the Markov assumption, capturing local word order but struggling with long-range dependencies.\n\n\n\n14.7.17 Training an LLM\n\nLLMs are trained to predict the next word in a sequence using neural networks and the softmax function.\nTraining involves computing loss based on prediction accuracy and updating model weights iteratively.\n\n\n\n14.7.18 Probing GPT and LLM Capabilities\n\nLLMs generate coherent, grammatical text and demonstrate understanding of linguistic structure, challenging previous assumptions about language learning.\n\n\n\n14.7.19 Applications of LLMs\n\nLLMs are used in customer support, research portals, automated help desks, and accessibility tools, with growing societal impact.\n\n\n\n14.7.20 Takeaways\n\nLanguage is complex and ambiguous, but LLMs are rapidly advancing toward human-level language ability.\nThe field is evolving quickly, with exciting research and significant real-world applications.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Concepts and Explanations Long</span>"
    ]
  },
  {
    "objectID": "summary.html#concepts-and-explanations---chapter-8",
    "href": "summary.html#concepts-and-explanations---chapter-8",
    "title": "14  Concepts and Explanations Long",
    "section": "14.8 Concepts and Explanations - Chapter 8",
    "text": "14.8 Concepts and Explanations - Chapter 8\n\n14.8.1 Word Meanings\n\nUnderstanding word meanings is central to NLP, but meanings are often complex and context-dependent.\nWords can have multiple unrelated meanings (homonymy) or multiple related senses (polysemy), and the mapping between words and concepts is many-to-many.\n\n\n\n14.8.2 Word Relations\n\nSynonymy: Words with similar meanings (e.g., “big” and “large”).\nAntonymy: Words with opposite meanings (e.g., “hot” and “cold”).\nSimilarity and Relatedness: Words can be similar (e.g., “cup” and “mug”) or just related (e.g., “doctor” and “hospital”).\nConnotation: Words carry emotional or cultural associations beyond their literal meaning (e.g., “childish” vs. “youthful”).\n\n\n\n\n14.8.3 Vector Semantics\n\nVector semantics represents words and documents as points (vectors) in a multidimensional space, allowing mathematical comparison of meanings.\nThe distributional hypothesis states that words appearing in similar contexts tend to have similar meanings (“You shall know a word by the company it keeps”).\nWords or documents are represented as vectors, often using term-document or word-context matrices.\n\n\n14.8.3.1 Comparing Word Vectors\n\nCosine Similarity: Measures the angle between two vectors, indicating how similar their directions are.\nEuclidean Distance: Measures the straight-line distance between vectors, reflecting overall difference.\nDot Product: Quantifies similarity based on both direction and magnitude of vectors.\nThese metrics help quantify semantic similarity or relatedness between words or documents.\n\n\n\n\n\n14.8.4 Word2Vec\n\n14.8.4.1 Learning the Embeddings\n\nWord2Vec learns dense vector representations (embeddings) for words by training a classifier to distinguish real context pairs from random pairs.\nThe model uses self-supervision: it learns from the natural co-occurrence of words in text, without human annotation.\n\n\n\n14.8.4.2 How Word2Vec Learns Embeddings\n\nFor each word, the model pairs it with nearby context words (positive examples) and with random words (negative examples).\nLogistic regression is used to train the model to distinguish positive from negative pairs, updating word vectors to maximize correct classification.\nAfter training, the classifier is discarded and the learned vectors are used as word embeddings.\n\n\n\n14.8.4.3 Analogy Reasoning with Embeddings\n\nWord2Vec embeddings capture abstract relationships, enabling analogy tasks (e.g., “man” is to “king” as “woman” is to “queen”).\nVector arithmetic on embeddings can reveal such relationships, demonstrating the power of learned representations.\n\n\n\n\n\n14.8.5 Takeaways\n\nVector Semantics: Dense vector representations enable mathematical operations on word meanings and support a wide range of NLP tasks.\nWord2Vec: Provides powerful, static word embeddings that capture semantic and syntactic relationships, useful for translation, sentiment analysis, and more.\nPractical steps include loading pre-trained embeddings, exploring similarities, solving analogies, and using embeddings for sentence classification or comparison with Bag-of-Words models.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Concepts and Explanations Long</span>"
    ]
  },
  {
    "objectID": "summary.html#concepts-and-explanations---chapter-9",
    "href": "summary.html#concepts-and-explanations---chapter-9",
    "title": "14  Concepts and Explanations Long",
    "section": "14.9 Concepts and Explanations - Chapter 9",
    "text": "14.9 Concepts and Explanations - Chapter 9\n\n14.9.1 Ambiguity in Language\n\nAmbiguity occurs when a sentence or word can be interpreted in more than one way, posing a challenge for NLP systems.\nTypes include lexical ambiguity (a word with multiple meanings) and structural ambiguity (a sentence with multiple possible parses).\nAddressing ambiguity often requires models that capture sentence structure, such as parse trees or dependency graphs.\n\n\n\n\n14.9.2 Simple Neural Networks and Neural Language Models\n\nNeural networks are composed of layers of units (neurons) that compute weighted sums of inputs, add a bias, and apply an activation function.\nThe basic structure includes an input layer, one or more hidden layers, and an output layer, enabling the network to learn complex mappings from input to output.\n\n\n\n\n14.9.3 Activation Functions\n\nSigmoid maps values to (0, 1), useful for probabilities but can cause vanishing gradients.\nTanh maps values to (-1, 1), is zero-centered, and often preferred over sigmoid.\nReLU outputs the input if positive, otherwise zero; it is computationally efficient and helps mitigate vanishing gradients.\nActivation functions introduce non-linearity, allowing networks to model complex relationships.\n\n\n\n\n14.9.4 Multi-Layer Neural Networks and XOR\n\nA single-layer perceptron can only solve linearly separable problems (like AND/OR), but not XOR, which is not linearly separable.\nMulti-layer networks with non-linear activation functions can learn complex functions like XOR by forming new representations in hidden layers.\nThis demonstrates the power of deep neural networks to model non-linear patterns.\n\n\n\n\n14.9.5 Feedforward Neural Networks (Multilayer Perceptrons)\n\nFeedforward networks consist of multiple layers where information flows from input to output without cycles.\nEach layer transforms its input through learned weights and activation functions, enabling the network to learn hierarchical representations.\nAdding hidden layers allows the network to capture more complex patterns than simple linear models.\n\n\n\n\n14.9.6 Applying Feedforward Networks to NLP Tasks\n\nFeedforward networks can be used for text classification, language modeling, and other NLP tasks by learning from word embeddings rather than hand-crafted features.\nThey can handle variable-length input using padding, truncation, or pooling methods (mean/max pooling).\nFor multi-class classification, a softmax output layer is used to produce class probabilities.\n\n\n\n\n14.9.7 Training Neural Networks\n\nTraining involves adjusting weights to minimize prediction error using backpropagation and gradient descent.\nThe process includes a forward pass (prediction), loss computation, backward pass (gradient calculation), and weight updates.\nTraining is iterative, and the learning rate controls the size of each update step; backpropagation efficiently computes gradients for all weights.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Concepts and Explanations Long</span>"
    ]
  },
  {
    "objectID": "summary.html#concepts-and-explanations---chapter-10",
    "href": "summary.html#concepts-and-explanations---chapter-10",
    "title": "14  Concepts and Explanations Long",
    "section": "14.10 Concepts and Explanations - Chapter 10",
    "text": "14.10 Concepts and Explanations - Chapter 10\n\n14.10.1 Descartes and the Nature of Thought\n\nPhilosophers have debated whether machines can truly “think” or only simulate understanding, raising questions about the nature of intelligence in AI.\nThis philosophical background frames the challenge of building AI that genuinely understands language.\n\n\n\n14.10.2 The Turing Test\n\nProposed by Alan Turing, this test evaluates if a machine’s responses are indistinguishable from a human’s in conversation.\nPassing the Turing Test is seen as a milestone for demonstrating machine intelligence.\n\n\n\n14.10.3 Singularity and AGI\n\nThe Singularity is a hypothetical point where AI surpasses human intelligence, leading to rapid technological change.\nAGI (Artificial General Intelligence) refers to machines capable of performing any intellectual task that humans can do.\n\n\n\n\n14.10.4 The Challenge of Language\n\nLanguage allows infinite expression with finite words and rules, making it complex for machines to process.\nSentences can have long-range dependencies, requiring models to capture relationships between distant words.\n\n\n\n\n14.10.5 Miracle of LLMs – 3 Key Insights\n\nBag of Words / N-grams: Early models represented text as unordered words or short sequences, missing deeper context.\nWord Embeddings: Words are mapped to vectors capturing semantic relationships, enabling analogies and richer understanding.\nThe Transformer: This architecture allows models to consider the entire context of a sentence, not just local word sequences.\n\n\n\n\n14.10.6 Word Embeddings\n\nWord embeddings are high-dimensional vectors representing word meanings and relationships (e.g., gender, capitals, comparatives).\nThey enable models to perform analogy reasoning and capture semantic similarities.\n\n\n\n\n14.10.7 Transformers\n\n14.10.7.1 The Problem with Static Embeddings\n\nStatic embeddings assign the same vector to a word regardless of context, failing to capture context-dependent meanings.\n\n\n\n14.10.7.2 Contextual Embeddings\n\nContextual embeddings give each word a unique vector in each context, allowing the model to distinguish meanings based on surrounding words.\n\n\n\n14.10.7.3 Attention Mechanism\n\nAttention enables each word to “attend” to other words in a sentence, integrating relevant information to build context-aware representations.\nThis mechanism allows models to resolve ambiguities and capture complex dependencies.\n\n\n\n14.10.7.4 Position Embeddings\n\nSince transformers lack inherent word order, position embeddings are added to word vectors to encode sequence information.\n\n\n\n14.10.7.5 Output: Logits and Softmax\n\nThe model outputs logits (raw scores) for each vocabulary word, which are converted to probabilities using the softmax function.\n\n\n\n\n\n14.10.8 BERT and Decoder-Only Models\n\n14.10.8.1 BERT (Encoder-only)\n\nBERT produces rich, context-aware embeddings for each token and is best for classification and embedding tasks.\nIt uses a special [CLS] token for sentence-level tasks and is fine-tuned for specific applications.\n\n\n\n14.10.8.2 Decoder-Only Models (e.g., GPT)\n\nThese models generate text by predicting the next word in a sequence, making them suitable for text generation and chatbots.\n\n\n\n\n\n14.10.9 Large Language Models (LLMs)\n\nLLMs are neural networks trained to predict the next word, enabling them to generate coherent and contextually relevant text.\nThey learn language, facts, and reasoning by training on massive datasets.\n\n\n\n\n14.10.10 Decoding and Sampling\n\nDecoding is the process of choosing the next word during text generation, balancing quality and diversity.\nMethods include random sampling, top-k, top-p (nucleus) sampling, and temperature adjustment, each affecting output creativity and coherence.\n\n\n\n\n14.10.11 Pretraining and Self-Supervised Learning\n\nLLMs are pretrained on large text corpora using self-supervised learning, where the next word prediction serves as the training signal.\nCross-entropy loss is used to optimize the model, and pretraining imparts factual, linguistic, and commonsense knowledge.\n\n\n\n\n14.10.12 Working with Large Language Models\n\n14.10.12.1 Model Types\n\nBase Model: Pretrained on large corpora.\nInstruct Model: Fine-tuned to follow instructions.\nChat Model: Further tuned for dialogue.\n\n\n\n14.10.12.2 Key Settings\n\nParameters like temperature, top-p, max length, and penalties control output randomness, diversity, and length.\n\n\n\n14.10.12.3 Prompt Structure and Techniques\n\nEffective prompts include instructions, context, input data, and output indicators.\nPrompting techniques: zero-shot (no examples), few-shot (with examples), and chain-of-thought (step-by-step reasoning).\n\n\n\n\n\n14.10.13 Takeaways\n\nTransformers use attention for context-aware word representations.\nBERT is best for classification and embeddings; GPT is best for text generation.\nPrompt engineering is crucial for effective use of LLMs.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Concepts and Explanations Long</span>"
    ]
  },
  {
    "objectID": "summary.html#concepts-and-explanations---chapter-11",
    "href": "summary.html#concepts-and-explanations---chapter-11",
    "title": "14  Concepts and Explanations Long",
    "section": "14.11 Concepts and Explanations - Chapter 11",
    "text": "14.11 Concepts and Explanations - Chapter 11\n\n14.11.1 Information Retrieval (IR)\n\nIR is the process of finding relevant documents from a large collection based on a user’s query.\nDocuments and queries are represented as vectors (e.g., bag-of-words, TF-IDF), and similarity (often cosine similarity) is used to rank results.\nKey metrics include precision (fraction of retrieved documents that are relevant) and recall (fraction of relevant documents that are retrieved).\n\n\n\n14.11.2 Vector Space Model & TF-IDF\n\nThe vector space model represents documents and queries as vectors in a high-dimensional space, enabling similarity computation.\nTF-IDF (Term Frequency–Inverse Document Frequency) weighs terms by their importance: frequent in a document but rare in the collection.\nThis approach helps distinguish important terms and improves retrieval effectiveness.\n\n\n\n14.11.3 Dense Retrieval with Neural Models\n\nDense retrieval uses neural networks (e.g., BERT) to encode queries and documents into dense embeddings.\nSimilarity between embeddings is used to match queries and documents, allowing for semantic matching beyond exact word overlap.\nThis helps address vocabulary mismatch, where different words or phrases express the same meaning.\n\n\n\n14.11.4 Question Answering Datasets\n\nDatasets like SQuAD provide passages and questions, with answers as text spans within the passage.\nSQuAD 2.0 introduces unanswerable questions to test model robustness.\nThese datasets are benchmarks for evaluating extractive QA systems.\n\n\n\n14.11.5 Extractive Question Answering\n\nExtractive QA involves selecting a span of text from a passage as the answer to a question.\nModels (e.g., BERT) predict the start and end positions of the answer span within the passage.\nThis approach is widely used for reading comprehension tasks.\n\n\n\n14.11.6 Entity Linking (Wikification)\n\nEntity linking maps mentions in text to specific entities in a knowledge base (e.g., Wikipedia pages).\nIt involves mention detection, candidate generation, and disambiguation based on context.\nThis process grounds text to structured knowledge, improving retrieval and reasoning.\n\n\n\n14.11.7 Knowledge-based Question Answering (KBQA)\n\nKBQA answers questions by mapping them to queries over structured knowledge bases (e.g., DBpedia, Wikidata).\nApproaches include graph-based QA (traversing entity-relation graphs) and semantic parsing (mapping questions to logical forms).\nRDF triples (subject, predicate, object) are a common data structure for representing facts.\n\n\n\n14.11.8 Supervision for Semantic Parsing\n\nFully supervised approaches use annotated logical forms for each question.\nWeakly supervised approaches use only the answer (denotation), treating the logical form as a latent variable.\nSupervision level affects the complexity and scalability of KBQA systems.\n\n\n\n14.11.9 Retrieve-and-Generate (RAG)\n\nRAG combines retrieval of relevant documents or passages with generative LLMs to produce answers.\nThe workflow involves retrieving relevant chunks, combining them with the question, and generating an answer using an LLM.\nThis approach enables up-to-date and contextually grounded answers.\n\n\n\n14.11.10 Evaluation of RAG Systems\n\nRetrieval evaluation checks if relevant chunks are retrieved (precision, recall).\nGeneration evaluation assesses if the answer is correct, relevant, and faithful to the retrieved context (metrics: BLEU, ROUGE, LLM-based judgments).\nFaithfulness and answer/context relevance are key criteria; hallucinations should be avoided.\n\n\n\n14.11.11 Agents\n\nAgents are systems capable of forming intentions, making plans, and acting to achieve goals, often using internal representations.\nIn practice, LLM-based agents control application flow (e.g., LangChain agents) and can process text, audio, or visual input via modality fusion.\nAgents extend LLMs’ capabilities to multi-step reasoning and action-taking.\n\n\n\n14.11.12 Formal vs. Functional Abilities of LLMs\n\nFormal linguistic competence refers to knowledge of grammar and language rules; functional competence is the ability to use language in real-world contexts.\nLLMs excel at formal competence but may struggle with functional competence, such as real-world reasoning and understanding.\nEvaluation should consider both types of competence and be aware of common fallacies (e.g., “good at language → good at thought”).\n\n\n\n14.11.13 Takeaways\n\nQuestion answering can be approached via IR, KBQA, or LLM-based methods (including RAG and agents).\nKey datasets include SQuAD, SimpleQuestions, and RDF-based resources like DBpedia.\nLLMs are powerful for formal linguistic tasks, but functional, real-world understanding remains a challenge.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Concepts and Explanations Long</span>"
    ]
  }
]