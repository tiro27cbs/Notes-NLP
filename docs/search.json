[
  {
    "objectID": "session1.html",
    "href": "session1.html",
    "title": "2  Natural Language Processing - Chapter 1",
    "section": "",
    "text": "3 Chapter 1: Introduction to Natural Language Processing",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Natural Language Processing - Chapter 1</span>"
    ]
  },
  {
    "objectID": "session1.html#the-revolution-in-nlp",
    "href": "session1.html#the-revolution-in-nlp",
    "title": "2  Natural Language Processing - Chapter 1",
    "section": "3.1 The Revolution in NLP",
    "text": "3.1 The Revolution in NLP\nNatural Language Processing (NLP) has undergone a revolution in recent years, primarily due to the introduction of the transformer model. Transformers are trained on massive datasets using objectives like masked language modeling (predicting missing words in a sentence). Further improvements are achieved through Reinforcement Learning from Human Feedback (RLHF), allowing models to better align with human preferences.\n\nExample:\nGiven the sentence: “The cat sat on the ___,” a transformer model predicts the missing word, such as “mat”.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Natural Language Processing - Chapter 1</span>"
    ]
  },
  {
    "objectID": "session1.html#language-is-hard",
    "href": "session1.html#language-is-hard",
    "title": "2  Natural Language Processing - Chapter 1",
    "section": "3.2 Language is Hard",
    "text": "3.2 Language is Hard\nDespite impressive progress, language remains a challenging domain for AI.\n\n3.2.1 Why is Language Hard?\n\nInfinite Possibilities:\nMost sentences you hear are unique—you’ve never heard them before and may never hear them again.\nAmbiguity:\n\nLexical Ambiguity: Words can have multiple meanings.\n&gt; Example: “bank” (river bank vs. financial bank)\nStructural Ambiguity: Sentences can be interpreted in different ways.\n&gt; Example: “I saw the man with the telescope.” (Who has the telescope?)\n\n\nMany thinkers have argued that true human-level language understanding may be impossible for machines. Current LLMs (Large Language Models) appear to process and reason with language at a high level, but is this really human-like understanding?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Natural Language Processing - Chapter 1</span>"
    ]
  },
  {
    "objectID": "session1.html#brief-history",
    "href": "session1.html#brief-history",
    "title": "2  Natural Language Processing - Chapter 1",
    "section": "3.3 Brief History",
    "text": "3.3 Brief History\n\nDescartes:\nArgued that a machine could never truly imitate a human; there would always be a way to tell the difference.\nTuring Test:\nProposed by Alan Turing as a test of a machine’s ability to exhibit intelligent behavior indistinguishable from a human.\n\nA human judge engages in a conversation with both a human and a machine.\nIf the judge cannot reliably tell which is which, the machine is said to have passed the test.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Natural Language Processing - Chapter 1</span>"
    ]
  },
  {
    "objectID": "session1.html#nlp-the-basic-approach",
    "href": "session1.html#nlp-the-basic-approach",
    "title": "2  Natural Language Processing - Chapter 1",
    "section": "3.4 NLP: The Basic Approach",
    "text": "3.4 NLP: The Basic Approach\n\n3.4.1 Background\n\nNumerical Data: Numbers, measurements, etc.\nCategorical Data: Discrete categories or labels.\nText Data: Unstructured, variable-length, and context-dependent (e.g., email content, headlines, political speeches).\n\nText data is fundamentally different from structured data. Can we treat language as structured data for machine learning?\n\n3.4.1.1 Making Language into Structured Data\n\nBag-of-Words Representation:\n\nAssign a feature (column) for each word in the vocabulary.\nFor a given text, the value is 1 if the word occurs, 0 otherwise.\nAlternative values: word counts or TF-IDF scores.\nMost features are 0 for any given text (sparse representation).\n\n\n\n\n3.4.1.2 Supervised ML for Text Processing\n\nLabeled Text Data: Enables building classifiers for:\n\nSpam Detection\nSentiment Analysis\nTopic Detection\n…and more\n\nRaises questions: Does this approach capture real understanding? How does it relate to the Turing Test?\n\n\n\n3.4.1.3 Bag-of-Words Limitation\n\nIgnores word order and context (“bag” of words).\nCannot distinguish between “dog bites man” and “man bites dog”.\n\n\n\n\n\n3.4.2 Language Modeling with N-grams\n\nGoal: Assign a probability to a sequence of words.\nMarkov Assumption: The probability of a word depends only on the previous n-1 words.\n\n\n3.4.2.1 Example: “He went to the store”\n\nUnigrams (1-grams): He, went, to, the, store\nBigrams (2-grams): He went, went to, to the, the store\nTrigrams (3-grams): He went to, went to the, to the store\n4-grams: He went to the, went to the store\n5-gram: He went to the store\n\n\n\n3.4.2.2 N-gram Approximations\n\nBigram Model:\n( p() = p() p(|) p(|) p(|) p(|) )\nTrigram Model:\n( p() = p() p(|) p(|) p(|) p(|) )\nKey Idea:\nN-gram models capture some local word order, but struggle with long-range dependencies and rare phrases.\n\n\n\n\n\n3.4.3 Training an LLM\n\n“A general language model (LM) should be able to compute the probability of (and also generate) any string.”\n(Radford et al., 2019)\n\n\n\n3.4.4 What Does Training an LLM Look Like?\nConsider how humans complete sentences: - As Descartes said, “I think, therefore I .” - For all intents and - *I learned how to drive a ___*\nOr in dialogue: &gt; Monica: Okay, everybody relax. This is not even a ___\n&gt; Rachel: Oh God… well, it started about a half hour before the ___\n&gt; Ross: (squatting and reading the instructions) I’m supposed to attach a brackety ___\nThe core training objective for LLMs is next word prediction:\nGiven a sequence of words, predict the most likely next word.\n\n3.4.4.1 Neural Network for Next Word Prediction\n\nThe model is a neural network that outputs a score for every word in the vocabulary.\nThese scores are converted into probabilities using the softmax function.\nFor example, with a vocabulary of 50,000 words, the output might look like: [fish: 0.00002, help: 0.00002, ..., the: 0.00002, ..., aardvarks: 0.00002]\n\n\n\n3.4.4.2 Training Process\n\nCompute Loss:\n\nThe true next word is masked (hidden).\nThe model predicts probabilities for all words.\nThe loss function measures how well the model predicts the correct word (e.g., Loss = 1 - prob(correct word)).\nIf the model assigns a probability of 1 to the correct word, loss is 0 (best). If 0, loss is 1 (worst).\n\nUpdate Weights:\n\nThe model adjusts its internal weights to increase the probability of the correct word.\nThis also slightly decreases the probability for all other words.\nEach training example provides a small update—repeated millions or billions of times.\n\n\n\n\n3.4.4.3 Example: Weight Updates\nSuppose the correct next word is “fish”: - The model increases the weights leading to “fish”. - The probabilities for other words (e.g., “help”, “the”, “aardvarks”) are slightly reduced.\n\nEach example nudges the model to make the correct word more likely in context, and less likely for others.\nOver time, the model learns to predict words in a wide variety of contexts.\n\n\nThis is the fundamental process behind training large language models: predict the next word, compute the loss, update the weights, and repeat—at massive scale.\n\n\n\n3.4.5 Probing GPT\nLarge Language Models (LLMs) today generate highly coherent, grammatical text that can be indistinguishable from human output. They demonstrate some understanding of hierarchical structure and abstract linguistic categories (Mahowald et al., 2024).\nWhile these models are not perfect learners of abstract linguistic rules, neither are humans. LLMs are progressing toward acquiring formal linguistic competence and have already challenged claims about the impossibility of learning certain linguistic knowledge—such as hierarchical structure and abstract categories—from input alone (Mahowald et al., 2024).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Natural Language Processing - Chapter 1</span>"
    ]
  },
  {
    "objectID": "session1.html#ai-where-are-we-heading",
    "href": "session1.html#ai-where-are-we-heading",
    "title": "2  Natural Language Processing - Chapter 1",
    "section": "3.5 AI: Where Are We Heading?",
    "text": "3.5 AI: Where Are We Heading?\nArtificial General Intelligence (AGI):\nAGI is defined as AI that matches or surpasses human cognitive capabilities across a wide range of tasks.\nAGI Benchmarks:\n\nThe Robot College Student Test (Goertzel):\nA machine enrolls in a university, takes and passes the same classes as humans, and obtains a degree. LLMs can now pass university-level exams without attending classes.\nThe Employment Test (Nilsson):\nA machine performs an economically important job at least as well as humans. AI is already replacing humans in roles ranging from fast food to marketing.\nThe Ikea Test (Marcus):\nAn AI views the parts and instructions of an Ikea flat-pack product, then controls a robot to assemble the furniture correctly.\nThe Coffee Test (Wozniak):\nA machine enters an average home and figures out how to make coffee: find the machine, coffee, water, mug, and brew coffee by pushing the right buttons. This remains unsolved.\nThe Modern Turing Test (Suleyman):\nAn AI is given $100,000 and must turn it into $1 million.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Natural Language Processing - Chapter 1</span>"
    ]
  },
  {
    "objectID": "session1.html#applications-of-llms",
    "href": "session1.html#applications-of-llms",
    "title": "2  Natural Language Processing - Chapter 1",
    "section": "3.6 Applications of LLMs",
    "text": "3.6 Applications of LLMs\n\nAI interfaces for customer support and onboarding\nResearch portals with Retrieval-Augmented Generation (RAG)\nAutomated customer support (e.g., Zendesk)\nAccessibility tools (e.g., BeMyAI)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Natural Language Processing - Chapter 1</span>"
    ]
  },
  {
    "objectID": "session1.html#takeaways",
    "href": "session1.html#takeaways",
    "title": "2  Natural Language Processing - Chapter 1",
    "section": "3.7 Takeaways",
    "text": "3.7 Takeaways\n\nLanguage is Hard:\n\nLanguage is infinite and ambiguous (both lexically and structurally).\n\nThe Revolution in NLP:\n\nLLMs now approach human-level language ability.\n\nExciting Research Directions:\n\nBuilding applications with LLMs\nProbing their abilities\n\nPowerful AI is Coming:\n\nThe field is rapidly advancing, with significant societal impact on the horizon.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Natural Language Processing - Chapter 1</span>"
    ]
  },
  {
    "objectID": "session2.html",
    "href": "session2.html",
    "title": "3  Natural Language Processing - Chapter 2",
    "section": "",
    "text": "4 Chapter 2: Introduction to NLP Libraries and Text Normalization",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Natural Language Processing - Chapter 2</span>"
    ]
  },
  {
    "objectID": "session2.html#libraries",
    "href": "session2.html#libraries",
    "title": "3  Natural Language Processing - Chapter 2",
    "section": "4.1 Libraries",
    "text": "4.1 Libraries\n\n\n\n\n\n\n\n\n\nNLTK\nspaCy\n\n\n\n\nMethods\nstring processing library\nobject-oriented approach - it parses a text, returns document object whose words and sentences are objects themselves\n\n\nTokenization\nuses regular expression-based methods which are not always accurate\nuses a rule-based approach to tokenization which is more accurate\n\n\nPOS Tagging\nprovides wide range of POS taggers (ranging from rule-based to machine learning-based)\nuses a deep learning-based POS tagger which is more accurate (also offers pre-trained models in multiple languages)\n\n\nNamed Entity Recognition (NER)\nprovides multiple NER taggers (ranging from rule-based to machine learning-based)\nuses a highly efficient deep learning-based NER tagger for detecting entities such as names, organizations, locations, etc.\n\n\nPerformance\nslower than spaCy\nfaster than NLTK (due to its optimized implementation in Cython)\n\n\n\n\nTextblob is another popular library for NLP in Python. It is built on top of NLTK and provides a simple API for common NLP tasks such as tokenization, POS tagging, and sentiment analysis.\n\nIt is not good for large scale production use",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Natural Language Processing - Chapter 2</span>"
    ]
  },
  {
    "objectID": "session2.html#normalization",
    "href": "session2.html#normalization",
    "title": "3  Natural Language Processing - Chapter 2",
    "section": "4.2 Normalization",
    "text": "4.2 Normalization\nNormalization is the process of converting a text into a standard form. This involves removing any characters that are not part of the standard English alphabet, converting all characters to lowercase, and removing any extra spaces.\nTasks involved in normalization include\n\nTokenization: Breaking a text into words, phrases, symbols, or other meaningful elements.\nCase Folding: Converting all characters to lowercase.\nRemoving Punctuation: Removing any non-alphanumeric characters.\nRemoving Stopwords: Removing common words that do not carry much meaning.\nStemming or Lemmatization: Reducing words to their base or root form.\nRemoving Extra Spaces: Removing any extra spaces between words.\nExpanding Contractions: Expanding contractions such as “don’t” to “do not”.\nRemoving URLs and Emails: Removing URLs, email addresses, and other web-related content.\nRemoving HTML Tags: Removing HTML tags from web pages.\nRemoving Emojis and Special Characters: Removing emojis, emoticons, and other special characters.\n\nRemoval of Stopwords in Python using NLTK\n\ntext = \"NLTK and spaCy are popular NLP libraries used for text processing.\"\ntokens = word_tokenize(text)\nstop_words = set(stopwords.words(\"english\"))\nfiltered_tokens = [word for word in tokens if word.lower() not in stop_words]\nprint(f\"NLTK stopwords removal: {' '.join(filtered_tokens)}\")\n\nNLTK stopwords removal: NLTK spaCy popular NLP libraries used text processing .\n\n\nRemoval of Stopwords in Python using spaCy\n\nnlp = spacy.load(\"en_core_web_sm\") # Load English language model\ntext = \"NLTK and spaCy are popular NLP libraries used for text processing.\"\ndoc = nlp(text)\nfiltered_tokens = [token.text for token in doc if not token.is_stop]\nprint(f\"spaCy stopwords removal: {' '.join(filtered_tokens)}\")\n\nspaCy stopwords removal: NLTK spaCy popular NLP libraries text processing .\n\n\n\n4.2.0.1 Morphological Normalization\n\nMorphological normalization is the process of reducing a word to its base or root form\nThis can involve stemming or lemmatization\nRoots are the base forms of words eg. “run” is the root of “running”, “ran”, “runs”\nAffixes are the prefixes and suffixes that are added to roots to create different forms of words eg. “ing”, “ed”, “s”\n\nPrefixes are added to the beginning of a word eg. “un” in “undo”\nSuffixes are added to the end of a word eg. “ly” in “quickly”\n\nInflectional affixes are added to a word to change its grammatical form eg. “s” for plural nouns, “ed” for past tense verbs eg. “dogs”, “walked”\n\n\n\n4.2.1 Tokenization\n\nTokenization is the process of breaking a text into words, phrases, symbols, or other meaningful elements.\nThe tokens are the words, sentences, characters, or subwords that are produced by the tokenization process.\nSpace based token is used to prepare the text for further processing such as stemming, lemmatization, and POS tagging.\nType is number of unique tokens in a text\nToken is a single instance of a token in a text\n\n\ntokens = word_tokenize(text)\ntotal_tokens = len(tokens)\ntotal_types = len(set(tokens))\n\n\nCorpus is a collection of text documents\n\n\n4.2.1.0.1 Tokenization of raw text in Python\n\nword.isalnum() - returns True if all characters in the word are alphanumeric\n\n\nshakespeare_url = \"https://ocw.mit.edu/ans7870/6/6.006/s08/lecturenotes/files/t8.shakespeare.txt\"\ncrime_punishment_url = \"http://www.gutenberg.org/files/2554/2554-0.txt\"\n\n# Tokenize Raw Text\n\nshakespeare_text = request.urlopen(shakespeare_url).read().decode(\"utf8\")\ncrime_punishment_text = request.urlopen(crime_punishment_url).read().decode(\"utf8\")\n\nshakespeare_tokenized = [word for word in word_tokenize(shakespeare_text.lower()) if word.isalnum()]\ncrime_punishment_tokenized = [\n    word for word in word_tokenize(crime_punishment_text.lower()) if word.isalnum()\n]\n\n# FreqDist (from nltk) to produce a frequency distribution (listing top 20 most common words)\n\nshakespeare_freq = FreqDist(shakespeare_tokenized)\ncrime_freq = FreqDist(crime_punishment_tokenized)\n\nshakespeare_common = shakespeare_freq.most_common(20)\ncrime_common = crime_freq.most_common(20)\n\nprint(\"Top 20 words in Shakespeare\")\nprint(shakespeare_common)\n\nprint(\"\\nTop 20 words in Crime and Punishment:\")\nprint(crime_common)\n\n\n\n4.2.1.0.2 NLTK Tokenizers\n\nword_tokenize(): Tokenizes a text into words using a regular expression-based tokenizer. A versatile tokenizer that handles contractions, abbreviations, and punctuation marks effectively. It is suitable for most general-purpose tokenization tasks.\nWordPunctTokenizer(): Tokenizes a text into words using a regular expression that matches punctuation as separate tokens. A specialized tokenizer that separates words and punctuation explicitly. It is useful when you need to distinguish between alphabetic and non-alphabetic characters.\n\n\n\n\n4.2.2 Stemming\n\nStemming is the process of reducing a word to its root or base form. For example, the word “running” would be reduced to “run” by a stemming algorithm.\nStemmers remove word suffixes by running input word tokens against a pre-defined list of common suffixes.\nStemming is a heuristic process that does not always produce a valid root word, but it can be useful for text processing tasks such as search indexing and information retrieval.\nPorter Stemmer is a popular (rule-based) stemming algorithm that is widely used in text processing applications.\nSnowball Stemmer is an improved version of the Porter Stemmer that supports multiple languages.\n\n\n4.2.2.1 Example of stemming in Python using NLTK\n\nsbs = SnowballStemmer(\"english\")\ntext=\"The stars twinkled in the dark, illuminating the night sky.\"\nmethod = TreebankWordTokenizer()\nword_tokens = method.tokenize(text)\nstemmed = [sbs.stem(token) for token in word_tokens]\n\nfor i in range(len(word_tokens)):\n    print(f\"{word_tokens[i]} | {stemmed[i]}\")\n\nThe | the\nstars | star\ntwinkled | twinkl\nin | in\nthe | the\ndark | dark\n, | ,\nilluminating | illumin\nthe | the\nnight | night\nsky | sky\n. | .\n\n\n\n\n\n4.2.3 Lemmatization\n\nThe process of reducing a word to its base or root form, known as a lemma.\nIs more sophisticated than stemming because it uses a dictionary to map words to their base forms.\nPOS tagging is used to determine the correct lemma for a word based on its part of speech.\nLemmatization is more accurate than stemming but can be slower and more computationally intensive.\nWordNet Lemmatizer is a popular lemmatization algorithm that is available in NLTK.\n\n\n4.2.3.1 Example of stemming in Python using NLTK\n\nsbs = SnowballStemmer(\"english\")\ntext=\"The stars twinkled in the dark, illuminating the night sky.\"\nmethod = TreebankWordTokenizer()\nword_tokens = method.tokenize(text)\nlemma = WordNetLemmatizer()\nlemmatized = [lemma.lemmatize(token) for token in word_tokens]\n\nfor i in range(len(word_tokens)):\n    print(f\"{word_tokens[i]} | {lemmatized[i]}\")\n\nThe | The\nstars | star\ntwinkled | twinkled\nin | in\nthe | the\ndark | dark\n, | ,\nilluminating | illuminating\nthe | the\nnight | night\nsky | sky\n. | .\n\n\nComparison table of stemming and lemmatization\n\n\n\n\n\n\n\nStemming\nLemmatization\n\n\n\n\nFaster\nSlower (Resource Intensive)\n\n\nLess accurate\nMore accurate\n\n\nUses heuristics (eg choppping off endings)\nUses a dictionary-based lookup\n\n\nProduces root words\nProduces base words\n\n\nRemoves word suffixes\nMaps words to base forms\n\n\nDoes not require POS tagging\nRequires POS tagging",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Natural Language Processing - Chapter 2</span>"
    ]
  },
  {
    "objectID": "session2.html#regular-expressions",
    "href": "session2.html#regular-expressions",
    "title": "3  Natural Language Processing - Chapter 2",
    "section": "4.3 Regular Expressions",
    "text": "4.3 Regular Expressions\n\nRegular expressions are a powerful tool for pattern matching and text processing.\nThey allow you to search for patterns in text, extract specific information, and perform complex text transformations.\n\nRegular Expression (Disjunction) Table\n\n\n\n\n\n\n\n\nPattern\nMatches\nExample\n\n\n\n\n.\nAny character except newline\n“a.b” matches “axb”, “a1b”, “a@b”\n\n\n^\nStart of string\n“^abc” matches “abc”, “abcd”, “abc123”\n\n\n$\nEnd of string\n“abc$” matches “abc”, “123abc”, “xyzabc”\n\n\n*\nZero or more occurrences\n“ab*c” matches “ac”, “abc”, “abbc”\n\n\n+\nOne or more occurrences\n“ab+c” matches “abc”, “abbc”, “abbbc”\n\n\n?\nZero or one occurrence\n“ab?c” matches “ac”, “abc”\n\n\n{n}\nExactly n occurrences\n“ab{2}c” matches “abbc”\n\n\n{n,}\nn or more occurrences\n“ab{2,}c” matches “abbc”, “abbbc”\n\n\n{n,m}\nBetween n and m occurrences\n“ab{2,3}c” matches “abbc”, “abbbc”\n\n\n[abc]\nAny character in the set\n“[abc]” matches “a”, “b”, “c”\n\n\n[^abc]\nAny character not in the set\n“[^abc]” matches “d”, “e”, “f”\n\n\n[A-Z]\nAny character in the range\n“[A-Z]” matches “A”, “B”, “C”\n\n\n[a-z]\nAny character in the range\n“[a-z]” matches “a”, “b”, “c”\n\n\n[0-9]\nAny digit\n“[0-9]” matches “0”, “1”, “2”\n\n\n\\d\nAny digit\n“\\d” matches “0”, “1”, “2”\n\n\n\\D\nAny non-digit\n“\\D” matches “a”, “b”, “c”\n\n\n\\w\nAny word character\n“\\w” matches “a”, “b”, “c”, “0”, “1”, “2”\n\n\n\\W\nAny non-word character\n“\\W” matches “@”, “#”, “$”\n\n\n\\s\nAny whitespace character\n“\\s” matches ” “,”, “”\n\n\n\\S\nAny non-whitespace character\n“\\S” matches “a”, “b”, “c”\n\n\n\nRegular Expression Method Table\n\n\n\nMethod\nDescription\n\n\n\n\nre.match()\nMatches a pattern at the beginning of a string\n\n\nre.search()\nSearches for a pattern in a string\n\n\nre.findall()\nFinds all occurrences of a pattern in a string\n\n\nre.split()\nSplits a string based on a pattern\n\n\nre.sub()\nReplaces a pattern in a string with another pattern\n\n\n\nExample of Regular Expressions in Python\n\ntext = \"The quick brown fox jumps over the lazy dog. The dog barks at the fox.\"\nsentences = sent_tokenize(text)\nwords = word_tokenize(text)\n\n# Find all words that start with \"b\"\npattern = r\"\\b[bB]\\w+\"  # \\b is a word boundary, \\w+ is one or more word characters\nfor word in words:\n    if re.match(pattern, word):\n        print(f\" All words that start with 'b': {word}\")\n\n# Find all sentences that contain the word \"dog\"\npattern = r\"\\b[dD]ogs?\\b\"  # \\b is a word boundary, ? is zero or one occurrence of previous character, s? matches \"dog\" or \"dogs\"\nfor sentence in sentences:\n    if re.search(pattern, sentence):\n        print(f\" All sentences containing word 'dog': {sentence}\")\n\n All words that start with 'b': brown\n All words that start with 'b': barks\n All sentences containing word 'dog': The quick brown fox jumps over the lazy dog.\n All sentences containing word 'dog': The dog barks at the fox.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Natural Language Processing - Chapter 2</span>"
    ]
  },
  {
    "objectID": "session2.html#pos-tagging",
    "href": "session2.html#pos-tagging",
    "title": "3  Natural Language Processing - Chapter 2",
    "section": "4.4 POS Tagging",
    "text": "4.4 POS Tagging\n\nPart-of-speech (POS) tagging is the process of assigning a part of speech to each word in a text. The part of speech indicates the grammatical category of the word, such as noun, verb, adjective, etc.\nThe goal of POS tagging is to identify the syntactic structure of a sentence and extract useful information about the text.\nTypes of POS Tagging\n\nRule-based POS Tagging:\n\nUses hand-crafted rules to assign POS tags to words based on their context.\nRelies on a predefined set of rules and patterns to determine the correct POS tag for a word.\nPros:\n\nSimple and easy to implement, but may not be as accurate as other methods.\nDoesnt require a lot of computational resources or training data.\nCan be easily customized and adapted to different languages and domains.\nCons:\n\nMay not be as accurate as other methods, especially for complex or ambiguous cases.\nRequires manual effort to define rules and patterns for different languages and domains.\nLimited to the rules and patterns defined by the developer, which may not cover all cases.\n\nExample: Rule-based POS taggers in NLTK such as the DefaultTagger and RegexpTagger and pos_tag method.  \n\n\nStatistical POS Tagging:\nUses statistical models (such as Hidden Markov Models) to assign POS tags to words based on their context and probability.\nLearns the patterns and relationships between words and their POS tags from a large corpus of annotated text.\n\nPros:\n\nMore accurate than rule-based methods, especially for complex or ambiguous cases.\nCan handle a wide range of languages and domains without manual intervention.\nCan be trained on large datasets to improve accuracy and performance.\n\nCons:\n\nRequires a large amount of annotated training data to train the statistical model.\nCan be computationally intensive and require significant resources for training and inference.\nMay not be as interpretable or transparent as rule-based methods, making it difficult to understand the model’s decisions.\n\nExamples: Machine learning-based POS taggers in spaCy such as the PerceptronTagger and CNNTagger.\n\n\n\nMost Common POS Tags (NLTK)\n\n\n\nTag\nDescription\nExample\n\n\n\n\nCC\nCoordinating conjunction\nand, but, or\n\n\nCD\nCardinal number\n1, 2, 3\n\n\nDT\nDeterminer\nthe, a, an\n\n\nEX\nExistential there\nthere\n\n\nFW\nForeign word\nbonjour\n\n\nIN\nPreposition or subordinating conjunction\nin, of, on\n\n\nJJ\nAdjective\nbig, green, happy\n\n\nJJR\nAdjective, comparative\nbigger, greener, happier\n\n\nJJS\nAdjective, superlative\nbiggest, greenest, happiest\n\n\nLS\nList item marker\n1, 2, 3\n\n\nMD\nModal\ncan, could, might\n\n\nNN\nNoun, singular or mass\ndog, cat, house\n\n\n\n \nMost Common POS Tags (SpaCy)\n\n\n\nTag\nDescription\nExample\n\n\n\n\nADJ\nAdjective\nbig, green, happy\n\n\nADP\nAdposition\nin, to, during\n\n\nADV\nAdverb\nvery, tomorrow, down\n\n\nAUX\nAuxiliary\nis, has (done), will\n\n\nCONJ\nConjunction\nand, or, but\n\n\nCCONJ\nCoordinating conjunction\nand, or, but\n\n\nDET\nDeterminer\na, an, the\n\n\nINTJ\nInterjection\npsst, ouch, bravo\n\n\nNOUN\nN\n\n\n\nNUM\nNumeral\n42, forty-two\n\n\n\n\n4.4.0.1 Example of POS Tagging in Python using NLTK\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk import pos_tag\n\ntext = \"The quick brown fox jumps over the lazy dog.\"\nwords = word_tokenize(text)\ntags = pos_tag(words)\n\nfor word, tag in tags:\n    print(f\"{word} | {tag}\")\n\n\n4.4.0.2 Example of POS Tagging in Python using spaCy\n\nMake sure to download the spaCy model using python -m spacy download en_core_web_sm\nThis model is a small English model trained on written web text (blogs, news, comments), which includes vocabulary, vectors, syntax, and entities.\nThe model is trained on the OntoNotes 5 corpus and supports POS tagging, dependency parsing, named entity recognition, and more.\n\n\nmodel = spacy.load(\"en_core_web_sm\")\nsample_text = \"The quick brown fox jumps over the lazy dog.\"\ndoc = model(sample_text)\n\nfor word in doc:\n    print(f\"{word.text} | {word.pos_}\")\n\nThe | DET\nquick | ADJ\nbrown | ADJ\nfox | NOUN\njumps | VERB\nover | ADP\nthe | DET\nlazy | ADJ\ndog | NOUN\n. | PUNCT",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Natural Language Processing - Chapter 2</span>"
    ]
  },
  {
    "objectID": "session2.html#named-entity-recognition-ner",
    "href": "session2.html#named-entity-recognition-ner",
    "title": "3  Natural Language Processing - Chapter 2",
    "section": "4.5 Named Entity Recognition (NER)",
    "text": "4.5 Named Entity Recognition (NER)\n\nNamed Entity Recognition (NER) is the process of identifying and classifying named entities in a text.\nNamed entities are real-world objects such as persons, organizations, locations, dates, and more.\nNER is an important task in NLP because it helps extract useful information from unstructured text and enables downstream tasks such as information retrieval, question answering, and text summarization.\nNER models are trained on annotated datasets that contain labeled examples of named entities in text.\nCommon types of named entities:\n\nPerson: Names of people, such as “John Doe” or “Alice Smith”.\nOrganization: Names of companies, institutions, or groups, such as “Google” or “United Nations”.\nLocation: Names of places, such as “New York” or “Mount Everest”.\nDate: Dates and times, such as “January 1, 2022” or “10:30 AM”.\nNumber: Numerical quantities, such as “100” or “3.14”.\nMiscellaneous: Other named entities, such as “Apple” (the company) or “Python” (the programming language).\nEvent: Names of events, such as “World War II” or “Super Bowl”.\nProduct: Names of products, such as “iPhone” or “Coca-Cola”.\n\n\n\n4.5.0.1 Example of Named Entity Recognition in Python using NLTK\nIn NLP, a tree structure is often used to represent the syntactic structure of a sentence. Each node in the tree represents a linguistic unit, such as a word or a phrase, and the edges represent the relationships between these units. The Tree class provides various methods for creating, traversing, and modifying these tree structures.\ntext = \"Apple is a technology company based in Cupertino, California.\"\nwords = word_tokenize(text)\ntags = pos_tag(words)\ntree = ne_chunk(tags)\n\nfor subtree in tree:\n    if isinstance(subtree, Tree):\n        label = subtree.label()\n        words = \" \".join([word for word, tag in subtree.leaves()])\n        print(f\"{label}: {words}\")\n\nNote: NLTK’s named entity recognizer has identified “Apple”, “Cupertino”, and “California” as geopolitical entities (GPE).\nThe NER model uses context and patterns learned from training data to classify named entities, but it is not always perfect and can sometimes make mistakes, as seen with “Apple” in this case.\n\n\n\n4.5.0.2 Example of Named Entity Recognition in Python using spaCy\nmodel = spacy.load(\"en_core_web_sm\")\nsample_text = \"Apple is a technology company based in Cupertino, California.\"\ndoc = model(sample_text)\n\ndisplacy.render(doc, style=\"ent\", jupyter=True) # style=\"ent\" is used to display named entities",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Natural Language Processing - Chapter 2</span>"
    ]
  },
  {
    "objectID": "session3.html",
    "href": "session3.html",
    "title": "4  Natural Language Processing - Chapter 3",
    "section": "",
    "text": "5 Lecture 3",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Natural Language Processing - Chapter 3</span>"
    ]
  },
  {
    "objectID": "session3.html#n-gram-probabilistic-lm",
    "href": "session3.html#n-gram-probabilistic-lm",
    "title": "4  Natural Language Processing - Chapter 3",
    "section": "5.1 N-Gram (Probabilistic LM)",
    "text": "5.1 N-Gram (Probabilistic LM)\nThis is model predicts the probability of a word given the previous n-1 words. It is based on the assumption that the probability of a word depends on the context provided by the previous n-1 words. N-grams are used in various NLP tasks such as speech recognition, machine translation, and text generation.\n\nUnigram: A single word\nBigram: A pair of words\nTrigram: A triplet of words ect…\n\nnumber of n-grams = (total number of words) - (n - 1)\nGenerating different n-grams for a text:\n\nfrom nltk import ngrams\nfrom nltk.tokenize import word_tokenize\ntext = \"Italy is famous for its cuisine.\"\n\ndef generate_ngrams(text, n):\n    tokens = word_tokenize(text)\n    ngrams_list = list(ngrams(tokens, n))\n    return [\" \".join(ngram) for ngram in ngrams_list]\n\ntrigrams = generate_ngrams(text, 3)\n\nprint(\"Trigrams:\", trigrams)\n\nTrigrams: ['Italy is famous', 'is famous for', 'famous for its', 'for its cuisine', 'its cuisine .']\n\n\n\n5.1.1 Conditional Probability and Chain Rule\n\nJoint Probability: The probability of two or more events occurring together. It is denoted as P(A, B) and is calculated as the probability of both events A and B occurring.\n\nJoint Probability Formula: \\(P(A \\cap B) = P(A) \\times P(B)\\)\nis equivalent to \\(P(A \\cap B) = P(A) \\times P(B)\\)  \n\nConditional Probability: The probability of an event given that another event has occurred. It is denoted as P(A | B) which is the probability of event A occurring given that event B has occurred.\nIt is calculated as the probability of both events A and B occurring divided by the probability of event B occurring.\n\nConditional Probability Formula: \\(P(A \\mid B) = \\frac{P(A, B)}{P(B)}\\)  \n\nChain Rule of Probability: The probability of a sequence of events can be calculated by multiplying the conditional probabilities of each event given the previous events in the sequence.\n\nChain Rule Formula: \\[\n\\small\nP(w_1, w_2, ..., w_n) = P(w_1) * P(w_2 \\mid w_1) * P(w_3 \\mid w_1, w_2) * ... * P(w_n \\mid w_1, w_2, ..., w_{n-1})\n\\]\nExample: The probability of the sequence “The quick brown fox” can be calculated using the chain rule as follows: \\[\n\\small\n\\begin{aligned}\nP(\\text{The quick brown fox}) &= P(\\text{The}) \\times P(\\text{quick} \\mid \\text{The}) \\\\\n&\\times P(\\text{brown} \\mid \\text{The quick}) \\\\\n&\\times P(\\text{fox} \\mid \\text{The quick brown})\n\\end{aligned}\n\\]\n\n“What is the probability of the 10th word given the previous 9 words?”\n\nNote: The chain rule is used to calculate the probability of a sequence of words in an N-gram model.\n\n\n\n\n5.1.2 Markov Assumption\nTo make things manageable, N-gram models approximate the chain rule by making a simplifying assumption called the Markov assumption. This assumption states that the probability of a word depends only on the previous n-1 words, not on all the previous words in the sequence. This simplifies the calculation of the conditional probability of a word given the previous n-1 words.\n\nFirst-Order Markov Model: The probability of a word depends only on the previous word.\n\n\\[P(w_n | w_1, w_2, ..., w_{n-1}) = P(w_n | w_{n-1})\\]\n“The probability of the 10th word depends only on the 9th word, not on the previous words.”\nExample: \\(P(\\text{fox} \\mid \\text{The quick brown}) = P(\\text{fox} \\mid \\text{brown})\\)\nNote: The first-order Markov model is a special case of the N-gram model where n=2.  \n\nSecond-Order Markov Model: The probability of a word depends only on the previous two words.\n\n\\[P(w_n | w_1, w_2, ..., w_{n-1}) = P(w_n | w_{n-2}, w_{n-1})\\]\n“The probability of the 10th word depends only on the 8th and 9th words, not on the previous words.”\nExample: \\(P(\\text{fox} \\mid \\text{The quick brown}) = P(\\text{fox} \\mid \\text{quick brown})\\)\nNote: The second-order Markov model is a special case of the N-gram model where n=3.\n\n\nN-Gram Probability Calculation\n\nMathematically, for a sequence of words \\(w_1, w_2, ..., w_n\\), the probability of the sequence can be calculated using the chain rule of probability:\n\n\\[\\small P(w_1, w_2, ..., w_n) = P(w_1) * P(w_2 \\mid w_1) * P(w_3 \\mid w_1, w_2) * ... * P(w_n \\mid w_1, w_2, ..., w_{n-1})\\]\n\nThe probability of a word given the previous n-1 words can be calculated using the formula:\n\n\\(P(w_n | w_1, w_2, ..., w_{n-1}) = \\frac{P(w_1, w_2, ..., w_n)}{P(w_1, w_2, ..., w_{n-1})}\\) \n\nThe probability of a sequence of words can be calculated by counting the occurrences of the n-gram in a corpus and dividing by the total number of n-grams in the corpus. \nThe Maximum Likelihood Estimation (MLE) of an N-gram is calculated as:\n\n\\(P(w_n | w_1, w_2, ..., w_{n-1}) = \\frac{\\text{Count(n-gram)}}{\\text{Count(of previous n-1 words)}}\\)  \nIssue: The MLE can assign zero probability to unseen n-grams, leading to sparsity and poor generalization.\n\n\n\n\n5.1.3 Padding\nIs a technique used to handle the boundary conditions in N-gram models where the context words are not available. - Start-of-Sentence (BOS) Padding: A special token that represents the beginning of a sentence. It is used to handle the first word in a sentence where the context words are not available. - Example: &lt;s&gt; The quick brown fox jumps over the lazy dog. Note &lt;s&gt; is equivalent to &lt;start&gt;  \n\nEnd-of-Sentence (EOS) Padding: A special token that represents the end of a sentence. It is used to handle the last word in a sentence where the context words are not available.\n\nExample: The fox jumps over the lazy dog. &lt;/s&gt;\n\n\nNotes: the choice of how many padding tokens to use depends on the order of the N-gram model. For a bigram model, you would use one padding token at the beginning of the sentence. For a trigram model, you would use two padding tokens at the beginning of the sentence. The padding tokens are not part of the vocabulary and are used only for modeling purposes.\nPadding Example in Python\n\nfrom nltk.tokenize import sent_tokenize, word_tokenize\ntext = \"I like NLP. NLP is fun. NLP in python is fun.\"\nsentences = sent_tokenize(text)\n\n# Pad each sentence individually with &lt;s&gt; and &lt;/s&gt; tokens and tokenize into words\npadded_sentences = []\nfor sentence in sentences:\n    words = word_tokenize(sentence)\n    padded = [\"&lt;s&gt;\"] + words + [\"&lt;/s&gt;\"]\n    padded_sentences.append(padded)\n\nfor sentence in padded_sentences:\n    print(sentence)\n\n['&lt;s&gt;', 'I', 'like', 'NLP', '.', '&lt;/s&gt;']\n['&lt;s&gt;', 'NLP', 'is', 'fun', '.', '&lt;/s&gt;']\n['&lt;s&gt;', 'NLP', 'in', 'python', 'is', 'fun', '.', '&lt;/s&gt;']\n\n\n\n\n5.1.4 Issue of Underflow\n\nUnderflow: A numerical issue that occurs when multiplying many small probabilities together, leading to a very small probability that may be rounded to zero.\nSolution: Use log probabilities to avoid underflow by converting the product of probabilities to the sum of log probabilities.\n\n\n\n5.1.5 Smoothing Techniques\n\nThis is a technique used to address the issue of zero probabilities in N-gram models. It assigns a small non-zero probability to unseen n-grams to improve the generalization of the model. You “test” these by comparing smoothed probabilities to unsmoothed ones.  \nLaplace (Add-One) Smoothing: A simple smoothing technique that adds one to the count of each n-gram to avoid zero probabilities.\n\nFormula: \\(P(w_n | w_1, w_2, ..., w_{n-1}) = \\frac{(\\text{Count(n-gram)} + 1)}{(\\text{Count(of previous n-1 words)} + V)}\\)\n\nV: The vocabulary size, which is the total number of unique words in the corpus.\nExample: \\[P(\\text{fox} \\mid \\text{quick brown}) = \\frac{\\text{Count}(\\text{quick brown fox}) + 1}{\\text{Count}(\\text{quick brown}) + V}\\]\n\n\nAdd-k Smoothing: A generalization of Laplace smoothing that adds a smaller constant k to the count of each n-gram to avoid zero probabilities.\n\nFormula: \\(P(w_n | w_1, w_2, ..., w_{n-1}) = \\frac{(\\text{Count(n-gram)} + k)}{(\\text{Count(of previous n-1 words)} + kV)}\\)\n\nk: A constant value that determines the amount of smoothing.\nExample: \\[P(\\text{fox} \\mid \\text{quick brown}) = \\frac{\\text{Count}(\\text{quick brown fox}) + k}{\\text{Count}(\\text{quick brown}) + kV}\\]\nTest: Tune k and compare probabilities for rare vs. frequent trigrams.\n\n\nGood-Turing Smoothing: A more sophisticated smoothing technique that estimates the probability of unseen n-grams based on the frequency of seen n-grams.\n\nadjusts the counts of n-grams based on the frequency of n-grams with similar counts.\nTest: compute adjusted probabilities for low-frequency n-grams and validate against held-out data.\n\nBackoff and Interpolation: A technique that combines n-gram models of different orders to improve the generalization of the model.\n\nBackoff: Uses lower-order n-grams when higher-order n-grams have zero probabilities.\nInterpolation: Combines probabilities from different n-gram models using linear interpolation.\nTest: Compare performance of backoff and interpolation on different datasets.\n\nKneser-Ney Smoothing: A state-of-the-art smoothing technique that estimates the probability of unseen n-grams based on the frequency of n-grams in the context of the n-gram.\n\nIt considers how often a word appears in a novel context, rather than just how often it appears overall.\nTest: Compare Kneser-Ney smoothing to other smoothing techniques on large datasets.\n\n\n\n\n5.1.6 Perplexity\n\nPerplexity: A measure of how well a language model predicts a given text. It is the inverse probability of the test set, normalized by the number of words.\nEvaluation Metric: “How well the model predicts the next word in a sequence.”\n\nPerplexity Formula: \\(PP(W) = P(w_1, w_2, ..., w_N)^{-\\frac{1}{N}}\\)\n\nwhich can be calculated as \\(PP(W) = \\left(\\frac{1}{P(w_1, w_2, ..., w_N)}\\right)^{\\frac{1}{N}}\\)\nwhere:\n\nP(w_1, w_2, …, w_N): Probability of the test set under the language model.\nN: Number of words in the test set.\nExample: If the perplexity of a language model is 100, it means that the model is as confused as if it had to choose uniformly among 100 words for each word in the test set.\n\nN: The number of words in the test set.\nExample: If the perplexity of a language model is 100, it means that the model is as confused as if it had to choose uniformly among 100 words for each word in the test set.\n\nLower Perplexity: better language model that predicts the test set more accurately.\nHigher Perplexity: worse language model that predicts the test set less accurately.\n\n\n\n\n5.1.7 Example of Trigram LM in Python\n\nSteps:\n\nTokenize text\nAdd padding tokens\nGenerate trigrams\nCount unique trigrams\nCalculate trigram probabilities (MLE)\nCalculate perplexity\n\n\nSTEP 1: tokenize text and add padding tokens\n\ntext = \"I like NLP. NLP is fun. NLP in python is fun. I like coding in python. NLP is cool.\"\ntokens = sent_tokenize(text)\npadded_sentences = []\nfor token in tokens:\n    words = word_tokenize(token)\n    padded = [\"&lt;s&gt;\"] + words + [\"&lt;/s&gt;\"]\n    padded_sentences.append(padded)\n\nprint(\"Padded Sentences:\")\nfor sent in padded_sentences:\n    print(sent)\n\nPadded Sentences:\n['&lt;s&gt;', 'I', 'like', 'NLP', '.', '&lt;/s&gt;']\n['&lt;s&gt;', 'NLP', 'is', 'fun', '.', '&lt;/s&gt;']\n['&lt;s&gt;', 'NLP', 'in', 'python', 'is', 'fun', '.', '&lt;/s&gt;']\n['&lt;s&gt;', 'I', 'like', 'coding', 'in', 'python', '.', '&lt;/s&gt;']\n['&lt;s&gt;', 'NLP', 'is', 'cool', '.', '&lt;/s&gt;']\n\n\nSTEP 2: Generate trigrams (and bigrams for MLE calculation)\n\ntrigrams = []\nfor sent in padded_sentences:\n    sent_trigrams = list(ngrams(sent, 3))\n    # sent_trigrams = list(ngrams(sent, 3, pad_left=False, pad_right=False, left_pad_symbol=\"&lt;s&gt;\", right_pad_symbol=\"&lt;/s&gt;\"))\n    # can use the above line to avoid padding tokens in trigrams, but its less flexible if you need to reuse the padded data.\n    trigrams.extend(sent_trigrams)\n\nbigrams = []\nfor sent in padded_sentences:\n    sent_bigrams = list(ngrams(sent, 2))\n    bigrams.extend(sent_bigrams)\n\nprint(\"\\nTrigrams:\")\nfor trigram in trigrams:\n    print(trigram)\n\n\nTrigrams:\n('&lt;s&gt;', 'I', 'like')\n('I', 'like', 'NLP')\n('like', 'NLP', '.')\n('NLP', '.', '&lt;/s&gt;')\n('&lt;s&gt;', 'NLP', 'is')\n('NLP', 'is', 'fun')\n('is', 'fun', '.')\n('fun', '.', '&lt;/s&gt;')\n('&lt;s&gt;', 'NLP', 'in')\n('NLP', 'in', 'python')\n('in', 'python', 'is')\n('python', 'is', 'fun')\n('is', 'fun', '.')\n('fun', '.', '&lt;/s&gt;')\n('&lt;s&gt;', 'I', 'like')\n('I', 'like', 'coding')\n('like', 'coding', 'in')\n('coding', 'in', 'python')\n('in', 'python', '.')\n('python', '.', '&lt;/s&gt;')\n('&lt;s&gt;', 'NLP', 'is')\n('NLP', 'is', 'cool')\n('is', 'cool', '.')\n('cool', '.', '&lt;/s&gt;')\n\n\nSTEP 3: Count unique trigrams and bigrams\n\nfrom collections import Counter\nfrom prettytable import PrettyTable\ntrigram_counts = Counter(trigrams)\nbigrams_counts = Counter(bigrams)\nunique_bigrams = len(bigrams_counts)\nunique_trigrams = len(trigram_counts)\nprint(\"\\nUnique Trigrams:\", unique_trigrams)\nprint(\"Unique Bigrams:\", unique_bigrams)\n\nc_tri_tab = PrettyTable([\"Index\", \"Unique Trigram\", \"Count\"])\nfor i, (trigram, count) in enumerate(trigram_counts.items()):\n    c_tri_tab.add_row([i, trigram, count])\nprint(c_tri_tab)\n\n\nUnique Trigrams: 20\nUnique Bigrams: 17\n+-------+----------------------------+-------+\n| Index |       Unique Trigram       | Count |\n+-------+----------------------------+-------+\n|   0   |    ('&lt;s&gt;', 'I', 'like')    |   2   |\n|   1   |    ('I', 'like', 'NLP')    |   1   |\n|   2   |    ('like', 'NLP', '.')    |   1   |\n|   3   |    ('NLP', '.', '&lt;/s&gt;')    |   1   |\n|   4   |    ('&lt;s&gt;', 'NLP', 'is')    |   2   |\n|   5   |    ('NLP', 'is', 'fun')    |   1   |\n|   6   |     ('is', 'fun', '.')     |   2   |\n|   7   |    ('fun', '.', '&lt;/s&gt;')    |   2   |\n|   8   |    ('&lt;s&gt;', 'NLP', 'in')    |   1   |\n|   9   |  ('NLP', 'in', 'python')   |   1   |\n|   10  |   ('in', 'python', 'is')   |   1   |\n|   11  |  ('python', 'is', 'fun')   |   1   |\n|   12  |  ('I', 'like', 'coding')   |   1   |\n|   13  |  ('like', 'coding', 'in')  |   1   |\n|   14  | ('coding', 'in', 'python') |   1   |\n|   15  |   ('in', 'python', '.')    |   1   |\n|   16  |  ('python', '.', '&lt;/s&gt;')   |   1   |\n|   17  |   ('NLP', 'is', 'cool')    |   1   |\n|   18  |    ('is', 'cool', '.')     |   1   |\n|   19  |   ('cool', '.', '&lt;/s&gt;')    |   1   |\n+-------+----------------------------+-------+\n\n\nSTEP 4: Calculate trigram probabilities (MLE)\n\ntri_mle = {}\nfor (w1, w2, w3), count in trigram_counts.items():\n    tri_mle[(w1, w2, w3)] = round(count / bigrams_counts[(w1, w2)], 3)\n\nprint(\"\\nTrigram MLE:\")\ntri_mle_tab = PrettyTable([\"Word 1\", \"Word 2\", \"Word 3\", \"MLE\"])\nfor (w1, w2, w3), mle in tri_mle.items():\n    tri_mle_tab.add_row([w1, w2, w3, mle])\nprint(tri_mle_tab)\n\n\nTrigram MLE:\n+--------+--------+--------+-------+\n| Word 1 | Word 2 | Word 3 |  MLE  |\n+--------+--------+--------+-------+\n|  &lt;s&gt;   |   I    |  like  |  1.0  |\n|   I    |  like  |  NLP   |  0.5  |\n|  like  |  NLP   |   .    |  1.0  |\n|  NLP   |   .    |  &lt;/s&gt;  |  1.0  |\n|  &lt;s&gt;   |  NLP   |   is   | 0.667 |\n|  NLP   |   is   |  fun   |  0.5  |\n|   is   |  fun   |   .    |  1.0  |\n|  fun   |   .    |  &lt;/s&gt;  |  1.0  |\n|  &lt;s&gt;   |  NLP   |   in   | 0.333 |\n|  NLP   |   in   | python |  1.0  |\n|   in   | python |   is   |  0.5  |\n| python |   is   |  fun   |  1.0  |\n|   I    |  like  | coding |  0.5  |\n|  like  | coding |   in   |  1.0  |\n| coding |   in   | python |  1.0  |\n|   in   | python |   .    |  0.5  |\n| python |   .    |  &lt;/s&gt;  |  1.0  |\n|  NLP   |   is   |  cool  |  0.5  |\n|   is   |  cool  |   .    |  1.0  |\n|  cool  |   .    |  &lt;/s&gt;  |  1.0  |\n+--------+--------+--------+-------+\n\n\n(Calculating MLE with Laplace Smoothing)\n\nV = len(trigram_counts)\nk = 1  # Laplace smoothing constant\ntri_mle_laplace = {}\n\nfor (w1, w2, w3), count in trigram_counts.items():\n    tri_mle_laplace[(w1, w2, w3)] = round((count + k) / (bigrams_counts[(w1, w2)] + k * V), 3)\n\nprint(\"\\nTrigram MLE with Laplace Smoothing:\")\ntri_mle_laplace_tab = PrettyTable([\"Word 1\", \"Word 2\", \"Word 3\", \"MLE (Laplace)\"])\nfor (w1, w2, w3), mle in tri_mle_laplace.items():\n    tri_mle_laplace_tab.add_row([w1, w2, w3, mle])\nprint(tri_mle_laplace_tab)\n\n\nTrigram MLE with Laplace Smoothing:\n+--------+--------+--------+---------------+\n| Word 1 | Word 2 | Word 3 | MLE (Laplace) |\n+--------+--------+--------+---------------+\n|  &lt;s&gt;   |   I    |  like  |     0.136     |\n|   I    |  like  |  NLP   |     0.091     |\n|  like  |  NLP   |   .    |     0.095     |\n|  NLP   |   .    |  &lt;/s&gt;  |     0.095     |\n|  &lt;s&gt;   |  NLP   |   is   |      0.13     |\n|  NLP   |   is   |  fun   |     0.091     |\n|   is   |  fun   |   .    |     0.136     |\n|  fun   |   .    |  &lt;/s&gt;  |     0.136     |\n|  &lt;s&gt;   |  NLP   |   in   |     0.087     |\n|  NLP   |   in   | python |     0.095     |\n|   in   | python |   is   |     0.091     |\n| python |   is   |  fun   |     0.095     |\n|   I    |  like  | coding |     0.091     |\n|  like  | coding |   in   |     0.095     |\n| coding |   in   | python |     0.095     |\n|   in   | python |   .    |     0.091     |\n| python |   .    |  &lt;/s&gt;  |     0.095     |\n|  NLP   |   is   |  cool  |     0.091     |\n|   is   |  cool  |   .    |     0.095     |\n|  cool  |   .    |  &lt;/s&gt;  |     0.095     |\n+--------+--------+--------+---------------+\n\n\nSTEP 5: Calculate Perplexity\n\nimport math\n\ntest_trigrams = trigrams  # Reusing the training trigrams for simplicity\n# Calculate the sum of log probabilities\nlog_prob_sum = 0\nN = len(test_trigrams)  # Number of trigrams in the test set\nfor trigram in test_trigrams:\n    prob = tri_mle.get(trigram, 0)  # Get MLE probability, default to 0 if unseen\n    if prob &gt; 0:  # Avoid log(0)\n        log_prob_sum += math.log2(prob) # use log to avoid underflow issues (A numerical issue that occurs when multiplying many small probabilities together, leading to a very small probability that may be rounded to zero)\n    else:\n        print(f\"Warning: Trigram {trigram} has zero probability (unseen in training)\")\n        log_prob_sum = float(\"-inf\")  # This will lead to infinite perplexity\n        break\n\n# Calculate perplexity\nif log_prob_sum != float(\"-inf\"):\n    avg_log_prob = log_prob_sum / N\n    perplexity = 2 ** (-avg_log_prob)\nelse:\n    perplexity = float(\"inf\")\n\nprint(f\"Number of test trigrams (N): {N}\")\nprint(f\"Sum of log probabilities: {log_prob_sum:.3f}\")\nprint(f\"Average log probability: {avg_log_prob:.3f}\")\nprint(f\"Perplexity: {perplexity:.3f}\")\n\nNumber of test trigrams (N): 24\nSum of log probabilities: -8.755\nAverage log probability: -0.365\nPerplexity: 1.288\n\n\n\n\n5.1.8 Example of Trigram LM with NLTK ABC Corpus\n\nprint(type(list))\n\n&lt;class 'type'&gt;\n\n\n\nimport time\nfrom nltk.util import trigrams, bigrams\n\nimport nltk\nnltk.download('abc')\n\n\nstart_time = time.time()\nfrom nltk.corpus import abc\n\n# Load the ABC corpus\nabc_text = abc.raw(\"rural.txt\")\n\n# Step 1 get sentences from corpus\nsentences = abc.sents()[0:2000]\nprint(\"Number of sentences:\", len(sentences))\n\n# Step 2: Tokenize text and add padding tokens\ntokens = []\nfor sentence in sentences:\n    padded_sentence = [\"&lt;s&gt;\"] + [word.lower() for word in sentence] + [\"&lt;/s&gt;\"]\n    tokens.extend(padded_sentence)\nprint(\"Example tokens:\", tokens[:10])\n\n# Step 3: Generate trigrams\ntrigram_list = list(trigrams(tokens))\nbigram_list = list(bigrams(tokens))\n\nprint(\"Example trigrams:\", trigram_list[:5])\nprint(\"Example bigrams:\", bigram_list[:5])\n\n# Step 4: Count unique trigrams\ntrigram_counts = Counter(trigram_list)\nbigram_counts = Counter(bigram_list)\nunique_trigrams = len(trigram_counts)\nunique_bigrams = len(bigram_counts)\nprint(\"Unique Trigrams:\", unique_trigrams)\nprint(\"Unique Bigrams:\", unique_bigrams)\n\n# Step 5: Calculate trigram probabilities (MLE) with Laplace smoothing and k=1\nV = len(trigram_counts)\nk = 0.01\ntrigram_mle_laplace = {}\nfor (w1, w2, w3), count in trigram_counts.items():\n    trigram_mle_laplace[(w1, w2, w3)] = (count + k) / (bigram_counts[(w1, w2)] + k * V)\n\n\n# Function to predict next word based on conditional probability\ndef predict_next_word(w1, w2, trigram_mle):\n    candidates = {w3: prob for (x1, x2, w3), prob in trigram_mle.items() if x1 == w1 and x2 == w2}\n    predicted = max(candidates, key=candidates.get) if candidates else None\n    prob = candidates.get(predicted, 0.0) if predicted else 0.0\n    return predicted, prob\n\n\n# Test prediction\nw1, w2 = \"the\", \"prime\"\npredicted_word, probability = predict_next_word(w1, w2, trigram_mle_laplace)\nprint(f\"Predicted next word after '{w1} {w2}': {predicted_word}\")\nprint(f\"Probability of the next word occurring: {probability:.5f}\")\n\nend_time = time.time()\nprint(f\"Execution time: {end_time - start_time:.5f} seconds\")\n\n[nltk_data] Downloading package abc to\n[nltk_data]     C:\\Users\\roess\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package abc is already up-to-date!\n\n\nNumber of sentences: 2000\nExample tokens: ['&lt;s&gt;', 'pm', 'denies', 'knowledge', 'of', 'awb', 'kickbacks', 'the', 'prime', 'minister']\nExample trigrams: [('&lt;s&gt;', 'pm', 'denies'), ('pm', 'denies', 'knowledge'), ('denies', 'knowledge', 'of'), ('knowledge', 'of', 'awb'), ('of', 'awb', 'kickbacks')]\nExample bigrams: [('&lt;s&gt;', 'pm'), ('pm', 'denies'), ('denies', 'knowledge'), ('knowledge', 'of'), ('of', 'awb')]\nUnique Trigrams: 45447\nUnique Bigrams: 30474\nPredicted next word after 'the prime': minister\nProbability of the next word occurring: 0.02978\nExecution time: 0.33075 seconds\n\n\n\n\n5.1.9 Example of Sentence Generation with Trigram LM\n\nimport random\n\nstart_time = time.time()\n\n\ndef generate_sentence(model, max_length):\n    current_bigram = random.choice(list(model.keys()))  # Pick a random starting bigram\n    get_text = list(current_bigram)  # Initialize with the two words from the bigram\n\n    for _ in range(max_length - 2):  # Start from the third word\n        w_next_prob = model.get(tuple(get_text[-2:]), {})  # Get trigram probabilities\n        if not w_next_prob:  # If no next word, break\n            break\n        w_next = random.choices(list(w_next_prob.keys()), weights=list(w_next_prob.values()))[0]\n        get_text.append(w_next)  # Append next word\n\n    return \" \".join(get_text)  # Return generated sentence as a string\n\n\n# Example usage\ngenerated_text = generate_sentence(trigram_mle_laplace, 15)\nprint(f\"Generated sentence: {generated_text}\")\nend_time = time.time()\nprint(f\"Execution time: {end_time - start_time:.5f} seconds\")\n\nGenerated sentence: thoroughbred industry has\nExecution time: 0.00159 seconds",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Natural Language Processing - Chapter 3</span>"
    ]
  },
  {
    "objectID": "session4.html",
    "href": "session4.html",
    "title": "5  Natural Language Processing - Chapter 4",
    "section": "",
    "text": "6 Lecture 4",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Natural Language Processing - Chapter 4</span>"
    ]
  },
  {
    "objectID": "session4.html#text-classification",
    "href": "session4.html#text-classification",
    "title": "5  Natural Language Processing - Chapter 4",
    "section": "6.1 Text Classification",
    "text": "6.1 Text Classification\n\nText classification is the process of assigning predefined categories or labels to text documents based on their content.\nIt is a supervised learning task where a model is trained on labeled data to learn the relationship between the text and its corresponding labels.\nText classification is used in various applications such as spam detection, sentiment analysis, topic categorization, and more.\nThe goal of text classification is to build a model that can accurately predict the category of unseen text documents based on their content.\nThe process of text classification involves several steps, including data preprocessing, feature extraction, model training, and evaluation.\nThe choice of features and the classification algorithm can significantly impact the performance of the model.\n\nThere are three main types of text classification techniques: - Supervised Learning: The model is trained on labeled data, where each document is associated with a predefined category. The model learns to map the input features to the corresponding labels. - Naive Bayes, Logistic Regression - Unsupervised Learning: The model is trained on unlabeled data, where the goal is to discover hidden patterns or clusters in the data. The model learns to group similar documents together without any predefined labels. - Latent Dirichlet Allocation (LDA), K-means clustering - Deep Learning: The model is trained on large amounts of data using deep neural networks. Deep learning models can automatically learn complex features and representations from the data, making them suitable for text classification tasks. - Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), Transformers (BERT, GPT-3)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Natural Language Processing - Chapter 4</span>"
    ]
  },
  {
    "objectID": "session4.html#bayes-theorem",
    "href": "session4.html#bayes-theorem",
    "title": "5  Natural Language Processing - Chapter 4",
    "section": "6.2 Bayes Theorem",
    "text": "6.2 Bayes Theorem\n\nBayes theorem is a fundamental concept in probability theory that describes the relationship between conditional probabilities. It is used to update the probability of a hypothesis based on new evidence.\nThe equation is given by: \\[P(H \\mid E) = \\frac{P(E \\mid H) \\cdot P(H)}{P(E)}\\]\nWhere:\n\nP(H|E): The probability of hypothesis H given evidence E (posterior probability).\nP(E|H): The probability of evidence E given hypothesis H (likelihood).\nP(H): The prior probability of hypothesis H.\nP(E): The total probability of evidence E.\n\nNaive Assumption: The naive assumption in Naive Bayes classifiers is that the features are conditionally independent given the class label. This simplifies the computation of the posterior probability.\nThe naive assumption allows us to calculate the joint probability of the features given the class label as the product of the individual probabilities of each feature.\nAn exmple of the naive assumption is:\n\\[P(X_1, X_2, ..., X_n \\mid C) = P(X_1 \\mid C) \\cdot P(X_2 \\mid C) \\cdot ... \\cdot P(X_n \\mid C)\\]\nWhere:\n\nP(X1, X2, …, Xn | C): The joint probability of features X1, X2, …, Xn given class C.\nP(Xi | C): The probability of feature Xi given class C.\n\nThe naive assumption is a simplification that allows Naive Bayes classifiers to work well in practice, even when the features are not truly independent.\n\nTypes of Naive Bayes Classifiers: - Gaussian Naive Bayes: Assumes that the features follow a Gaussian (normal) distribution. It is suitable for continuous features. - Multinomial Naive Bayes: Assumes that the features are counts or frequencies. It is suitable for discrete features, such as word counts in text classification. - Bernoulli Naive Bayes: Assumes that the features are binary (0 or 1). It is suitable for binary features, such as presence or absence of words in text classification.\nImplementing Naive Bayes - Step 1: Load the dataset, clean and preprocess the text data. - Step 2: Split the dataset into training and testing sets. - Step 3: Extract features from the text data using techniques such as Bag of Words (BoW) - Step 4: Train the Naive Bayes classifier on the training set. - Step 5: Evaluate the classifier on the testing set using metrics such as accuracy, precision, recall, and F1-score.\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.datasets import fetch_20newsgroups\nfrom nltk.corpus import stopwords\nstop_words = set(stopwords.words(\"english\"))\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import PorterStemmer\nstemmer = PorterStemmer()\nfrom nltk import download\nfrom nltk import pos_tag\nfrom nltk import ne_chunk\nfrom nltk.tree import Tree\nfrom nltk.tokenize import word_tokenize\n\n\n# Load the dataset\nnewsgroups = fetch_20newsgroups(\n    subset=\"all\", categories=[\"alt.atheism\", \"sci.space\"], remove=(\"headers\", \"footers\", \"quotes\")\n)\ndf = pd.DataFrame({\"text\": newsgroups.data, \"label\": newsgroups.target})\n# Preprocess the text data\n\n\ndef preprocess_text(text):\n    # Tokenize the text\n    tokens = word_tokenize(text)\n    # Remove stop words\n    tokens = [word for word in tokens if word.lower() not in stop_words]\n    # Stem the words\n    tokens = [stemmer.stem(word) for word in tokens]\n    return \" \".join(tokens)\n\n\ndf[\"text\"] = df[\"text\"].apply(preprocess_text)\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(\n    df[\"text\"], df[\"label\"], test_size=0.2, random_state=42\n)\n\n# Create a pipeline with CountVectorizer and MultinomialNB\npipeline = make_pipeline(CountVectorizer(), MultinomialNB())\n\n# Train the Naive Bayes classifier\npipeline.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred = pipeline.predict(X_test)\n\n# Evaluate the classifier\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy: {accuracy:.4f}\")\n\nprint(\"Classification Report:\")\nprint(classification_report(y_test, y_pred, target_names=newsgroups.target_names))\n\nAccuracy: 0.9358\nClassification Report:\n              precision    recall  f1-score   support\n\n alt.atheism       0.90      0.96      0.93       157\n   sci.space       0.96      0.92      0.94       201\n\n    accuracy                           0.94       358\n   macro avg       0.93      0.94      0.94       358\nweighted avg       0.94      0.94      0.94       358",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Natural Language Processing - Chapter 4</span>"
    ]
  },
  {
    "objectID": "session4.html#evaluation-metrics",
    "href": "session4.html#evaluation-metrics",
    "title": "5  Natural Language Processing - Chapter 4",
    "section": "6.3 Evaluation Metrics",
    "text": "6.3 Evaluation Metrics\n\nAccuracy: The proportion of correctly classified instances out of the total instances.\n\nFormula: \\(Accuracy = \\frac{TP + TN}{TP + TN + FP + FN}\\)\nWhere:\n\nTP: True Positives\nTN: True Negatives\nFP: False Positives\nFN: False Negatives\n\n\nPrecision: The proportion of true positive predictions out of all positive predictions.\n\nFormula: \\(Precision = \\frac{TP}{TP + FP}\\)\nIndicates how many of the predicted positive instances are actually positive.\nHigh precision means fewer false positives.\n\nRecall: The proportion of true positive predictions out of all actual positive instances.\n\nFormula: \\(Recall = \\frac{TP}{TP + FN}\\)\nIndicates how many of the actual positive instances were correctly predicted.\nHigh recall means fewer false negatives.\nAlso known as Sensitivity or True Positive Rate.\nHigh precision and low recall means the model is conservative in making positive predictions.\n\nF1-Score: The harmonic mean of precision and recall. It is a single metric that balances both precision and recall.\n\nFormula: \\(F1 = 2 \\cdot \\frac{Precision \\cdot Recall}{Precision + Recall}\\)\nA high F1-score indicates a good balance between precision and recall.\nUseful when the class distribution is imbalanced, because accuracy can be misleading.\nClassification Report: A summary of precision, recall, and F1-score for each class in a multi-class classification problem.\n\n\nDisadvatages of Naive Bayes:\n\nIndependence Assumption: The naive assumption of feature independence may not hold in real-world data, leading to suboptimal performance.\nZero Probability Problem: If a feature value is not present in the training data for a particular class, the model will assign a probability of zero to that class, which can be problematic.\nProbability Estimation Bias: Naive Bayes may produce biased probability estimates, especially for rare events or classes with limited training data.\nFeature Revelance: Naive Bayes may not perform well when features are highly correlated or when the feature space is large and sparse.\nLimited Expressiveness: Naive Bayes is a linear classifier and may not capture complex relationships between features and classes.\n\nIf you are using a Naive Bayes classifier for a dataset where 95% of the tweets are from positive class. Can you suggest any method to handle this imbalance, and briefly explain how it helps improve model performance?\nAnswer: To handle class imbalance in a dataset where 95% of the tweets are from the positive class, you can use the following methods:\n\nResampling Techniques:\n\nOversampling: Increase the number of instances in the minority class (negative class) by duplicating existing instances or generating synthetic samples (e.g., using SMOTE).\nUndersampling: Decrease the number of instances in the majority class (positive class) by randomly removing instances.\nCombination: Use a combination of oversampling and undersampling to balance the classes.\nHow it helps: Resampling techniques help to create a more balanced dataset, which allows the model to learn better from both classes and reduces the bias towards the majority class. This can lead to improved performance in terms of precision, recall, and F1-score for the minority class.\n\nClass Weights: Assign higher weights to the minority class during model training. This can be done by using the class_weight parameter in scikit-learn classifiers.\n\nHow it helps: By assigning higher weights to the minority class, the model is penalized more for misclassifying instances from that class, which encourages it to focus on learning from the minority class.\n\nEnsemble Methods: Use ensemble methods such as Random Forest or Gradient Boosting, which can handle class imbalance better than individual classifiers.\n\nHow it helps: Ensemble methods combine the predictions of multiple classifiers, which can improve overall performance and robustness against class imbalance.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Natural Language Processing - Chapter 4</span>"
    ]
  },
  {
    "objectID": "session5.html",
    "href": "session5.html",
    "title": "6  Natural Language Processing - Chapter 5",
    "section": "",
    "text": "7 Lecture 5",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Natural Language Processing - Chapter 5</span>"
    ]
  },
  {
    "objectID": "session5.html#logistic-regression",
    "href": "session5.html#logistic-regression",
    "title": "6  Natural Language Processing - Chapter 5",
    "section": "7.1 Logistic Regression",
    "text": "7.1 Logistic Regression\n\nLogistic regression is a statistical method used for binary classification problems. It models the relationship between a binary dependent variable and one or more independent variables by estimating the probability of the dependent variable being in a particular class.\nIt is a type of regression analysis that uses the logistic function to model the probability of a binary outcome.\nLogistic regression is widely used in various fields, including social sciences, healthcare, marketing, and finance, for tasks such as predicting customer churn, disease diagnosis, and credit risk assessment.\nLogistic regression is a linear model that uses the logistic function to map the linear combination of input features to a probability value between 0 and 1.\nThe logistic function is defined as:\n\n\\[f(x) = \\frac{1}{1 + e^{-x}}\\]\nWhere:\n\nx: The linear combination of input features and their corresponding weights.\ne: The base of the natural logarithm (approximately 2.71828).\nf(x): The output probability value between 0 and 1.\nThis is known as the sigmoid function.\n\n\nThe logistic regression model can be represented as:\n\\[P(Y=1|X) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_nX_n)}}\\]\nWhere:\n\nP(Y=1|X): The probability of the dependent variable Y being equal to 1 given the input features X.\nβ0: The intercept term (bias).\nβ1, β2, …, βn: The coefficients (weights) for each input feature X1, X2, …, Xn.\n\nThe coefficients are learned during the training process using maximum likelihood estimation (MLE), which finds the values of the coefficients that maximize the likelihood of the observed data given the model.\nThe bias teram helps the model to fit the data better by allowing it to shift the decision boundary. A bias &lt;0 means the model is more likely to predict class 0, while a bias &gt;0 means the model is more likely to predict class 1.\n\nLog-Odds:\n\nThe log-odds (or logit) is the logarithm of the odds ratio, which is the ratio of the probability of an event occurring to the probability of it not occurring.\nThe log-odds can be calculated as:\n\n\\[\\text{log-odds} = \\log\\left(\\frac{P(Y=1|X)}{1 - P(Y=1|X)}\\right) = = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_nX_n\\]\n\nThe log-odds transformation is useful because it maps the probability values (0, 1) to the entire real line (-∞, +∞), allowing for a linear relationship between the input features and the log-odds.\nThe log-odds can be interpreted as the change in the log-odds of the dependent variable for a one-unit increase in the independent variable.\nLogg odds of the probability of the positive class can be modelled as a linear function of the input features\n\n\n7.1.1 Training Logistic Regression\n\nMaximum Likelihood Estimation (MLE) is used to estimate the coefficients of the logistic regression model.\n\nThe MLE finds the values of the coefficients that maximize the likelihood of the observed data given the model.\n\nL2 regularization (Ridge) is often used to prevent overfitting by adding a penalty term to the loss function.\n\nThe regularization term helps to constrain the coefficients, reducing their variance and improving the model’s generalization to unseen data.\nThe choice of regularization strength is crucial, as too much regularization can lead to underfitting, while too little can result in overfitting.\nCross-validation is commonly used to select the optimal regularization parameter.\nIt is important to evaluate the model’s performance on a validation set to ensure that it generalizes well to new data.\n\nL1 regularization (Lasso) can also be used to perform feature selection by driving some coefficients to zero, effectively removing those features from the model.\nElastic Net is a combination of L1 and L2 regularization, allowing for both feature selection and coefficient shrinkage.\nBayesian logistic regression is another approach that incorporates prior distributions on the coefficients, allowing for uncertainty quantification and regularization.\n\nScikit-Learn’s LogisticRegression\n\nthe most commonly used solver is L2-regularized MLE with the liblinear solver.\nThe available optimization solvers are:\n\nliblinear: A coordinate descent algorithm for L1 and L2 regularization. Better for small datasets of size &lt; 10,000.\nsaga: A stochastic average gradient descent algorithm for L1 and L2 regularization. Suitable for large datasets of size &gt; 10,000.\nnewton-cg: A Newton’s method algorithm for L2 regularization.\nlbfgs: An optimization algorithm based on the limited-memory Broyden-Fletcher-Goldfarb-Shanno (BFGS) method for L2 regularization. Suitable for small to medium-sized datasets\n\n\n\n\n7.1.2 Bag of Words vs. TF-IDF\n\nBag of Words (BoW): A simple representation of text data where each document is represented as a vector of word counts. It ignores the order of words and focuses on the frequency of each word in the document.\n\nPros: Simple to implement, easy to interpret, works well for many applications.\nCons: Ignores word order, can lead to high-dimensional sparse vectors, may not capture semantic meaning.\n\nTerm Frequency-Inverse Document Frequency (TF-IDF): A more sophisticated representation of text data that considers both the frequency of words in a document and their importance across the entire corpus. It assigns higher weights to words that are frequent in a document but rare in the corpus.\nTF-IDF is calculated as:\n\\[\\text{TF-IDF}(w, d) = \\text{TF}(w, d) \\cdot \\text{IDF}(w)\\]\n\nTF(w, d): The term frequency of word w in document d.\nIDF(w): The inverse document frequency of word w, calculated as:\n\n\\[\\text{IDF}(w) = \\log\\left(\\frac{N}{DF(w)}\\right)\\]\nWhere:\n\nN: The total number of documents in the corpus.\nDF(w): The number of documents containing word w.\nIn this context documents is the number of tokens in the corpus.\n\n\nTF-IDF captures the importance of words in a document relative to the entire corpus, making it more effective for tasks such as text classification and information retrieval.\nPros: Captures word importance, reduces the impact of common words, better for semantic understanding.\nCons: More complex to implement, may still lead to high-dimensional sparse vectors.\n\nExample: In a corpus of 100 documents, the word “the” appears in 90 documents, while the word “NLP” appears in 10 documents. The TF-IDF score for “the” will be lower than that for “NLP” because “the” is common across many documents, while “NLP” is more specific to certain documents.\n\nMake table of columns: Feature | BoW | TF-IDF\n\n\n\n\n\n\n\n\nFeature\nBag of Words (BoW)\nTF-IDF\n\n\n\n\nConcept\nCounts word frequencies\nWeights word importance\n\n\nDimensionality\nHigh-dimensional sparse vectors\nHigh-dimensional sparse vectors, but weights may reduce freature noise\n\n\nComplexity\nSimple to implement\nMore complex to implement, due to IDF calculation\n\n\nWord Importance\nIgnores word importance\nConsiders word importance relative to the corpus\n\n\nContext Sensitivity\nIgnores word order and context\nIgnores word order, but captures context through IDF\n\n\nPeformance\nWorks well for many applications\nBetter for semantic understanding and information retrieval\n\n\nUse Cases\nText classification, sentiment analysis\nText classification, information retrieval, search engines\n\n\nLimitations\nIgnores word order, may lead to high-dimensional vectors\nMay still lead to high-dimensional vectors, sensitive to document length, Does not capture word order\n\n\n\n\n\n7.1.3 LR Model Assumptions\n\nLinearity: The relationship between the independent variables and the log-odds of the dependent variable is linear. This means that a one-unit change in an independent variable will result in a constant change in the log-odds of the dependent variable.\nIndependence: The observations are independent of each other. This means that the outcome of one observation does not influence the outcome of another observation.\nNo Multicollinearity: The independent variables are not highly correlated with each other. This means that the model should not include highly correlated features, as this can lead to unstable coefficient estimates and difficulty in interpreting the results.\nNo Outliers: The model assumes that there are no extreme outliers in the data that could disproportionately influence the results. Outliers can affect the estimates of the coefficients and the overall fit of the model.\n\n\n\n7.1.4 LR Limitations\n\nLinearity Assumption: Logistic regression assumes a linear relationship between the independent variables and the log-odds of the dependent variable. This may not hold true for all datasets, leading to poor performance.\nFeature Independence: Logistic regression assumes that the independent variables are independent of each other. In practice, this assumption may not hold, leading to multicollinearity issues.\nSensitivity to Outliers: Logistic regression can be sensitive to outliers, which can disproportionately influence the model’s coefficients and predictions.\nInability to Capture Complex Relationships: Logistic regression is a linear model and may not capture complex relationships between features and the target variable. Non-linear relationships may require more advanced models, such as decision trees or neural networks.\n-Requires Large Sample Size: Logistic regression requires a sufficient amount of data to produce reliable estimates. Small sample sizes may lead to overfitting and unreliable predictions. Sample size should be at least 10 times the number of features. eg if you have 5 features, you should have at least 50 samples.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Natural Language Processing - Chapter 5</span>"
    ]
  },
  {
    "objectID": "session6.html",
    "href": "session6.html",
    "title": "7  Natural Language Processing - Chapter 6",
    "section": "",
    "text": "8 Lecture 6",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Natural Language Processing - Chapter 6</span>"
    ]
  },
  {
    "objectID": "session6.html#sentiment-analysis-sa",
    "href": "session6.html#sentiment-analysis-sa",
    "title": "7  Natural Language Processing - Chapter 6",
    "section": "8.1 Sentiment Analysis (SA)",
    "text": "8.1 Sentiment Analysis (SA)\n\nSentiment analysis is the process of determining the sentiment or opinion expressed in a piece of text, such as positive, negative, or neutral.\nA lexicon\n\nA lexicon is a collection of words and phrases that are associated with specific sentiments, which can be used to analyze the sentiment of a given text.\nA sentiment lexicon can include words like “happy,” “sad,” “angry,” and “excited,” each associated with a specific sentiment score.\n\n\nEkman’s Six Basic Emotions are: - Happiness - Sadness - Anger - Fear - Surprise - Disgust",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Natural Language Processing - Chapter 6</span>"
    ]
  },
  {
    "objectID": "session6.html#resources-for-sentiment-lexicons",
    "href": "session6.html#resources-for-sentiment-lexicons",
    "title": "7  Natural Language Processing - Chapter 6",
    "section": "8.2 Resources for Sentiment Lexicons",
    "text": "8.2 Resources for Sentiment Lexicons\n\nVADER (Valence Aware Dictionary and sEntiment Reasoner): A lexicon specifically designed for sentiment analysis in social media. It provides sentiment scores for words and phrases, along with rules for handling negations and intensifiers.\n\nVADER is particularly effective for analyzing short texts, such as tweets and product reviews, and is widely used in natural language processing tasks.\nVADER uses a combination of lexical features, syntactic rules, and sentiment intensity scores to determine the sentiment of a given text.\nIt is available in the NLTK library and can be easily integrated into Python applications.\nUses valence score (-1 to 1) to represent the sentiment of a word or phrase.\n\nAFINN (Affective Norms for English Words): A lexicon that provides affective ratings for English words, including valence, arousal, and dominance scores.\n\nIt is a list of 3300+ English words rated for valence (positive or negative sentiment) on a scale from -5 to +5.\nAFINN is useful for sentiment analysis tasks, particularly in social media and online reviews.\n\nSentiWordNet: A lexical resource that assigns sentiment scores to WordNet synsets. It provides positive, negative, and objective scores for each synset (group of synonymous words which are words that have similar/same meanings eg Happy and Joyful), allowing for more nuanced sentiment analysis.\n\nIt is beneficial for tasks that require understanding the sentiment of words in context.\n\nLIWC (Linguistic Inquiry and Word Count): A text analysis tool that categorizes words into psychological and linguistic categories. It provides insights into emotional, cognitive, and structural aspects of text.\n\nLIWC is widely used in psychology and social sciences for analyzing the emotional content of text data.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Natural Language Processing - Chapter 6</span>"
    ]
  },
  {
    "objectID": "session6.html#how-to-do-sentiment-analysis",
    "href": "session6.html#how-to-do-sentiment-analysis",
    "title": "7  Natural Language Processing - Chapter 6",
    "section": "8.3 How to do Sentiment Analysis?",
    "text": "8.3 How to do Sentiment Analysis?\n\nTwo main approaches to sentiment analysis:\nRule-based: Uses predefined rules and lexicons to determine sentiment. It relies on linguistic patterns, such as negations, intensifiers, and sentiment-bearing words.\n\nExample: “I love this product!” would be classified as positive based on the presence of the word “love.”\nPros: Simple to implement, interpretable results.\nCons: Limited flexibility, may not capture complex sentiments or sarcasm.\n\nScoring in Lexicon-based sentiment analysis:\n\nPrepare a sentiment lexicon with words and their corresponding sentiment scores. Scoring can be binary, where words are classified as positive, negative, or neutral. or categorical, where words are classified into multiple categories (e.g., happy, sad, angry). or scale based, where words are assigned a score on a scale (e.g., -1 to 1).\nText preprocessing: Clean and preprocess the text data by removing stop words, punctuation, and special characters. Tokenize the text into words or phrases.\nMatch and Score: For each word in the text, check if it exists in the sentiment lexicon. If it does, retrieve its sentiment score and add it to the overall sentiment score for the text.\nCalculate the overall sentiment score by summing the individual word scores. The final sentiment score can be positive, negative, or neutral based on a predefined threshold.\nExample: If the sentiment score is greater than 0, classify the text as positive; if less than 0, classify it as negative; and if equal to 0, classify it as neutral.\n\nMachine learning-based: Uses supervised learning algorithms to train a model on labeled data. It learns to classify text based on features extracted from the text, such as word frequencies or TF-IDF scores.\nUses supervised learning algorithms, such as Naive Bayes, Logistic Regression, Support Vector Machines (SVMs), Neural Networks, to classify text into sentiment categories.\n\nExample: A model trained on positive and negative movie reviews can classify new reviews as positive or negative based on learned patterns.\nUses deep learning models, such as Recurrent Neural Networks (RNNs) or Transformers, to capture complex relationships in the text data.\n\nExample: A model trained on positive and negative movie reviews can classify new reviews as positive or negative based on learned patterns.\n\nPros: More flexible, can capture complex sentiments and context.\nCons: Requires labeled data, may be less interpretable.\n\n\n\n\n\n\n\n\n\n\nFeature\nRule-Based Sentiment Analysis\nMachine Learning-based Sentiment Analysis\n\n\n\n\nMethodology\nUses predefined rules and lexicons\nUses supervised learning algorithms\n\n\nExamples\nVADER, AFINN, LIWC\nNaive Bayes, Logistic Regression, SVMs\n\n\nAdaptability\nLimited flexibility, may not capture complex sentiments\nMore flexible, can capture complex sentiments and context\n\n\nData Requirements\nLow. Requires a sentiment lexicon\nHigh. Requires labeled training data\n\n\nContext Sensitivity\nLimited context sensitivity\nCan capture context and nuances\n\n\nLanguage evolution\nMay not adapt well to new language trends\nCan learn from new data and adapt to language evolution\n\n\nApplication\nSuitable for simple tasks\nSuitable for complex tasks and large datasets\n\n\nPrecision and Recall\nMay have lower precision and recall\nCan achieve higher precision and recall with sufficient data\n\n\n\n\nHybrid Approach: Combines rule-based and machine learning-based methods to leverage the strengths of both approaches. It uses predefined rules for initial sentiment classification and then refines the results using machine learning models.\n\nExample: A hybrid model may use VADER for initial sentiment scoring and then apply a machine learning model to improve accuracy.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Natural Language Processing - Chapter 6</span>"
    ]
  },
  {
    "objectID": "session6.html#python-libraries-and-tools-for-sa",
    "href": "session6.html#python-libraries-and-tools-for-sa",
    "title": "7  Natural Language Processing - Chapter 6",
    "section": "8.4 Python Libraries and Tools for SA",
    "text": "8.4 Python Libraries and Tools for SA\n\nNLTK (Natural Language Toolkit): A popular library for natural language processing in Python. It provides tools for text preprocessing, tokenization, stemming, and sentiment analysis using VADER and WordNet.\nTextBlob: A simple library for processing textual data. It provides a user-friendly API for common natural language processing tasks, including sentiment analysis, part-of-speech tagging, and noun phrase extraction.\nTensorFlow: An open-source machine learning library developed by Google. It provides tools for building and training deep learning models, including recurrent neural networks (RNNs) and transformers for sentiment analysis. Suited for large-scale applications and production environments.\nPyTorch: An open-source machine learning library developed by Facebook. It is widely used for deep learning applications and provides a flexible platform for building and training neural networks.\nHugging Face Transformers: A library that provides pre-trained models for natural language processing tasks, including sentiment analysis, text generation, and translation. It supports various architectures like BERT, GPT-2, and T5, making it easy to implement state-of-the-art models.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Natural Language Processing - Chapter 6</span>"
    ]
  },
  {
    "objectID": "session6.html#sa-challenges",
    "href": "session6.html#sa-challenges",
    "title": "7  Natural Language Processing - Chapter 6",
    "section": "8.5 SA Challenges",
    "text": "8.5 SA Challenges\n\nBiggest challenges in lexicon-based sentiment analysis?\nThe biggest challenge in lexicon-based sentiment analysis is the inability to capture context and sarcasm. Lexicon-based methods rely on predefined sentiment lexicons, which may not account for the nuances of language, such as:\nContextual Meaning: Words can have different meanings depending on the context in which they are used. For example, the word “sick” can be positive (e.g., “That movie was sick!”) or negative (e.g., “I feel sick”). Lexicon-based methods may misinterpret such words without considering context.\nSarcasm and Irony: Lexicon-based methods may struggle to identify sarcasm or irony, where the intended sentiment is opposite to the literal meaning of the words. For example, “Great job!” can be sarcastic in a negative context.\nAmbiguity: Some words may have multiple meanings or sentiments, leading to ambiguity in sentiment classification. For example, the word “bark” can refer to the sound a dog makes or the outer covering of a tree.\nCultural and Domain-Specific Language: Different cultures and domains may use language differently, leading to variations in sentiment expression. Lexicon-based methods may not generalize well across different contexts.\nNegation Handling: Lexicon-based methods may struggle to handle negations effectively. For example, “not good” should be classified as negative, but a simple lexicon lookup may misinterpret it as positive.\n\nAdd your notes and code examples below as you progress through the chapter.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Natural Language Processing - Chapter 6</span>"
    ]
  },
  {
    "objectID": "session7.html",
    "href": "session7.html",
    "title": "8  Natural Language Processing - Chapter 7",
    "section": "",
    "text": "9 Topic Modeling",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Natural Language Processing - Chapter 7</span>"
    ]
  },
  {
    "objectID": "session7.html#topic-modeling-1",
    "href": "session7.html#topic-modeling-1",
    "title": "8  Natural Language Processing - Chapter 7",
    "section": "9.1 Topic Modeling",
    "text": "9.1 Topic Modeling\nAs the volume of information grows, it becomes increasingly difficult to locate relevant content. Topic modeling is a technique used to uncover the hidden thematic structure in a collection of documents.\nWhen might you need to find hidden topics in text? - Organizing large collections of documents - Summarizing unstructured text data - Improving information retrieval and search - Feature selection for downstream machine learning tasks\n\n9.1.1 What is Topic Modeling?\n\nTopic modeling refers to a set of unsupervised machine learning techniques (such as clustering) for discovering the abstract “topics” that occur in a collection of documents.\nIt provides methods for automatically organizing, understanding, searching, and summarizing large electronic archives.\nCommon applications include document clustering, organizing large blocks of textual data, information retrieval from unstructured text, and feature selection.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Natural Language Processing - Chapter 7</span>"
    ]
  },
  {
    "objectID": "session7.html#latent-dirichlet-allocation-lda",
    "href": "session7.html#latent-dirichlet-allocation-lda",
    "title": "8  Natural Language Processing - Chapter 7",
    "section": "9.2 Latent Dirichlet Allocation (LDA)",
    "text": "9.2 Latent Dirichlet Allocation (LDA)\nLatent Dirichlet Allocation (LDA) is a foundational topic modeling technique. Let’s break down the name:\n\nLatent: Hidden or underlying (the topics are not directly observed).\nDirichlet: Refers to the Dirichlet distribution, which models the distribution of topics in documents and words in topics.\nAllocation: Assigning topics to words in documents.\n\n\n\n9.2.1 LDA Intuition\nLDA is a generative model: it assumes that documents are created by mixing topics, and each topic is a collection of words.\n\nEach topic is a distribution over words.\nEach document is a mixture of topics.\nEach word in a document is drawn from one of the document’s topics.\n\nIn short:\nDocuments are treated as bags of words. Each document is generated by a mixture of topics, and each topic is a mixture of words. The goal of LDA is to uncover these hidden topics from the observed words.\n\n\n\n9.2.2 How LDA Works\n\nFor each document, choose a distribution over topics.\nFor each word in the document:\n\nPick a topic from the document’s topic distribution.\nPick a word from the topic’s word distribution.\n\n\nLDA tries to reverse-engineer this process: given the words, it infers the topics.\n\n\n\n9.2.3 Key Components\n\nWord distribution per topic (β_k): Which words are likely in each topic.\nTopic distribution per document (θ_d): Which topics are likely in each document.\nTopic assignment for each word (z_{d,n}): Which topic generated each word.\n\n\n\n\n\n\n\nFigure 9.1: LDA as a graphical model\n\n\n\n\n\n\n9.2.4 The Trade-off: Document vs. Topic Sparsity\nLDA balances two goals:\n\nDocument sparsity: Each document should use as few topics as possible.\nTopic sparsity: Each topic should use as few words as possible.\n\nThese are in tension:\n- If all words in a document come from one topic, that topic must cover many words (less topic sparsity). - If each topic uses only a few words, documents must use more topics (less document sparsity).\nLDA finds a balance, resulting in topics that best explain the documents.\n\n\n\n9.2.5 LDA Parameters\n\nDocument density factor (α): Controls how many topics are expected per document.\nTopic word density factor (β): Controls how many words are expected per topic.\nNumber of topics (K): How many topics to extract.\n\n\n\n\n9.2.6 Parameter Estimation in LDA\nLDA uses Bayes’ Theorem to estimate the hidden variables (topics):\n[ P( ) = ]\n\nPrior: What we believe before seeing the data.\nPosterior: What we believe after seeing the data.\n\nKey challenge:\nComputing the exact posterior is intractable.\nCommon solutions: - Direct methods: Expectation Maximization, Variational Inference, Expectation Propagation. - Indirect methods: Estimate the posterior using sampling (e.g., Gibbs sampling, a type of Markov Chain Monte Carlo).\n# Python Example: LDA with scikit-learn and gensim\nimport numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation\nfrom sklearn.datasets import fetch_20newsgroups\nimport gensim\nfrom gensim import corpora\nimport pyLDAvis.gensim_models\nimport pyLDAvis\n\n# Example 1: LDA with scikit-learn\n# Load sample data\nnewsgroups = fetch_20newsgroups(subset='train', \n                               categories=['alt.atheism', 'sci.space', 'rec.sport.hockey'],\n                               remove=('headers', 'footers', 'quotes'))\n\n# Preprocess and vectorize\nvectorizer = CountVectorizer(max_features=1000, stop_words='english', \n                           min_df=2, max_df=0.95)\ndoc_term_matrix = vectorizer.fit_transform(newsgroups.data)\n\n# Fit LDA model\nlda = LatentDirichletAllocation(n_components=3, random_state=42, \n                              max_iter=10, learning_method='online')\nlda.fit(doc_term_matrix)\n\n# Display top words per topic\nfeature_names = vectorizer.get_feature_names_out()\nfor topic_idx, topic in enumerate(lda.components_):\n    top_words = [feature_names[i] for i in topic.argsort()[-10:]]\n    print(f\"Topic {topic_idx}: {', '.join(top_words)}\")\n\n# Example 2: LDA with gensim (more advanced)\n# Prepare documents\ntexts = [doc.split() for doc in newsgroups.data[:100]]  # Simple tokenization\ndictionary = corpora.Dictionary(texts)\ncorpus = [dictionary.doc2bow(text) for text in texts]\n\n# Train LDA model\nlda_model = gensim.models.LdaModel(corpus=corpus, id2word=dictionary, \n                                 num_topics=3, random_state=42,\n                                 alpha='auto', per_word_topics=True)\n\n# Print topics\nfor idx, topic in lda_model.print_topics(-1):\n    print(f'Topic {idx}: {topic}')\n\n# Get document-topic probabilities\ndoc_topics = lda_model[corpus[0]]\nprint(f\"Document 0 topic distribution: {doc_topics[0]}\")\n# Visualize (requires pyLDAvis)\nvis = pyLDAvis.gensim_models.prepare(lda_model, corpus, dictionary)\npyLDAvis.display(vis)\n\n\n\n9.2.7 Gibbs Sampling in LDA\nGibbs sampling is a Markov Chain Monte Carlo (MCMC) method used to estimate the posterior distribution of the hidden variables in LDA, specifically the topic assignments for each word.\n\n9.2.7.1 What does Gibbs sampling do in LDA?\n\nIt generates samples from the joint probability distribution of the topic assignments for all words in all documents.\nFor each word, it samples a new topic assignment based on the current state of all other assignments.\nThis iterative process gradually approximates the true posterior distribution.\n\n\n\n9.2.7.2 The Gibbs Sampling Algorithm\n\nInitialization:\nRandomly assign each word in each document to one of the K topics.\nIterative Sampling:\nFor each word ( w_{d,n} ) in document ( d ):\n\nRemove the current topic assignment for ( w_{d,n} ).\nCompute the probability of assigning topic ( k ) to ( w_{d,n} ) as: [ P(z_{d,n} = k z_{-}, w) ] where:\n\n( n_{d,k}^{-n} ): Number of words in document ( d ) assigned to topic ( k ), excluding the current word.\n( n_{k,w}^{-n} ): Number of times word ( w ) is assigned to topic ( k ), excluding the current word.\n( ), ( ): Dirichlet priors.\n( K ): Number of topics.\n( V ): Vocabulary size.\n\nSample a new topic for ( w_{d,n} ) from this distribution.\nUpdate the counts accordingly.\n\nRepeat:\nIterate over all words multiple times (epochs) until the topic assignments stabilize.\n\n\n\n9.2.7.3 Intuition\n\nDocument-topic counts encourage each document to use as few topics as possible (document sparsity).\nTopic-word counts encourage each topic to use as few words as possible (topic sparsity).\nOver many iterations, the assignments converge to a good approximation of the true topic structure.\n\n\n\n9.2.7.4 Summary\nGibbs sampling provides an efficient way to estimate the hidden topic structure in LDA by iteratively updating topic assignments for each word based on the current state of the model.\n\n\n\n\n9.2.8 Practical Considerations for LDA\n\nSimilar to k-means clustering: LDA is unsupervised and requires the number of topics to be specified in advance.\nBag-of-words assumption: Ignores word order and semantic context.\nPreprocessing is crucial: Good results require cleaning (stop words, stemming, lemmatization, etc.).\nNot ideal for short texts: LDA struggles with very short documents (e.g., tweets, headlines).\n\n\n\n\n9.2.9 Visualizing and Using LDA Results\n\nDocument-topic probabilities: Each document can be represented by its topic probability vector.\nWord-topic assignments: Each word in a document can be colored or labeled by its assigned topic.\nDocument similarity: Documents are similar if they have similar topic distributions (e.g., using Euclidean distance).",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Natural Language Processing - Chapter 7</span>"
    ]
  },
  {
    "objectID": "session7.html#top2vec-and-bertopic",
    "href": "session7.html#top2vec-and-bertopic",
    "title": "8  Natural Language Processing - Chapter 7",
    "section": "9.3 Top2Vec and BERTopic",
    "text": "9.3 Top2Vec and BERTopic\n\n9.3.1 Top2Vec: Distributed Topic Representations\nTop2Vec is a modern topic modeling algorithm (arXiv 2020, GitHub: ddangelov/Top2Vec) that automatically discovers topics in text data without requiring you to specify the number of topics in advance. It also works without extensive preprocessing (e.g., stop word removal, stemming, or lemmatization).\n\n9.3.1.1 Key Ideas\n\nVector Space Representation:\n\nEach document and word is embedded into a high-dimensional vector space.\nTerms are axes; documents and words are points or vectors.\n\nDimensionality Reduction:\n\nUMAP is used to reduce the dimensionality of the embeddings, preserving semantic relationships (using cosine similarity).\n\nClustering:\n\nHDBSCAN clusters the reduced vectors to identify topic groups.\n\nTopic Vectors:\n\nFor each cluster, a topic vector is calculated.\nThe closest words to each topic vector define the topic.\n\n\n\n\n9.3.1.2 Top2Vec Workflow\n\nEmbed documents and words (e.g., using doc2vec or word2vec).\nReduce dimensionality (UMAP).\nCluster embeddings (HDBSCAN).\nCalculate topic vectors for each cluster.\nFind closest words to each topic vector to define topics.\n\n\n\n\n\n\n\nFigure 9.2: Top2Vec\n\n\n\n\n\n9.3.1.3 Hyperparameters\n\nmin_count: Minimum word frequency to include in the model.\nembedding_model: Embedding method (e.g., ‘doc2vec’, ‘word2vec’).\numap_args: UMAP settings for dimensionality reduction.\nhdbscan_args: HDBSCAN clustering settings.\nvector_size: Size of embedding vectors.\n\n\n\n9.3.1.4 Pros and Cons\nPros: - Automatically determines the number of topics. - Preserves semantic meaning better than traditional models. - Minimal preprocessing required.\nCons: - Computationally intensive—use small datasets for initial testing. - Many hyperparameters to tune; results can vary. - Requires experimentation to achieve optimal results.\n# Python Example: Top2Vec\n\nfrom top2vec import Top2Vec\nfrom sklearn.datasets import fetch_20newsgroups\nimport numpy as np\n\n# Load sample data\nnewsgroups = fetch_20newsgroups(subset='train', \n                               categories=['alt.atheism', 'sci.space', 'rec.sport.hockey'],\n                               remove=('headers', 'footers', 'quotes'))\n\n# Take a smaller subset for faster processing\ndocuments = newsgroups.data[:200]  # Use only 200 documents for demo\n\n# Create Top2Vec model\n# Note: This may take several minutes to run\nmodel = Top2Vec(documents, \n                embedding_model='universal-sentence-encoder',  # Easier to use, no C compiler needed\n                speed='fast',  # Options: 'fast', 'deep', 'learn'\n                workers=1)\n\n# Get number of topics found\nnum_topics = model.get_num_topics()\nprint(f\"Number of topics found: {num_topics}\")\n\n# Get topic words for each topic\ntopic_words, word_scores, topic_nums = model.get_topics()\n\n# Display topics\nfor i, topic_num in enumerate(topic_nums):\n    print(f\"\\nTopic {topic_num}:\")\n    print(f\"Words: {', '.join(topic_words[i][:10])}\")  # Show top 10 words\n\n# Get topic for a specific document\ndoc_topics, doc_scores, doc_ids = model.search_documents_by_topic(topic_num=0, num_docs=3)\nprint(f\"\\nSample documents for Topic 0:\")\nfor i, doc_id in enumerate(doc_ids):\n    print(f\"Doc {doc_id}: {documents[doc_id][:100]}...\")\n\n# Search for similar documents using keywords\ntry:\n    doc_topics, doc_scores, doc_ids = model.search_documents_by_keywords(keywords=[\"space\", \"NASA\"], num_docs=3)\n    print(f\"\\nDocuments similar to 'space, NASA':\")\n    for i, doc_id in enumerate(doc_ids):\n        print(f\"Score: {doc_scores[i]:.3f} - {documents[doc_id][:100]}...\")\nexcept:\n    print(\"No documents found for these keywords\")\n\n\n\n\n9.3.2 BERTopic\nBERTopic is another modern topic modeling technique inspired by Top2Vec, but leverages transformer-based embeddings (e.g., BERT).\n\n9.3.2.1 Key Features\n\nNo centroid calculation:\nEach document in a cluster is treated as unique.\nTopic extraction:\nUses class-based TF-IDF to extract topic words from clusters.\nTransformer-based:\nUtilizes contextual embeddings for improved topic quality, especially in nuanced or short texts.\nComputational cost:\nMore resource-intensive due to transformer models.\n\n\n\n\n\n9.3.3 K-means Clustering with Word2Vec\n\nEmbed documents using Word2Vec.\nCluster the embeddings using k-means.\nBest for short texts (e.g., tweets, headlines).\n\n\nSummary Table\n\n\n\n\n\n\n\n\n\n\n\nMethod\nNeeds # Topics?\nPreprocessing\nHandles Short Texts\nComputational Cost\nNotes\n\n\n\n\nLDA\nYes\nYes\nNo\nModerate\nClassic, interpretable\n\n\nTop2Vec\nNo\nNo\nYes\nHigh\nFinds topics automatically\n\n\nBERTopic\nNo\nNo\nYes\nVery High\nTransformer-based\n\n\nK-means+W2V\nYes\nYes\nYes\nLow/Moderate\nSimple, fast\n\n\n\n\nPractical Tips: - Start with small datasets to experiment with Top2Vec or BERTopic. - Tune hyperparameters for your specific data and task. - Consider computational resources—transformer-based models can be slow. - Use visualizations to interpret and validate discovered topics.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Natural Language Processing - Chapter 7</span>"
    ]
  },
  {
    "objectID": "session7.html#the-revolution-in-nlp",
    "href": "session7.html#the-revolution-in-nlp",
    "title": "8  Natural Language Processing - Chapter 7",
    "section": "9.4 The Revolution in NLP",
    "text": "9.4 The Revolution in NLP\nNatural Language Processing (NLP) has undergone a revolution in recent years, primarily due to the introduction of the transformer model. Transformers are trained on massive datasets using objectives like masked language modeling (predicting missing words in a sentence). Further improvements are achieved through Reinforcement Learning from Human Feedback (RLHF), allowing models to better align with human preferences.\n\nExample:\nGiven the sentence: “The cat sat on the ___,” a transformer model predicts the missing word, such as “mat”.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Natural Language Processing - Chapter 7</span>"
    ]
  },
  {
    "objectID": "session7.html#language-is-hard",
    "href": "session7.html#language-is-hard",
    "title": "8  Natural Language Processing - Chapter 7",
    "section": "9.5 Language is Hard",
    "text": "9.5 Language is Hard\nDespite impressive progress, language remains a challenging domain for AI.\n\n9.5.1 Why is Language Hard?\n\nInfinite Possibilities:\nMost sentences you hear are unique—you’ve never heard them before and may never hear them again.\nAmbiguity:\n\nLexical Ambiguity: Words can have multiple meanings.\n&gt; Example: “bank” (river bank vs. financial bank)\n\nStructural Ambiguity: Sentences can be interpreted in different ways.\n&gt; Example: “I saw the man with the telescope.” (Who has the telescope?)\n\n\nMany thinkers have argued that true human-level language understanding may be impossible for machines. Current LLMs (Large Language Models) appear to process and reason with language at a high level, but is this really human-like understanding?",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Natural Language Processing - Chapter 7</span>"
    ]
  },
  {
    "objectID": "session7.html#brief-history",
    "href": "session7.html#brief-history",
    "title": "8  Natural Language Processing - Chapter 7",
    "section": "9.6 Brief History",
    "text": "9.6 Brief History\n\nDescartes:\nArgued that a machine could never truly imitate a human; there would always be a way to tell the difference.\nTuring Test:\nProposed by Alan Turing as a test of a machine’s ability to exhibit intelligent behavior indistinguishable from a human.\n\nA human judge engages in a conversation with both a human and a machine.\n\nIf the judge cannot reliably tell which is which, the machine is said to have passed the test.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Natural Language Processing - Chapter 7</span>"
    ]
  },
  {
    "objectID": "session7.html#nlp-the-basic-approach",
    "href": "session7.html#nlp-the-basic-approach",
    "title": "8  Natural Language Processing - Chapter 7",
    "section": "9.7 NLP: The Basic Approach",
    "text": "9.7 NLP: The Basic Approach\n\n9.7.1 Background\n\nNumerical Data: Numbers, measurements, etc.\nCategorical Data: Discrete categories or labels.\nText Data: Unstructured, variable-length, and context-dependent (e.g., email content, headlines, political speeches).\n\nText data is fundamentally different from structured data. Can we treat language as structured data for machine learning?\n\n9.7.1.1 Making Language into Structured Data\n\nBag-of-Words Representation:\n\nAssign a feature (column) for each word in the vocabulary.\n\nFor a given text, the value is 1 if the word occurs, 0 otherwise.\n\nAlternative values: word counts or TF-IDF scores.\n\nMost features are 0 for any given text (sparse representation).\n\n\n\n\n9.7.1.2 Supervised ML for Text Processing\n\nLabeled Text Data: Enables building classifiers for:\n\nSpam Detection\n\nSentiment Analysis\n\nTopic Detection\n\n…and more\n\n\nRaises questions: Does this approach capture real understanding? How does it relate to the Turing Test?\n\n\n\n9.7.1.3 Bag-of-Words Limitation\n\nIgnores word order and context (“bag” of words).\n\nCannot distinguish between “dog bites man” and “man bites dog”.\n\n\n\n\n\n9.7.2 Language Modeling with N-grams\n\nGoal: Assign a probability to a sequence of words.\nMarkov Assumption: The probability of a word depends only on the previous n-1 words.\n\n\n9.7.2.1 Example: “He went to the store”\n\nUnigrams (1-grams): He, went, to, the, store\nBigrams (2-grams): He went, went to, to the, the store\nTrigrams (3-grams): He went to, went to the, to the store\n4-grams: He went to the, went to the store\n5-gram: He went to the store\n\n\n\n9.7.2.2 N-gram Approximations\n\nBigram Model:\n( p() = p() p( ) p( ) p( ) p( ) )\nTrigram Model:\n( p() = p() p( ) p( ) p( ) p( ) )\nKey Idea:\nN-gram models capture some local word order, but struggle with long-range dependencies and rare phrases.\n\n\n\n\n\n9.7.3 Training an LLM\n\n“A general language model (LM) should be able to compute the probability of (and also generate) any string.”\n(Radford et al., 2019)\n\n\n\n9.7.4 What Does Training an LLM Look Like?\nConsider how humans complete sentences:\n- *As Descartes said, “I think, therefore I .”*\n- For all intents and**\n- I learned how to drive a ___\nOr in dialogue:\n&gt; Monica: Okay, everybody relax. This is not even a ___\n&gt; Rachel: Oh God… well, it started about a half hour before the ___\n&gt; Ross: (squatting and reading the instructions) I’m supposed to attach a brackety ___\nThe core training objective for LLMs is next word prediction:\nGiven a sequence of words, predict the most likely next word.\n\n9.7.4.1 Neural Network for Next Word Prediction\n\nThe model is a neural network that outputs a score for every word in the vocabulary.\nThese scores are converted into probabilities using the softmax function.\nFor example, with a vocabulary of 50,000 words, the output might look like: [fish: 0.00002, help: 0.00002, ..., the: 0.00002, ..., aardvarks: 0.00002]\n\n\n\n9.7.4.2 Training Process\n\nCompute Loss:\n\nThe true next word is masked (hidden).\n\nThe model predicts probabilities for all words.\n\nThe loss function measures how well the model predicts the correct word (e.g., Loss = 1 - prob(correct word)).\n\nIf the model assigns a probability of 1 to the correct word, loss is 0 (best). If 0, loss is 1 (worst).\n\nUpdate Weights:\n\nThe model adjusts its internal weights to increase the probability of the correct word.\n\nThis also slightly decreases the probability for all other words.\n\nEach training example provides a small update—repeated millions or billions of times.\n\n\n\n\n9.7.4.3 Example: Weight Updates\nSuppose the correct next word is “fish”:\n- The model increases the weights leading to “fish”.\n- The probabilities for other words (e.g., “help”, “the”, “aardvarks”) are slightly reduced.\n\nEach example nudges the model to make the correct word more likely in context, and less likely for others.\nOver time, the model learns to predict words in a wide variety of contexts.\n\n\nThis is the fundamental process behind training large language models: predict the next word, compute the loss, update the weights, and repeat—at massive scale.\n\n\n\n9.7.5 Probing GPT\nLarge Language Models (LLMs) today generate highly coherent, grammatical text that can be indistinguishable from human output. They demonstrate some understanding of hierarchical structure and abstract linguistic categories (Mahowald et al., 2024).\nWhile these models are not perfect learners of abstract linguistic rules, neither are humans. LLMs are progressing toward acquiring formal linguistic competence and have already challenged claims about the impossibility of learning certain linguistic knowledge—such as hierarchical structure and abstract categories—from input alone (Mahowald et al., 2024).",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Natural Language Processing - Chapter 7</span>"
    ]
  },
  {
    "objectID": "session7.html#ai-where-are-we-heading",
    "href": "session7.html#ai-where-are-we-heading",
    "title": "8  Natural Language Processing - Chapter 7",
    "section": "9.8 AI: Where Are We Heading?",
    "text": "9.8 AI: Where Are We Heading?\nArtificial General Intelligence (AGI):\nAGI is defined as AI that matches or surpasses human cognitive capabilities across a wide range of tasks.\nAGI Benchmarks:\n\nThe Robot College Student Test (Goertzel):\nA machine enrolls in a university, takes and passes the same classes as humans, and obtains a degree. LLMs can now pass university-level exams without attending classes.\nThe Employment Test (Nilsson):\nA machine performs an economically important job at least as well as humans. AI is already replacing humans in roles ranging from fast food to marketing.\nThe Ikea Test (Marcus):\nAn AI views the parts and instructions of an Ikea flat-pack product, then controls a robot to assemble the furniture correctly.\nThe Coffee Test (Wozniak):\nA machine enters an average home and figures out how to make coffee: find the machine, coffee, water, mug, and brew coffee by pushing the right buttons. This remains unsolved.\nThe Modern Turing Test (Suleyman):\nAn AI is given $100,000 and must turn it into $1 million.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Natural Language Processing - Chapter 7</span>"
    ]
  },
  {
    "objectID": "session7.html#applications-of-llms",
    "href": "session7.html#applications-of-llms",
    "title": "8  Natural Language Processing - Chapter 7",
    "section": "9.9 Applications of LLMs",
    "text": "9.9 Applications of LLMs\n\nAI interfaces for customer support and onboarding\nResearch portals with Retrieval-Augmented Generation (RAG)\nAutomated customer support (e.g., Zendesk)\nAccessibility tools (e.g., BeMyAI)",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Natural Language Processing - Chapter 7</span>"
    ]
  },
  {
    "objectID": "session7.html#takeaways",
    "href": "session7.html#takeaways",
    "title": "8  Natural Language Processing - Chapter 7",
    "section": "9.10 Takeaways",
    "text": "9.10 Takeaways\n\nLanguage is Hard:\n\nLanguage is infinite and ambiguous (both lexically and structurally).\n\nThe Revolution in NLP:\n\nLLMs now approach human-level language ability.\n\nExciting Research Directions:\n\nBuilding applications with LLMs\n\nProbing their abilities\n\nPowerful AI is Coming:\n\nThe field is rapidly advancing, with significant societal impact on the horizon.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Natural Language Processing - Chapter 7</span>"
    ]
  },
  {
    "objectID": "session8.html",
    "href": "session8.html",
    "title": "9  Natural Language Processing - Chapter 8",
    "section": "",
    "text": "10 Chapter 8: Lexical Semantics and Vector Embeddings",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Natural Language Processing - Chapter 8</span>"
    ]
  },
  {
    "objectID": "session8.html#word-meanings",
    "href": "session8.html#word-meanings",
    "title": "9  Natural Language Processing - Chapter 8",
    "section": "10.1 Word Meanings",
    "text": "10.1 Word Meanings\nUnderstanding word meanings is fundamental in NLP. Here are some key points:\n\nWords have meanings: But meanings can be complex and context-dependent.\nSome words are similar to others: For example, “car” and “automobile”.\n\n\n10.1.1 Challenges in Defining Word Meanings\n\nHomonymy: Words with multiple unrelated meanings (e.g., “bank” as a financial institution vs. “bank” of a river).\nPolysemy: Words with multiple related senses (e.g., “paper” as material vs. “paper” as an academic article).\nMany-to-many mapping: Concepts can be associated with multiple words, and words can represent multiple concepts.\n\n\n\n10.1.2 Word Relations\n\nSynonymy: Words with similar meanings (e.g., “big” and “large”).\nAntonymy: Words with opposite meanings (e.g., “hot” and “cold”).\nSimilarity: Degree to which words are alike (e.g., “cup” and “mug”).\nRelatedness: Words that are related but not necessarily similar (e.g., “doctor” and “hospital”).\nConnotation: Emotional or cultural association (e.g., “childish” vs. “youthful”).",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Natural Language Processing - Chapter 8</span>"
    ]
  },
  {
    "objectID": "session8.html#vector-semantics",
    "href": "session8.html#vector-semantics",
    "title": "9  Natural Language Processing - Chapter 8",
    "section": "10.2 Vector Semantics",
    "text": "10.2 Vector Semantics\nVector semantics represents words and documents as points in a multidimensional space.\n\n10.2.1 Idea 1: Defining Meaning by Linguistic Distribution\n\n“You shall know a word by the company it keeps.”\n— J.R. Firth\n\nWords that appear in similar contexts tend to have similar meanings.\n\n\n10.2.2 Idea 2: Meaning as a Point in Multidimensional Space\nEach word or document is represented as a vector of numbers.\n\n10.2.2.1 Example: Term-Document Matrix\n\n\n\n\nDoc1\nDoc2\nDoc3\n\n\n\n\napple\n2\n0\n1\n\n\norange\n1\n1\n0\n\n\nbank\n0\n2\n1\n\n\n\nEach row is a word, each column is a document, and the numbers are word counts.\n\n\n\nVisualizing Document Vectors\n\n\n\n\n10.2.2.2 Comparing Document Vectors\nSuppose we have two document vectors:\n- Doc1: [2, 1, 0]\n- Doc2: [0, 1, 2]\nWe can measure their similarity using various metrics.\n\n\n10.2.2.3 Example: Word Context Matrix\n\n\n\n\ncontext1\ncontext2\ncontext3\n\n\n\n\nbank\n3\n0\n2\n\n\nriver\n0\n2\n3\n\n\nmoney\n2\n3\n0\n\n\n\n\n\n\n\n10.2.3 Comparing Word Vectors\n\nCosine Similarity: Measures the angle between two vectors (e.g., “king” and “queen” have high cosine similarity).\n::: {#fd6921e7 .cell execution_count=1} ``` {.python .cell-code} from sklearn.metrics.pairwise import cosine_similarity import numpy as np\nvec_king = np.array([0.5, 0.8, 0.1]) vec_queen = np.array([0.51, 0.79, 0.12])\ncos_sim = cosine_similarity([vec_king], [vec_queen]) print(“Cosine Similarity:”, cos_sim[0][0]) ```\n::: {.cell-output .cell-output-stdout} Cosine Similarity: 0.9996667100448761 ::: :::\nEuclidean Distance: Measures the straight-line distance between vectors.\n::: {#32c297bd .cell execution_count=2} ``` {.python .cell-code} from scipy.spatial.distance import euclidean import numpy as np\nvec_king = np.array([0.5, 0.8, 0.1]) vec_queen = np.array([0.51, 0.79, 0.12])\ndist = euclidean(vec_king, vec_queen) print(“Euclidean Distance:”, dist) ```\n::: {.cell-output .cell-output-stdout} Euclidean Distance: 0.02449489742783178 ::: :::\nDot Product: Measures similarity based on direction and magnitude.\n::: {#d0ff5f3a .cell execution_count=3} ``` {.python .cell-code} import numpy as np\nvec_king = np.array([0.5, 0.8, 0.1]) vec_queen = np.array([0.51, 0.79, 0.12])\ndot = np.dot(vec_king, vec_queen) print(“Dot Product:”, dot) ```\n::: {.cell-output .cell-output-stdout} Dot Product: 0.8990000000000001 ::: :::\n\nThese metrics help quantify how similar or related two words or documents are in vector space.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Natural Language Processing - Chapter 8</span>"
    ]
  },
  {
    "objectID": "session8.html#word2vec",
    "href": "session8.html#word2vec",
    "title": "9  Natural Language Processing - Chapter 8",
    "section": "10.3 Word2Vec",
    "text": "10.3 Word2Vec\n\n10.3.1 Learning the Embeddings\nInstead of simply counting how often each word w occurs near “bank”, Word2Vec takes a different approach:\nit trains a classifier on a binary prediction task.\nWe don’t actually care about the classifier’s predictions; instead, we use the learned classifier weights as the word embeddings.\nBig idea: Self-supervision\nA word c that occurs near “bank” in the corpus is a good candidate for a word that is similar to “bank”—no need for human annotations.\n\nDense vectors (embeddings) work better in nearly every NLP task than sparse vectors.\nWe don’t completely understand all the reasons for this.\n— SLP ch 6, p17\n\n\n\n\n10.3.2 Learning Word2Vec Embeddings\n\nTreat the target word t and a neighboring context word c as a positive example.\nRandomly sample other words in the vocabulary to get negative examples.\nUse logistic regression to train a classifier to distinguish positive from negative pairs.\nUse the learned weights as the word embeddings.\n\n\n\n\n10.3.3 How Word2Vec Learns Embeddings\n\nEmbeddings: For each word (w) and context (c), Word2Vec learns a dense vector representation.\nSkip-Gram Training Data: The model uses pairs of (target word, context word) from a corpus.\nLearning Objective:\n\nMaximize the similarity between target words and their true context words (positive pairs).\nMinimize the similarity between target words and randomly chosen words (negative pairs).\n\n\n\n10.3.3.1 Training Process\n\nInitialization: Start with random vectors for each word in the vocabulary.\nPositive Examples: For each word in the corpus, pair it with its nearby context words.\nNegative Examples: Pair the target word with randomly sampled words from the vocabulary.\nClassifier Training: Use logistic regression to distinguish positive from negative pairs, updating the vectors to improve classification.\nResult: Discard the classifier; the learned vectors are the word embeddings.\n\n\n\n10.3.3.2 Analogy Reasoning with Embeddings\nWord2Vec embeddings capture relationships between words. For example, vector arithmetic can reveal analogies:",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Natural Language Processing - Chapter 8</span>"
    ]
  },
  {
    "objectID": "session8.html#takeaways",
    "href": "session8.html#takeaways",
    "title": "9  Natural Language Processing - Chapter 8",
    "section": "10.4 Takeaways",
    "text": "10.4 Takeaways\n\n10.4.1 Key Takeaways\n\nVector Semantics:\nSummarizes word contexts as dense vectors, enabling mathematical operations on word meanings.\nWord2Vec:\n\nProvides powerful word representations for many NLP tasks (e.g., translation, sentiment analysis, question answering).\nUses the skip-gram word prediction method to learn embeddings.\nEmbeddings capture abstract relationships (e.g., male–female, capital–city, comparative–superlative).\nNote: Word2Vec produces static embeddings—each word has a single vector, regardless of context.\n\n\n\n10.4.1.1 Practical Steps\n\nLoad pre-trained Word2Vec embeddings.\nExplore vector similarities to find related words.\nSolve analogies (e.g., “man” is to “king” as “woman” is to ?).\nCompute sentence embeddings (e.g., by averaging word vectors).\nUse embeddings for sentence classification and compare with Bag-of-Words (BoW) models.\n\nAdd your notes and code examples below as you progress through the chapter.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Natural Language Processing - Chapter 8</span>"
    ]
  },
  {
    "objectID": "session9.html",
    "href": "session9.html",
    "title": "10  Natural Language Processing - Chapter 9",
    "section": "",
    "text": "11 Chapter 9: Simple Neural Networks",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Natural Language Processing - Chapter 9</span>"
    ]
  },
  {
    "objectID": "session9.html#ambiguity",
    "href": "session9.html#ambiguity",
    "title": "10  Natural Language Processing - Chapter 9",
    "section": "11.1 Ambiguity",
    "text": "11.1 Ambiguity\nAmbiguity arises when a sentence can be interpreted in more than one way. For example:\n\n“I saw the man with the telescope.”\n\nWho has the telescope—the speaker or the man?\n\n\nAmbiguity is a significant challenge in NLP, as it can lead to multiple interpretations of the same sentence.\nThere are several types of ambiguity, including:\n\nLexical ambiguity: When a word has multiple meanings.\nStructural (syntactic) ambiguity: When a sentence can be parsed in different ways.\n\nTo address ambiguity, we need models that capture sentence structure, such as:\n\nParse trees\nDependency graphs\nOther structural representations",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Natural Language Processing - Chapter 9</span>"
    ]
  },
  {
    "objectID": "session9.html#simple-neural-networks-and-neural-language-models",
    "href": "session9.html#simple-neural-networks-and-neural-language-models",
    "title": "10  Natural Language Processing - Chapter 9",
    "section": "11.2 Simple Neural Networks and Neural Language Models",
    "text": "11.2 Simple Neural Networks and Neural Language Models\nNeural networks are powerful models that consist of layers of simple computational units. Each unit takes a vector of input values, applies weights and a bias, and produces a single output value through an activation function.\n\n11.2.1 Structure of a Simple Neural Network\nA typical neural network consists of:\n\nInput layer: Receives the input features.\nWeights: Each input is multiplied by a corresponding weight.\nWeighted sum: The weighted inputs are summed, and a bias term is added.\nActivation function: The sum is passed through a non-linear function.\nOutput layer: Produces the final prediction.\n\n\n11.2.1.1 Mathematical Formulation\nGiven input vector x = (x₁, x₂, …, xₙ), weight vector w = (w₁, w₂, …, wₙ), and bias b, the unit computes:\n\\[\nz = \\mathbf{w} \\cdot \\mathbf{x} + b\n\\]\nThe output y is then:\n\\[\ny = a = f(z)\n\\]\nwhere f is the activation function.\n\n\n\n11.2.2 Example Walkthrough\nSuppose we have a unit with weights w = [0.5, -0.3], bias b = 0.1, and input x = [2, 3].\n\nWeighted sum: \\[\nz = (0.5 \\times 2) + (-0.3 \\times 3) + 0.1 = 1.0 - 0.9 + 0.1 = 0.2\n\\]\nActivation function:\nUsing the sigmoid function: \\[\n\\text{sigmoid}(z) = \\frac{1}{1 + e^{-z}} = \\frac{1}{1 + e^{-0.2}} \\approx 0.55\n\\]\n\nSo, the output of this unit is approximately 0.55.\n\n\n11.2.3 Common Activation Functions\n\nSigmoid:\n\\[\n\\sigma(z) = \\frac{1}{1 + e^{-z}}\n\\] Maps output to (0, 1). Useful for probabilities, but can cause vanishing gradients.\nTanh:\n\\[\n\\tanh(z) = \\frac{e^{z} - e^{-z}}{e^{z} + e^{-z}}\n\\] Maps output to (-1, 1). Zero-centered and often preferred over sigmoid.\nReLU (Rectified Linear Unit):\n\\[\n\\text{ReLU}(z) = \\max(0, z)\n\\] Simple and effective. Avoids vanishing gradients for positive values.\n\n\n\n11.2.4 Why Activation Functions Matter\nActivation functions introduce non-linearity, allowing neural networks to model complex relationships. For example, ReLU is widely used because it is computationally efficient and helps mitigate the vanishing gradient problem, which can occur with sigmoid or tanh for large input values.\n\n\n\n\nNeural Network Unit\n\n\nFigure: A single neural network unit computes a weighted sum of its inputs, adds a bias, and applies an activation function to produce its output.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Natural Language Processing - Chapter 9</span>"
    ]
  },
  {
    "objectID": "session9.html#the-power-of-multi-layer-neural-networks-xor-example",
    "href": "session9.html#the-power-of-multi-layer-neural-networks-xor-example",
    "title": "10  Natural Language Processing - Chapter 9",
    "section": "11.3 The Power of Multi-Layer Neural Networks: XOR Example",
    "text": "11.3 The Power of Multi-Layer Neural Networks: XOR Example\nA single neural unit (perceptron) can compute simple logical functions like AND and OR, but not XOR. This is because XOR is not linearly separable—a single line cannot separate its positive and negative cases.\n\n11.3.1 Truth Tables\n\n\n\nx₁\nx₂\nAND\nOR\nXOR\n\n\n\n\n0\n0\n0\n0\n0\n\n\n0\n1\n0\n1\n1\n\n\n1\n0\n0\n1\n1\n\n\n1\n1\n1\n1\n0\n\n\n\n\n\n11.3.2 Perceptron as Linear Classifier\nA perceptron computes: \\[\ny = f(\\mathbf{w} \\cdot \\mathbf{x} + b)\n\\] where \\(f\\) is a step function. For AND and OR, weights and bias can be chosen so that the perceptron outputs the correct result. For XOR, no single line (decision boundary) can separate the outputs.\n\n\n11.3.3 Why XOR Needs Multiple Layers\n\nAND/OR: Can be separated by a line (linear).\nXOR: Requires a curve or multiple lines (non-linear).\n\n\n\n11.3.4 Multi-Layer Solution for XOR\nA two-layer network with ReLU activation can compute XOR. For example:\n\nHidden layer: 2 units\nOutput layer: 1 unit\n\nSuppose we choose weights and biases so that for input \\(\\mathbf{x} = [0, 0]\\), the hidden layer outputs \\([0, 0]\\) and the final output is 0. For other inputs, the network can be set up so that the output is 1 for \\([0, 1]\\) and \\([1, 0]\\), and 0 for \\([1, 1]\\).\nExercise: Try computing the outputs for all input pairs to see how the network solves XOR.\n\n\n11.3.5 Key Takeaway\nMulti-layer neural networks with non-linear activation functions can learn complex functions (like XOR) by forming new representations in hidden layers. This ability to learn useful representations is a major strength of neural networks.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Natural Language Processing - Chapter 9</span>"
    ]
  },
  {
    "objectID": "session9.html#feedforward-neural-networks",
    "href": "session9.html#feedforward-neural-networks",
    "title": "10  Natural Language Processing - Chapter 9",
    "section": "11.4 Feedforward Neural Networks",
    "text": "11.4 Feedforward Neural Networks\nFeedforward neural networks are the foundation of many modern neural architectures. While more complex models like RNNs and Transformers are widely used in NLP, the simple feedforward (or multilayer perceptron, MLP) network remains a crucial building block.\n\n11.4.1 What is a Feedforward Neural Network?\nA feedforward network consists of multiple layers of units (neurons) where information flows in one direction—from input to output—without cycles or loops. Each layer receives inputs from the previous layer and passes outputs to the next.\n\nInput layer: Receives the raw input features.\nHidden layers: Transform the input through learned weights and activation functions.\nOutput layer: Produces the final prediction (e.g., class probabilities).\n\n\nNote: Historically, “multilayer perceptron” (MLP) refers to these networks, though modern MLPs use activation functions like ReLU or tanh, not the original perceptron step function.\n\n\n\n11.4.2 Structure and Computation\nLet’s revisit logistic regression. Binary logistic regression can be viewed as a 1-layer network (not counting the input layer):\n\nInput: Vector \\(\\mathbf{x}\\)\nWeights: Vector \\(\\mathbf{w}\\)\nBias: Scalar \\(b\\)\nOutput: \\(y = \\sigma(\\mathbf{w} \\cdot \\mathbf{x} + b)\\), where \\(\\sigma\\) is the sigmoid function.\n\n\n11.4.2.1 Example: Logistic Regression as a Neural Network\nSuppose \\(\\mathbf{x} = [2, 3]\\), \\(\\mathbf{w} = [0.5, -0.3]\\), \\(b = 0.1\\):\n\\[\nz = (0.5 \\times 2) + (-0.3 \\times 3) + 0.1 = 0.2 \\\\\ny = \\sigma(0.2) \\approx 0.55\n\\]\n\n\n11.4.2.2 Adding a Hidden Layer\nA feedforward network with one hidden layer computes:\n\nHidden layer:\n\\[\n\\mathbf{h} = g(\\mathbf{W} \\mathbf{x} + \\mathbf{b})\n\\] where \\(g\\) is an activation function (e.g., ReLU, tanh), \\(\\mathbf{W}\\) is the weight matrix, and \\(\\mathbf{b}\\) is the bias vector.\nOutput layer:\n\\[\n\\mathbf{z} = \\mathbf{U} \\mathbf{h}\n\\] \\[\n\\mathbf{y} = \\text{softmax}(\\mathbf{z})\n\\] For binary classification, use sigmoid instead of softmax.\n\n\n\n11.4.2.3 Example: Feedforward Network for Classification\nSuppose we have:\n\nInput \\(\\mathbf{x} \\in \\mathbb{R}^3\\)\nHidden layer: 2 units, ReLU activation\nOutput layer: 3 units (for 3 classes), softmax activation\n\nLet: - \\(\\mathbf{W}\\) is \\(2 \\times 3\\) (hidden layer weights) - \\(\\mathbf{U}\\) is \\(3 \\times 2\\) (output layer weights) - \\(\\mathbf{b}\\) is \\(2 \\times 1\\) (hidden layer bias)\nThe computation is:\n\nHidden layer: \\[\n\\mathbf{h} = g(\\mathbf{W} \\mathbf{x} + \\mathbf{b})\n\\] where \\(g\\) is the ReLU activation function.\nOutput layer: \\[\n\\mathbf{z} = \\mathbf{U} \\mathbf{h}\n\\] \\[\n\\mathbf{y} = \\text{softmax}(\\mathbf{z})\n\\]\n\n\n\n\nFeedforward Neural Network",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Natural Language Processing - Chapter 9</span>"
    ]
  },
  {
    "objectID": "session9.html#applying-feedforward-networks-to-nlp-tasks",
    "href": "session9.html#applying-feedforward-networks-to-nlp-tasks",
    "title": "10  Natural Language Processing - Chapter 9",
    "section": "11.5 Applying Feedforward Networks to NLP Tasks",
    "text": "11.5 Applying Feedforward Networks to NLP Tasks\nFeedforward neural networks can be applied to a variety of NLP tasks. Let’s look at two common examples: text classification and language modeling.\n\n11.5.1 1. Text Classification\nSuppose we want to classify text (e.g., movie reviews as positive or negative). Traditionally, we might use logistic regression with binary features (e.g., word presence). With a feedforward network, we can do more:\n\nInput layer: Each word is represented as a vector (embedding), not just a binary feature.\nHidden layer: Allows the network to learn non-linear interactions between features.\nOutput layer: Produces a probability (e.g., positive or negative sentiment).\n\nWhy add a hidden layer?\nA hidden layer enables the network to capture complex patterns and interactions between words, which may improve performance over simple linear models.\nExample:\nSuppose our input is a review:\n\"The movie was surprisingly good.\"\nEach word is mapped to an embedding. The network can learn to focus on words like “surprisingly” and “good” to predict a positive sentiment.\n\n\n11.5.2 2. Learning Features Automatically\nA key strength of deep learning is the ability to learn features from data rather than relying on hand-crafted features. Hidden nodes can learn abstract representations by focusing on specific patterns in the embeddings.\n\nInstead of manually designing features, the network discovers useful patterns during training.\nThis is fundamental to the power of deep learning for NLP.\n\n\n\n11.5.3 3. Handling Variable-Length Input\nFeedforward networks expect fixed-size input. In NLP, sentences and documents vary in length. Some common solutions:\n\nPadding: Pad shorter inputs with zeros to match the length of the longest input.\nTruncation: Cut longer inputs to a fixed length.\nPooling: Create a single “sentence embedding” by combining word embeddings, e.g.:\n\nMean pooling: Take the average of all word embeddings.\nMax pooling: For each dimension, take the maximum value across all words.\n\n\n\n\n11.5.4 4. Multi-Class Classification\nIf you have more than two output classes (e.g., topic classification), add more output units—one for each class—and use a softmax layer to produce class probabilities.\n\n\n11.5.5 5. Language Modeling\nTask: Predict the next word \\(w_t\\) given previous words \\(w_{t-1}, w_{t-2}, \\ldots\\)\n\nTraditional approach: N-gram language models, which have limited ability to generalize.\nNeural approach: Feedforward neural language models use word embeddings and can generalize better.\n\nExample:\nSuppose the training data contains:\nI have to make sure that the cat gets fed.\nBut never:\ndog gets fed\nAt test time:\nI forgot to make sure that the dog gets ___\n\nAn N-gram model may not predict “fed” after “dog gets”.\nA neural language model can use the similarity between “cat” and “dog” embeddings to predict “fed” after “dog”.\n\nHandling sequences:\nFeedforward models use a sliding window of fixed length (e.g., previous 3 words) as input. More advanced models (like RNNs and Transformers) handle arbitrary-length sequences.\n\nSummary:\nFeedforward neural networks provide a foundation for many NLP tasks. They enable learning from data, handle non-linear patterns, and can generalize better than traditional models, especially when using learned word embeddings.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Natural Language Processing - Chapter 9</span>"
    ]
  },
  {
    "objectID": "session9.html#training-neural-networks",
    "href": "session9.html#training-neural-networks",
    "title": "10  Natural Language Processing - Chapter 9",
    "section": "11.6 Training Neural Networks",
    "text": "11.6 Training Neural Networks\nTraining a neural network involves adjusting its weights to minimize the difference between the predicted output and the true output. This is typically done using an algorithm called backpropagation combined with gradient descent.\n\n11.6.1 Training Steps\nFor each training example \\((\\mathbf{x}, y)\\):\n\nForward Pass:\nCompute the network’s prediction \\(\\hat{y}\\) by passing \\(\\mathbf{x}\\) through the network.\nCompute Loss:\nCalculate the loss \\(L(y, \\hat{y})\\), which measures how far the prediction is from the true value.\nBackward Pass (Backpropagation):\nCompute the gradients of the loss with respect to each weight in the network.\nUpdate Weights:\nAdjust each weight \\(w\\) using the gradients to reduce the loss (e.g., \\(w \\leftarrow w - \\eta \\frac{\\partial L}{\\partial w}\\), where \\(\\eta\\) is the learning rate).\n\n\n\n11.6.2 What Happens During Backpropagation?\n\nFor each output node, compute how much it contributed to the loss.\nFor each hidden node, determine how much it contributed to the output error (its “blame”).\nUse these contributions to update the weights from the hidden layer to the output layer, and from the input layer to the hidden layer.\n\nThis process is repeated for many epochs (passes through the training data) until the network’s predictions are sufficiently accurate.\nKey Points: - Training is iterative and data-driven. - Backpropagation efficiently computes gradients for all weights. - The learning rate controls how big each update step is.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Natural Language Processing - Chapter 9</span>"
    ]
  },
  {
    "objectID": "session10.html",
    "href": "session10.html",
    "title": "11  Natural Language Processing - Chapter 10",
    "section": "",
    "text": "12 Chapter 10: The Transformer and Pre-trained Language Models",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Natural Language Processing - Chapter 10</span>"
    ]
  },
  {
    "objectID": "session10.html#background",
    "href": "session10.html#background",
    "title": "11  Natural Language Processing - Chapter 10",
    "section": "12.1 Background",
    "text": "12.1 Background\n\n12.1.1 Descartes and the Nature of Thought\nPhilosophers like Descartes have long debated what it means to “think.” In the context of artificial intelligence, this leads us to consider whether machines can truly understand or just simulate understanding.\n\n\n12.1.2 The Turing Test\nAlan Turing proposed a test to determine if a machine can exhibit intelligent behavior indistinguishable from a human. If a human evaluator cannot reliably tell the machine from a human based on their responses, the machine is said to have passed the Turing Test.\n\n\n12.1.3 Singularity and AGI\nThe concept of the Singularity refers to a hypothetical point where artificial intelligence surpasses human intelligence, leading to rapid technological growth. AGI (Artificial General Intelligence) is the idea of machines that can perform any intellectual task that a human can.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Natural Language Processing - Chapter 10</span>"
    ]
  },
  {
    "objectID": "session10.html#the-challenge-of-language",
    "href": "session10.html#the-challenge-of-language",
    "title": "11  Natural Language Processing - Chapter 10",
    "section": "12.2 The Challenge of Language",
    "text": "12.2 The Challenge of Language\nLanguage is complex and powerful. It allows us to express an infinite number of ideas using a finite set of words and rules. Sentences can have unbounded dependencies, such as:\n\n“The cat that the dog chased ran away.”\n\nHere, “the cat” and “ran away” are connected, even though other words intervene.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Natural Language Processing - Chapter 10</span>"
    ]
  },
  {
    "objectID": "session10.html#the-miracle-of-llms-3-key-insights",
    "href": "session10.html#the-miracle-of-llms-3-key-insights",
    "title": "11  Natural Language Processing - Chapter 10",
    "section": "12.3 The Miracle of LLMs – 3 Key Insights",
    "text": "12.3 The Miracle of LLMs – 3 Key Insights\n\nBag of Words / N-grams\nEarly models represented text as unordered collections of words (bag of words) or short sequences (n-grams). For example, the sentence “The quick brown fox” as a bag of words is just {“The”, “quick”, “brown”, “fox”}.\nWord Embeddings and Self-Learning\nWord embeddings map words to high-dimensional vectors that capture semantic relationships. For example:\n\n“king” - “man” + “woman” ≈ “queen”\n“Paris” - “France” + “Italy” ≈ “Rome” These vectors capture analogies and relationships between words.\n\nThe Transformer\nTransformers (covered in detail next) revolutionized NLP by allowing models to consider the entire context of a sentence, not just local word sequences.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Natural Language Processing - Chapter 10</span>"
    ]
  },
  {
    "objectID": "session10.html#word-embeddings",
    "href": "session10.html#word-embeddings",
    "title": "11  Natural Language Processing - Chapter 10",
    "section": "12.4 Word Embeddings",
    "text": "12.4 Word Embeddings\nWord vectors are rich representations of word meaning and usage. They capture relationships such as:\n\nGender:\n“king” - “man” + “woman” ≈ “queen”\nCapital Cities:\n“Paris” - “France” + “Italy” ≈ “Rome”\nComparatives:\n“big” - “bigger” + “small” ≈ “smaller”\n\nThese embeddings allow models to understand and manipulate language in a way that reflects real-world relationships.\nWhat we need is a method to capture how words are combined to produce sentences and meaning—this is where models like the Transformer come in.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Natural Language Processing - Chapter 10</span>"
    ]
  },
  {
    "objectID": "session10.html#transformers",
    "href": "session10.html#transformers",
    "title": "11  Natural Language Processing - Chapter 10",
    "section": "12.5 Transformers",
    "text": "12.5 Transformers\n\n12.5.1 The Problem with Static Embeddings\nTraditional embeddings like word2vec are static:\nA word always has the same vector, regardless of context.\n\nExample:\n“The chicken didn’t cross the road because it was too tired.”\n\nWhat does “it” refer to?\nA static embedding for “it” cannot capture the difference between “chicken” and “road” as possible referents.\nKey Insight:\nA word’s meaning should change depending on its context!\n\n\n12.5.2 Contextual Embeddings\n\nContextual embeddings assign a different vector to each word in each context.\nThe meaning of “it” in the above sentence depends on the surrounding words.\n\nHow can we compute contextual embeddings?\n→ Attention\n\n\n12.5.3 Attention Mechanism\nIntuition:\nTo build a contextual embedding for a word, we selectively integrate information from all other words in the sentence.\n\nEach word “attends to” other words, weighting them by relevance.\nThe embedding for a word is a weighted sum of the embeddings of all words in the sentence.\n\n\nExample 1:\n“The chicken didn’t cross the road because it was too tired.”\nHere, “it” likely refers to “chicken”.\n\n\nExample 2:\n“The chicken didn’t cross the road because it was too wide.”\nHere, “it” likely refers to “road”.\n\nAt the word “it”, the model uses attention to decide whether “it” refers to “chicken” or “road”, based on context.\nFormally:\nGiven token embeddings:\nx₁, x₂, x₃, …, xₙ\nFor each word:\naᵢ = weighted sum of x₁, x₂, …, xₙ\nWeights are based on similarity to xᵢ (the current word).\nResult:\n- Each word’s embedding is context-dependent. - Attention enables models to capture complex relationships and meanings in language.\n\n\n12.5.4 Position Embeddings\nTransformers have no inherent sense of word order.\nPosition embeddings solve this by assigning each position in the sequence a unique vector.\n\nFor a sequence of length N, learn a position embedding matrix Eₚₒₛ of shape [1 × N].\nEach position (e.g., 1, 2, 3, …) gets its own embedding, learned during training.\nThese are added to the word embeddings so the model knows the order of words.\n\n\nExample:\nThe embedding for “fish” at position 3 is different from “fish” at position 17.\n\n\n\n\nPosition Embeddings\n\n\n\n\n12.5.5 Output: Logits and Softmax\nAfter processing, the model produces a logit (score) for each word in the vocabulary.\n\nLogits: Vector of size [1 × |V|], where |V| is the vocabulary size.\nSoftmax: Converts logits into probabilities over the vocabulary.\n\n\n\n\nFinal Transformer Model\n\n\n\n\n12.5.6 BERT: Bidirectional Encoder Representations from Transformers\n\nBERT is an encoder-only Transformer.\nGoal: Produce a rich vector representation for each token in the input sequence.\nNot a chatbot; best for classification tasks (e.g., sentiment analysis, spam detection, ticket routing).\n\n\nExample:\nGiven a sentence, BERT can classify its sentiment as positive or negative.\n\n\nBERT adds a special [CLS] token to represent the entire sequence (sentence embedding).\nFor specific tasks, BERT is further trained (fine-tuned) with labeled data.\n\n\n\n12.5.7 Decoder-Only Models (e.g., GPT)\n\nGoal: Generate new output sequences from input sequences (e.g., text generation, chatbots).\nUnlike BERT, these models predict the next word in a sequence.\n\n\nExample:\nGiven “The weather is”, GPT might generate “sunny today in Berlin.”\n\nSummary Table: Encoder vs Decoder Transformers\n\n\n\n\n\n\n\n\n\nModel Type\nExample\nMain Use\nSpecial Token\n\n\n\n\nEncoder-only\nBERT\nClassification, embeddings\n[CLS]\n\n\nDecoder-only\nGPT\nText generation\nNone",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Natural Language Processing - Chapter 10</span>"
    ]
  },
  {
    "objectID": "session10.html#large-language-models-llms",
    "href": "session10.html#large-language-models-llms",
    "title": "11  Natural Language Processing - Chapter 10",
    "section": "12.6 Large Language Models (LLMs)",
    "text": "12.6 Large Language Models (LLMs)\nLarge Language Models (LLMs) are advanced neural networks trained to predict the next word in a sequence, enabling them to generate coherent and contextually relevant text.\n\n12.6.1 From N-gram Models to LLMs\n\nN-gram Language Models:\nAssign probabilities to word sequences based on observed counts in large text corpora.\nExample:\nGiven “The cat sat on the”, an n-gram model predicts “mat” if that sequence is common.\nLLMs:\nAlso assign probabilities to word sequences, but learn these probabilities by training on massive datasets and using deep neural networks (Transformers).\nExample:\nGiven “The cat sat on the”, an LLM might generate “mat”, “sofa”, or “floor”, depending on context.\n\n\n\n12.6.2 Why LLMs Are Powerful\n\nEven though LLMs are trained only to predict the next word, they learn a lot about language, facts, and reasoning.\nMany tasks can be reframed as next-word prediction.\n\nExamples:\n\nSentiment Analysis\nPrompt:\n&gt; The sentiment of the sentence “I like Jackie Chan” is\nLLM likely predicts: “positive”\nSummarization\nPrompt:\n&gt; Summarize: “The movie was exciting and full of twists.”\nLLM might generate: “Exciting and unpredictable movie.”",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Natural Language Processing - Chapter 10</span>"
    ]
  },
  {
    "objectID": "session10.html#decoding-and-sampling",
    "href": "session10.html#decoding-and-sampling",
    "title": "11  Natural Language Processing - Chapter 10",
    "section": "12.7 Decoding and Sampling",
    "text": "12.7 Decoding and Sampling\nWhen generating text, LLMs must choose the next word based on predicted probabilities. This process is called decoding.\n\n12.7.1 Random Sampling\n\nThe model samples the next word according to its probability.\nMost of the time, high-probability words are chosen, but occasionally, rare words are picked, which can lead to unexpected or odd sentences.\n\nExample:\nPrompt: “The sky is”\nPossible outputs: “blue”, “clear”, “falling”, “delicious” (the last is odd, but possible with low probability).\n\n\n12.7.2 Balancing Quality and Diversity\n\nEmphasize high-probability words:\n\nMore accurate, coherent, and factual\n– Can be repetitive or boring\n\nEmphasize middle-probability words:\n\nMore creative and diverse\n– May be less factual or coherent\n\n\n\n\n12.7.3 Top-k Sampling\n\nChoose a number k (e.g., 10).\nFor each prediction, keep only the top k most probable words.\nRandomly sample from these k words.\n\nExample:\nIf the top 3 words after “The sky is” are “blue”, “clear”, “cloudy”, only these are considered for sampling.\n\n\n12.7.4 Top-p (Nucleus) Sampling\n\nInstead of a fixed k, keep the smallest set of words whose total probability exceeds a threshold p (e.g., 0.9).\nThis set may be larger or smaller depending on the context.\n\nExample:\nIf “blue” has 0.6, “clear” 0.2, “cloudy” 0.1, “stormy” 0.05, “red” 0.05, then top-p with p=0.9 includes “blue”, “clear”, “cloudy”.\n\n\n12.7.5 Temperature Sampling\n\nAdjusts the “sharpness” of the probability distribution.\nLow temperature (τ &lt; 1):\nMakes the model more confident; high-probability words become even more likely.\nHigh temperature (τ &gt; 1):\nMakes the model more random; low-probability words are more likely to be chosen.\n\nExample:\nWith τ = 0.5, “blue” is much more likely than “cloudy”.\nWith τ = 2.0, “cloudy” or “red” might be chosen more often.\nHow it works:\nDivide the logits (raw scores) by τ before applying softmax.\n- τ = 1: No change\n- τ &lt; 1: Sharper distribution\n- τ &gt; 1: Flatter distribution\nSummary Table: Decoding Methods\n\n\n\n\n\n\n\n\nMethod\nHow it works\nEffect\n\n\n\n\nRandom\nSample from all words by probability\nMost diverse, least accurate\n\n\nTop-k\nSample from top k words\nBalances quality and diversity\n\n\nTop-p\nSample from smallest set covering p prob.\nAdaptive, flexible\n\n\nTemperature\nAdjusts probability sharpness\nControls randomness",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Natural Language Processing - Chapter 10</span>"
    ]
  },
  {
    "objectID": "session10.html#pretraining",
    "href": "session10.html#pretraining",
    "title": "11  Natural Language Processing - Chapter 10",
    "section": "12.8 Pretraining",
    "text": "12.8 Pretraining\nThe big idea behind the success of language models:\n\nPretrain a transformer model on enormous amounts of text.\nFine-tune it for new tasks.\n\n\n12.8.1 Self-Supervised Training\n\nTrain models to predict the next word in a sequence.\nAt each time step t, ask the model to predict the next word.\nUse gradient descent to minimize the prediction error.\n\nWhy “self-supervised”?\nBecause the next word in the text itself serves as the label—no manual annotation needed.\n\n\n12.8.2 Language Model Training: Loss Function\n\nUse cross-entropy loss.\nWe want the model to assign a high probability to the true next word.\nIf the model assigns too low a probability, the loss is high.\nTraining adjusts weights to increase the probability of the correct word.\n\n\n\n12.8.3 Pretraining Data\n\nCommon Crawl: Billions of web pages.\nColossal Clean Crawled Corpus (C4): 156 billion tokens of English, filtered for quality.\n\nIncludes patent documents, Wikipedia, news sites, etc.\n\n\n\n\n12.8.4 What Does a Model Learn from Pretraining?\n\nFactual knowledge:\n\n“The author of ‘A Room of One’s Own’ is Virginia Woolf.”\n“The square root of 4 is 2.”\n\nLanguage patterns:\n\n“There are canines everywhere! One dog in the front room, and two dogs…”\n“It wasn’t just big, it was enormous.”\n\nCommonsense reasoning:\n\n“The doctor told me that he…”\n\n\nText contains enormous amounts of knowledge.\nPretraining on large, diverse text corpora gives language models their remarkable abilities.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Natural Language Processing - Chapter 10</span>"
    ]
  },
  {
    "objectID": "session10.html#working-with-large-language-models",
    "href": "session10.html#working-with-large-language-models",
    "title": "11  Natural Language Processing - Chapter 10",
    "section": "12.9 Working with Large Language Models",
    "text": "12.9 Working with Large Language Models\n\n12.9.1 Model Types\n\nBase Model: Result of pre-training on large text corpora.\nInstruct Model: Base model fine-tuned to follow instructions.\nChat Model: Further tuned for dialogue and conversational tasks.\n\n\n\n12.9.2 Key Settings\n\nTemperature:\nControls randomness in output.\n\nLow temperature (e.g., 0.2): More deterministic, focused responses.\n\nHigh temperature (e.g., 1.0): More random, creative outputs.\n\nTop-p (Nucleus Sampling):\nOnly considers the smallest set of words whose cumulative probability exceeds p (e.g., 0.9).\n\nHigher p: More possible words, more diverse outputs.\n\nMax Length:\nMaximum number of tokens in the generated response.\nStop Sequences:\nSpecify tokens or phrases where generation should stop.\nFrequency Penalty:\nPenalizes repeated words to encourage variety.\nPresence Penalty:\nPenalizes new words to encourage sticking to the prompt context.\n\n\n\n12.9.3 Prompt Structure\nA good prompt often includes:\n\nInstruction: What you want the model to do.\nExample: “Summarize the following text.”\nContext: Extra information to guide the model.\nExample: “The text is a news article.”\nInput Data: The main content or question.\nExample: “The movie was exciting and full of twists.”\nOutput Indicator: Desired format or type of output.\nExample: “Summary:”\n\n\n\n12.9.4 Common Tasks\n\nText Summarization\nInformation Extraction\nQuestion Answering\nText Classification\nConversation\nCode Generation\nReasoning\n\n\n\n12.9.5 Prompting Techniques\n\nZero-shot Prompting:\nAsk the model to perform a task without examples.\nExample:\n&gt; “Translate to French: ‘Good morning.’”\nFew-shot Prompting:\nProvide a few examples to guide the model.\nExample:\n&gt; “Translate to French:\n&gt; English: ‘Good morning.’ → French: ‘Bonjour.’\n&gt; English: ‘How are you?’ → French:”\nChain-of-Thought Prompting:\nEncourage the model to explain its reasoning step by step.\nExample:\n&gt; “If there are 3 apples and you eat 1, how many are left? Let’s think step by step.”",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Natural Language Processing - Chapter 10</span>"
    ]
  },
  {
    "objectID": "session10.html#takeaways",
    "href": "session10.html#takeaways",
    "title": "11  Natural Language Processing - Chapter 10",
    "section": "12.10 Takeaways",
    "text": "12.10 Takeaways\n\nTransformers use attention to create context-aware word representations.\nBERT: Encoder model, best for classification and embedding tasks.\nGPT: Decoder model, best for text generation and chat.\nPrompt engineering is key to getting useful outputs from LLMs.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Natural Language Processing - Chapter 10</span>"
    ]
  }
]