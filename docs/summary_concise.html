<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.42">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Tim Roessling">

<title>13&nbsp; Concepts and Explanations Short – NLP Notes</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./summary.html" rel="next">
<link href="./session11.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-2f5df379a58b258e96c21c0638c20c03.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-8da5b4427184b79ecddefad3d342027e.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./summary_concise.html"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Concepts and Explanations Short</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">NLP Notes</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">NLP Notes</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./session1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Introduction to Natural Language Processing</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./session2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">NLP Libraries and Text Normalization</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./session3.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">N-Gram Language Models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./session4.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Text Classification and Naive Bayes</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./session5.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Logistic Regression and Text Representation</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./session6.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Sentiment Analysis</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./session7.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Topic Modeling</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./session8.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Lexical Semantics and Vector Embeddings</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./session9.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Simple Neural Networks</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./session10.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">The Transformer and Pre-trained Language Models</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./session11.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Question Answering and Information Retrieval</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./summary_concise.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Concepts and Explanations Short</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./summary.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Concepts and Explanations Long</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#chapter-1-foundations-of-nlp" id="toc-chapter-1-foundations-of-nlp" class="nav-link active" data-scroll-target="#chapter-1-foundations-of-nlp"><span class="header-section-number">13.1</span> Chapter 1: Foundations of NLP</a>
  <ul class="collapse">
  <li><a href="#the-revolution-in-nlp" id="toc-the-revolution-in-nlp" class="nav-link" data-scroll-target="#the-revolution-in-nlp"><span class="header-section-number">13.1.1</span> The Revolution in NLP</a></li>
  <li><a href="#language-is-hard" id="toc-language-is-hard" class="nav-link" data-scroll-target="#language-is-hard"><span class="header-section-number">13.1.2</span> Language is Hard</a></li>
  <li><a href="#brief-history" id="toc-brief-history" class="nav-link" data-scroll-target="#brief-history"><span class="header-section-number">13.1.3</span> Brief History</a></li>
  <li><a href="#nlp-the-basic-approach" id="toc-nlp-the-basic-approach" class="nav-link" data-scroll-target="#nlp-the-basic-approach"><span class="header-section-number">13.1.4</span> NLP: The Basic Approach</a></li>
  <li><a href="#supervised-ml-for-text-processing" id="toc-supervised-ml-for-text-processing" class="nav-link" data-scroll-target="#supervised-ml-for-text-processing"><span class="header-section-number">13.1.5</span> Supervised ML for Text Processing</a></li>
  <li><a href="#bag-of-words-limitation" id="toc-bag-of-words-limitation" class="nav-link" data-scroll-target="#bag-of-words-limitation"><span class="header-section-number">13.1.6</span> Bag-of-Words Limitation</a></li>
  <li><a href="#language-modeling-with-n-grams" id="toc-language-modeling-with-n-grams" class="nav-link" data-scroll-target="#language-modeling-with-n-grams"><span class="header-section-number">13.1.7</span> Language Modeling with N-grams</a></li>
  <li><a href="#training-an-llm" id="toc-training-an-llm" class="nav-link" data-scroll-target="#training-an-llm"><span class="header-section-number">13.1.8</span> Training an LLM</a></li>
  <li><a href="#probing-gpt" id="toc-probing-gpt" class="nav-link" data-scroll-target="#probing-gpt"><span class="header-section-number">13.1.9</span> Probing GPT</a></li>
  <li><a href="#ai-where-are-we-heading" id="toc-ai-where-are-we-heading" class="nav-link" data-scroll-target="#ai-where-are-we-heading"><span class="header-section-number">13.1.10</span> AI: Where Are We Heading?</a></li>
  <li><a href="#applications-of-llms" id="toc-applications-of-llms" class="nav-link" data-scroll-target="#applications-of-llms"><span class="header-section-number">13.1.11</span> Applications of LLMs</a></li>
  <li><a href="#takeaways" id="toc-takeaways" class="nav-link" data-scroll-target="#takeaways"><span class="header-section-number">13.1.12</span> Takeaways</a></li>
  </ul></li>
  <li><a href="#chapter-2-core-nlp-tools-and-preprocessing" id="toc-chapter-2-core-nlp-tools-and-preprocessing" class="nav-link" data-scroll-target="#chapter-2-core-nlp-tools-and-preprocessing"><span class="header-section-number">13.2</span> Chapter 2: Core NLP Tools and Preprocessing</a>
  <ul class="collapse">
  <li><a href="#nlp-libraries" id="toc-nlp-libraries" class="nav-link" data-scroll-target="#nlp-libraries"><span class="header-section-number">13.2.1</span> NLP Libraries</a></li>
  <li><a href="#normalization" id="toc-normalization" class="nav-link" data-scroll-target="#normalization"><span class="header-section-number">13.2.2</span> Normalization</a></li>
  <li><a href="#morphological-normalization" id="toc-morphological-normalization" class="nav-link" data-scroll-target="#morphological-normalization"><span class="header-section-number">13.2.3</span> Morphological Normalization</a></li>
  <li><a href="#tokenization" id="toc-tokenization" class="nav-link" data-scroll-target="#tokenization"><span class="header-section-number">13.2.4</span> Tokenization</a></li>
  <li><a href="#regular-expressions" id="toc-regular-expressions" class="nav-link" data-scroll-target="#regular-expressions"><span class="header-section-number">13.2.5</span> Regular Expressions</a></li>
  <li><a href="#pos-tagging" id="toc-pos-tagging" class="nav-link" data-scroll-target="#pos-tagging"><span class="header-section-number">13.2.6</span> POS Tagging</a></li>
  <li><a href="#named-entity-recognition-ner" id="toc-named-entity-recognition-ner" class="nav-link" data-scroll-target="#named-entity-recognition-ner"><span class="header-section-number">13.2.7</span> Named Entity Recognition (NER)</a></li>
  </ul></li>
  <li><a href="#chapter-3-language-modeling-with-n-grams" id="toc-chapter-3-language-modeling-with-n-grams" class="nav-link" data-scroll-target="#chapter-3-language-modeling-with-n-grams"><span class="header-section-number">13.3</span> Chapter 3: Language Modeling with N-grams</a>
  <ul class="collapse">
  <li><a href="#n-gram-language-models" id="toc-n-gram-language-models" class="nav-link" data-scroll-target="#n-gram-language-models"><span class="header-section-number">13.3.1</span> N-Gram Language Models</a></li>
  <li><a href="#conditional-probability-and-chain-rule" id="toc-conditional-probability-and-chain-rule" class="nav-link" data-scroll-target="#conditional-probability-and-chain-rule"><span class="header-section-number">13.3.2</span> Conditional Probability and Chain Rule</a></li>
  <li><a href="#markov-assumption" id="toc-markov-assumption" class="nav-link" data-scroll-target="#markov-assumption"><span class="header-section-number">13.3.3</span> Markov Assumption</a></li>
  <li><a href="#n-gram-probability-calculation-mle" id="toc-n-gram-probability-calculation-mle" class="nav-link" data-scroll-target="#n-gram-probability-calculation-mle"><span class="header-section-number">13.3.4</span> N-Gram Probability Calculation &amp; MLE</a></li>
  <li><a href="#padding" id="toc-padding" class="nav-link" data-scroll-target="#padding"><span class="header-section-number">13.3.5</span> Padding</a></li>
  <li><a href="#underflow" id="toc-underflow" class="nav-link" data-scroll-target="#underflow"><span class="header-section-number">13.3.6</span> Underflow</a></li>
  <li><a href="#smoothing-techniques" id="toc-smoothing-techniques" class="nav-link" data-scroll-target="#smoothing-techniques"><span class="header-section-number">13.3.7</span> Smoothing Techniques</a></li>
  <li><a href="#perplexity" id="toc-perplexity" class="nav-link" data-scroll-target="#perplexity"><span class="header-section-number">13.3.8</span> Perplexity</a></li>
  <li><a href="#practical-implementation" id="toc-practical-implementation" class="nav-link" data-scroll-target="#practical-implementation"><span class="header-section-number">13.3.9</span> Practical Implementation</a></li>
  </ul></li>
  <li><a href="#chapter-4-text-classification" id="toc-chapter-4-text-classification" class="nav-link" data-scroll-target="#chapter-4-text-classification"><span class="header-section-number">13.4</span> Chapter 4: Text Classification</a>
  <ul class="collapse">
  <li><a href="#text-classification" id="toc-text-classification" class="nav-link" data-scroll-target="#text-classification"><span class="header-section-number">13.4.1</span> Text Classification</a></li>
  <li><a href="#types-of-text-classification-techniques" id="toc-types-of-text-classification-techniques" class="nav-link" data-scroll-target="#types-of-text-classification-techniques"><span class="header-section-number">13.4.2</span> Types of Text Classification Techniques</a></li>
  <li><a href="#bayes-theorem" id="toc-bayes-theorem" class="nav-link" data-scroll-target="#bayes-theorem"><span class="header-section-number">13.4.3</span> Bayes Theorem</a></li>
  <li><a href="#types-of-naive-bayes-classifiers" id="toc-types-of-naive-bayes-classifiers" class="nav-link" data-scroll-target="#types-of-naive-bayes-classifiers"><span class="header-section-number">13.4.4</span> Types of Naive Bayes Classifiers</a></li>
  <li><a href="#implementing-naive-bayes" id="toc-implementing-naive-bayes" class="nav-link" data-scroll-target="#implementing-naive-bayes"><span class="header-section-number">13.4.5</span> Implementing Naive Bayes</a></li>
  <li><a href="#evaluation-metrics" id="toc-evaluation-metrics" class="nav-link" data-scroll-target="#evaluation-metrics"><span class="header-section-number">13.4.6</span> Evaluation Metrics</a></li>
  <li><a href="#disadvantages-of-naive-bayes" id="toc-disadvantages-of-naive-bayes" class="nav-link" data-scroll-target="#disadvantages-of-naive-bayes"><span class="header-section-number">13.4.7</span> Disadvantages of Naive Bayes</a></li>
  <li><a href="#handling-class-imbalance" id="toc-handling-class-imbalance" class="nav-link" data-scroll-target="#handling-class-imbalance"><span class="header-section-number">13.4.8</span> Handling Class Imbalance</a></li>
  </ul></li>
  <li><a href="#chapter-5-logistic-regression" id="toc-chapter-5-logistic-regression" class="nav-link" data-scroll-target="#chapter-5-logistic-regression"><span class="header-section-number">13.5</span> Chapter 5: Logistic Regression</a>
  <ul class="collapse">
  <li><a href="#logistic-regression" id="toc-logistic-regression" class="nav-link" data-scroll-target="#logistic-regression"><span class="header-section-number">13.5.1</span> Logistic Regression</a></li>
  <li><a href="#log-odds-logit" id="toc-log-odds-logit" class="nav-link" data-scroll-target="#log-odds-logit"><span class="header-section-number">13.5.2</span> Log-Odds (Logit)</a></li>
  <li><a href="#training-logistic-regression" id="toc-training-logistic-regression" class="nav-link" data-scroll-target="#training-logistic-regression"><span class="header-section-number">13.5.3</span> Training Logistic Regression</a></li>
  <li><a href="#bag-of-words-vs.-tf-idf" id="toc-bag-of-words-vs.-tf-idf" class="nav-link" data-scroll-target="#bag-of-words-vs.-tf-idf"><span class="header-section-number">13.5.4</span> Bag of Words vs.&nbsp;TF-IDF</a></li>
  <li><a href="#lr-model-assumptions" id="toc-lr-model-assumptions" class="nav-link" data-scroll-target="#lr-model-assumptions"><span class="header-section-number">13.5.5</span> LR Model Assumptions</a></li>
  <li><a href="#lr-limitations" id="toc-lr-limitations" class="nav-link" data-scroll-target="#lr-limitations"><span class="header-section-number">13.5.6</span> LR Limitations</a></li>
  </ul></li>
  <li><a href="#chapter-6-sentiment-analysis" id="toc-chapter-6-sentiment-analysis" class="nav-link" data-scroll-target="#chapter-6-sentiment-analysis"><span class="header-section-number">13.6</span> Chapter 6: Sentiment Analysis</a>
  <ul class="collapse">
  <li><a href="#sentiment-analysis-sa" id="toc-sentiment-analysis-sa" class="nav-link" data-scroll-target="#sentiment-analysis-sa"><span class="header-section-number">13.6.1</span> Sentiment Analysis (SA)</a></li>
  <li><a href="#resources-for-sentiment-lexicons" id="toc-resources-for-sentiment-lexicons" class="nav-link" data-scroll-target="#resources-for-sentiment-lexicons"><span class="header-section-number">13.6.2</span> Resources for Sentiment Lexicons</a></li>
  <li><a href="#sentiment-analysis-approaches" id="toc-sentiment-analysis-approaches" class="nav-link" data-scroll-target="#sentiment-analysis-approaches"><span class="header-section-number">13.6.3</span> Sentiment Analysis Approaches</a></li>
  <li><a href="#model-comparison" id="toc-model-comparison" class="nav-link" data-scroll-target="#model-comparison"><span class="header-section-number">13.6.4</span> Model Comparison</a></li>
  <li><a href="#python-libraries-and-tools-for-sa" id="toc-python-libraries-and-tools-for-sa" class="nav-link" data-scroll-target="#python-libraries-and-tools-for-sa"><span class="header-section-number">13.6.5</span> Python Libraries and Tools for SA</a></li>
  <li><a href="#sa-challenges" id="toc-sa-challenges" class="nav-link" data-scroll-target="#sa-challenges"><span class="header-section-number">13.6.6</span> SA Challenges</a></li>
  </ul></li>
  <li><a href="#chapter-7-topic-modeling" id="toc-chapter-7-topic-modeling" class="nav-link" data-scroll-target="#chapter-7-topic-modeling"><span class="header-section-number">13.7</span> Chapter 7: Topic Modeling</a>
  <ul class="collapse">
  <li><a href="#topic-modeling" id="toc-topic-modeling" class="nav-link" data-scroll-target="#topic-modeling"><span class="header-section-number">13.7.1</span> Topic Modeling</a></li>
  <li><a href="#latent-dirichlet-allocation-lda" id="toc-latent-dirichlet-allocation-lda" class="nav-link" data-scroll-target="#latent-dirichlet-allocation-lda"><span class="header-section-number">13.7.2</span> Latent Dirichlet Allocation (LDA)</a></li>
  <li><a href="#lda-parameters-and-trade-offs" id="toc-lda-parameters-and-trade-offs" class="nav-link" data-scroll-target="#lda-parameters-and-trade-offs"><span class="header-section-number">13.7.3</span> LDA Parameters and Trade-offs</a></li>
  <li><a href="#practical-considerations-for-lda" id="toc-practical-considerations-for-lda" class="nav-link" data-scroll-target="#practical-considerations-for-lda"><span class="header-section-number">13.7.4</span> Practical Considerations for LDA</a></li>
  <li><a href="#top2vec" id="toc-top2vec" class="nav-link" data-scroll-target="#top2vec"><span class="header-section-number">13.7.5</span> Top2Vec</a></li>
  <li><a href="#bertopic" id="toc-bertopic" class="nav-link" data-scroll-target="#bertopic"><span class="header-section-number">13.7.6</span> BERTopic</a></li>
  <li><a href="#k-means-clustering-with-word2vec" id="toc-k-means-clustering-with-word2vec" class="nav-link" data-scroll-target="#k-means-clustering-with-word2vec"><span class="header-section-number">13.7.7</span> K-means Clustering with Word2Vec</a></li>
  </ul></li>
  <li><a href="#chapter-8-word-meanings-and-vector-semantics" id="toc-chapter-8-word-meanings-and-vector-semantics" class="nav-link" data-scroll-target="#chapter-8-word-meanings-and-vector-semantics"><span class="header-section-number">13.8</span> Chapter 8: Word Meanings and Vector Semantics</a>
  <ul class="collapse">
  <li><a href="#word-meanings" id="toc-word-meanings" class="nav-link" data-scroll-target="#word-meanings"><span class="header-section-number">13.8.1</span> Word Meanings</a></li>
  <li><a href="#word-relations" id="toc-word-relations" class="nav-link" data-scroll-target="#word-relations"><span class="header-section-number">13.8.2</span> Word Relations</a></li>
  <li><a href="#vector-semantics" id="toc-vector-semantics" class="nav-link" data-scroll-target="#vector-semantics"><span class="header-section-number">13.8.3</span> Vector Semantics</a></li>
  <li><a href="#word2vec" id="toc-word2vec" class="nav-link" data-scroll-target="#word2vec"><span class="header-section-number">13.8.4</span> Word2Vec</a></li>
  </ul></li>
  <li><a href="#chapter-9-neural-networks-in-nlp" id="toc-chapter-9-neural-networks-in-nlp" class="nav-link" data-scroll-target="#chapter-9-neural-networks-in-nlp"><span class="header-section-number">13.9</span> Chapter 9: Neural Networks in NLP</a>
  <ul class="collapse">
  <li><a href="#ambiguity-in-language" id="toc-ambiguity-in-language" class="nav-link" data-scroll-target="#ambiguity-in-language"><span class="header-section-number">13.9.1</span> Ambiguity in Language</a></li>
  <li><a href="#simple-neural-networks" id="toc-simple-neural-networks" class="nav-link" data-scroll-target="#simple-neural-networks"><span class="header-section-number">13.9.2</span> Simple Neural Networks</a></li>
  <li><a href="#activation-functions" id="toc-activation-functions" class="nav-link" data-scroll-target="#activation-functions"><span class="header-section-number">13.9.3</span> Activation Functions</a></li>
  <li><a href="#multi-layer-networks-and-xor" id="toc-multi-layer-networks-and-xor" class="nav-link" data-scroll-target="#multi-layer-networks-and-xor"><span class="header-section-number">13.9.4</span> Multi-Layer Networks and XOR</a></li>
  <li><a href="#feedforward-neural-networks" id="toc-feedforward-neural-networks" class="nav-link" data-scroll-target="#feedforward-neural-networks"><span class="header-section-number">13.9.5</span> Feedforward Neural Networks</a></li>
  <li><a href="#applying-to-nlp" id="toc-applying-to-nlp" class="nav-link" data-scroll-target="#applying-to-nlp"><span class="header-section-number">13.9.6</span> Applying to NLP</a></li>
  <li><a href="#training-neural-networks" id="toc-training-neural-networks" class="nav-link" data-scroll-target="#training-neural-networks"><span class="header-section-number">13.9.7</span> Training Neural Networks</a></li>
  </ul></li>
  <li><a href="#chapter-10-transformers-and-large-language-models" id="toc-chapter-10-transformers-and-large-language-models" class="nav-link" data-scroll-target="#chapter-10-transformers-and-large-language-models"><span class="header-section-number">13.10</span> Chapter 10: Transformers and Large Language Models</a>
  <ul class="collapse">
  <li><a href="#key-innovations" id="toc-key-innovations" class="nav-link" data-scroll-target="#key-innovations"><span class="header-section-number">13.10.1</span> Key Innovations</a></li>
  <li><a href="#transformers" id="toc-transformers" class="nav-link" data-scroll-target="#transformers"><span class="header-section-number">13.10.2</span> Transformers</a></li>
  <li><a href="#bert-and-decoder-only-models" id="toc-bert-and-decoder-only-models" class="nav-link" data-scroll-target="#bert-and-decoder-only-models"><span class="header-section-number">13.10.3</span> BERT and Decoder-Only Models</a></li>
  <li><a href="#large-language-models-llms" id="toc-large-language-models-llms" class="nav-link" data-scroll-target="#large-language-models-llms"><span class="header-section-number">13.10.4</span> Large Language Models (LLMs)</a></li>
  <li><a href="#pretraining-and-self-supervised-learning" id="toc-pretraining-and-self-supervised-learning" class="nav-link" data-scroll-target="#pretraining-and-self-supervised-learning"><span class="header-section-number">13.10.5</span> Pretraining and Self-Supervised Learning</a></li>
  <li><a href="#working-with-llms" id="toc-working-with-llms" class="nav-link" data-scroll-target="#working-with-llms"><span class="header-section-number">13.10.6</span> Working with LLMs</a></li>
  </ul></li>
  <li><a href="#chapter-11-question-answering-and-information-retrieval" id="toc-chapter-11-question-answering-and-information-retrieval" class="nav-link" data-scroll-target="#chapter-11-question-answering-and-information-retrieval"><span class="header-section-number">13.11</span> Chapter 11: Question Answering and Information Retrieval</a>
  <ul class="collapse">
  <li><a href="#information-retrieval-ir" id="toc-information-retrieval-ir" class="nav-link" data-scroll-target="#information-retrieval-ir"><span class="header-section-number">13.11.1</span> Information Retrieval (IR)</a></li>
  <li><a href="#vector-space-model-tf-idf" id="toc-vector-space-model-tf-idf" class="nav-link" data-scroll-target="#vector-space-model-tf-idf"><span class="header-section-number">13.11.2</span> Vector Space Model &amp; TF-IDF</a></li>
  <li><a href="#dense-retrieval-with-neural-models" id="toc-dense-retrieval-with-neural-models" class="nav-link" data-scroll-target="#dense-retrieval-with-neural-models"><span class="header-section-number">13.11.3</span> Dense Retrieval with Neural Models</a></li>
  <li><a href="#question-answering-datasets" id="toc-question-answering-datasets" class="nav-link" data-scroll-target="#question-answering-datasets"><span class="header-section-number">13.11.4</span> Question Answering Datasets</a></li>
  <li><a href="#extractive-question-answering" id="toc-extractive-question-answering" class="nav-link" data-scroll-target="#extractive-question-answering"><span class="header-section-number">13.11.5</span> Extractive Question Answering</a></li>
  <li><a href="#entity-linking-wikification" id="toc-entity-linking-wikification" class="nav-link" data-scroll-target="#entity-linking-wikification"><span class="header-section-number">13.11.6</span> Entity Linking (Wikification)</a></li>
  <li><a href="#knowledge-based-question-answering-kbqa" id="toc-knowledge-based-question-answering-kbqa" class="nav-link" data-scroll-target="#knowledge-based-question-answering-kbqa"><span class="header-section-number">13.11.7</span> Knowledge-based Question Answering (KBQA)</a></li>
  <li><a href="#supervision-for-semantic-parsing" id="toc-supervision-for-semantic-parsing" class="nav-link" data-scroll-target="#supervision-for-semantic-parsing"><span class="header-section-number">13.11.8</span> Supervision for Semantic Parsing</a></li>
  <li><a href="#retrieve-and-generate-rag" id="toc-retrieve-and-generate-rag" class="nav-link" data-scroll-target="#retrieve-and-generate-rag"><span class="header-section-number">13.11.9</span> Retrieve-and-Generate (RAG)</a></li>
  <li><a href="#evaluation-of-rag-systems" id="toc-evaluation-of-rag-systems" class="nav-link" data-scroll-target="#evaluation-of-rag-systems"><span class="header-section-number">13.11.10</span> Evaluation of RAG Systems</a></li>
  <li><a href="#agents" id="toc-agents" class="nav-link" data-scroll-target="#agents"><span class="header-section-number">13.11.11</span> Agents</a></li>
  <li><a href="#formal-vs.-functional-abilities-of-llms" id="toc-formal-vs.-functional-abilities-of-llms" class="nav-link" data-scroll-target="#formal-vs.-functional-abilities-of-llms"><span class="header-section-number">13.11.12</span> Formal vs.&nbsp;Functional Abilities of LLMs</a></li>
  <li><a href="#takeaways-1" id="toc-takeaways-1" class="nav-link" data-scroll-target="#takeaways-1"><span class="header-section-number">13.11.13</span> Takeaways</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Concepts and Explanations Short</span></h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Tim Roessling </p>
          </div>
  </div>
    
  
    
  </div>
  


</header>


<section id="chapter-1-foundations-of-nlp" class="level2" data-number="13.1">
<h2 data-number="13.1" class="anchored" data-anchor-id="chapter-1-foundations-of-nlp"><span class="header-section-number">13.1</span> Chapter 1: Foundations of NLP</h2>
<section id="the-revolution-in-nlp" class="level3" data-number="13.1.1">
<h3 data-number="13.1.1" class="anchored" data-anchor-id="the-revolution-in-nlp"><span class="header-section-number">13.1.1</span> The Revolution in NLP</h3>
<ul>
<li>Transformer models have enabled NLP systems to learn from massive datasets, using objectives like masked language modeling.</li>
<li>RLHF (Reinforcement Learning from Human Feedback) further aligns models with human preferences.</li>
</ul>
</section>
<section id="language-is-hard" class="level3" data-number="13.1.2">
<h3 data-number="13.1.2" class="anchored" data-anchor-id="language-is-hard"><span class="header-section-number">13.1.2</span> Language is Hard</h3>
<ul>
<li>Language is infinitely expressive and ambiguous, making it difficult for machines to achieve true understanding.</li>
<li>Even advanced models may not fully replicate human-like comprehension.</li>
</ul>
</section>
<section id="brief-history" class="level3" data-number="13.1.3">
<h3 data-number="13.1.3" class="anchored" data-anchor-id="brief-history"><span class="header-section-number">13.1.3</span> Brief History</h3>
<ul>
<li>Descartes doubted machines could imitate humans; Turing proposed the Turing Test to evaluate machine intelligence via conversation.</li>
<li>Passing the Turing Test means a machine’s responses are indistinguishable from a human’s.</li>
</ul>
</section>
<section id="nlp-the-basic-approach" class="level3" data-number="13.1.4">
<h3 data-number="13.1.4" class="anchored" data-anchor-id="nlp-the-basic-approach"><span class="header-section-number">13.1.4</span> NLP: The Basic Approach</h3>
<ul>
<li>Text data is unstructured and context-dependent, unlike numerical data.</li>
<li>Bag-of-words (BoW) representations structure text but ignore word order and context.</li>
</ul>
</section>
<section id="supervised-ml-for-text-processing" class="level3" data-number="13.1.5">
<h3 data-number="13.1.5" class="anchored" data-anchor-id="supervised-ml-for-text-processing"><span class="header-section-number">13.1.5</span> Supervised ML for Text Processing</h3>
<ul>
<li>Labeled text enables tasks like spam detection and sentiment analysis.</li>
<li>Raises questions about whether these approaches capture real language understanding.</li>
</ul>
</section>
<section id="bag-of-words-limitation" class="level3" data-number="13.1.6">
<h3 data-number="13.1.6" class="anchored" data-anchor-id="bag-of-words-limitation"><span class="header-section-number">13.1.6</span> Bag-of-Words Limitation</h3>
<ul>
<li>BoW ignores word order, so it cannot distinguish between sentences with the same words in different orders.</li>
</ul>
</section>
<section id="language-modeling-with-n-grams" class="level3" data-number="13.1.7">
<h3 data-number="13.1.7" class="anchored" data-anchor-id="language-modeling-with-n-grams"><span class="header-section-number">13.1.7</span> Language Modeling with N-grams</h3>
<ul>
<li>N-gram models assign probabilities to word sequences based on the Markov assumption (dependence on previous n-1 words).</li>
<li>They capture local word order but struggle with long-range dependencies and rare phrases.</li>
</ul>
</section>
<section id="training-an-llm" class="level3" data-number="13.1.8">
<h3 data-number="13.1.8" class="anchored" data-anchor-id="training-an-llm"><span class="header-section-number">13.1.8</span> Training an LLM</h3>
<ul>
<li>LLMs are trained to predict the next word using neural networks and the softmax function.</li>
<li>Training involves computing loss and updating model weights to improve predictions.</li>
</ul>
</section>
<section id="probing-gpt" class="level3" data-number="13.1.9">
<h3 data-number="13.1.9" class="anchored" data-anchor-id="probing-gpt"><span class="header-section-number">13.1.9</span> Probing GPT</h3>
<ul>
<li>LLMs generate coherent, grammatical text and show some understanding of linguistic structure.</li>
<li>They challenge previous claims about the impossibility of learning abstract linguistic knowledge from input alone.</li>
</ul>
</section>
<section id="ai-where-are-we-heading" class="level3" data-number="13.1.10">
<h3 data-number="13.1.10" class="anchored" data-anchor-id="ai-where-are-we-heading"><span class="header-section-number">13.1.10</span> AI: Where Are We Heading?</h3>
<ul>
<li>AGI aims for machines to match or surpass human cognitive abilities across tasks.</li>
<li>Benchmarks include passing exams, performing jobs, and completing complex real-world tasks.</li>
</ul>
</section>
<section id="applications-of-llms" class="level3" data-number="13.1.11">
<h3 data-number="13.1.11" class="anchored" data-anchor-id="applications-of-llms"><span class="header-section-number">13.1.11</span> Applications of LLMs</h3>
<ul>
<li>LLMs are used in customer support, research portals, automated help desks, and accessibility tools.</li>
</ul>
</section>
<section id="takeaways" class="level3" data-number="13.1.12">
<h3 data-number="13.1.12" class="anchored" data-anchor-id="takeaways"><span class="header-section-number">13.1.12</span> Takeaways</h3>
<ul>
<li>Language is inherently difficult for machines due to its infinite and ambiguous nature.</li>
<li>LLMs are rapidly advancing toward human-level language ability.</li>
</ul>
<hr>
</section>
</section>
<section id="chapter-2-core-nlp-tools-and-preprocessing" class="level2" data-number="13.2">
<h2 data-number="13.2" class="anchored" data-anchor-id="chapter-2-core-nlp-tools-and-preprocessing"><span class="header-section-number">13.2</span> Chapter 2: Core NLP Tools and Preprocessing</h2>
<section id="nlp-libraries" class="level3" data-number="13.2.1">
<h3 data-number="13.2.1" class="anchored" data-anchor-id="nlp-libraries"><span class="header-section-number">13.2.1</span> NLP Libraries</h3>
<ul>
<li><strong>NLTK</strong>: Flexible and feature-rich, but slower; supports POS tagging, NER, and more.</li>
<li><strong>spaCy</strong>: Fast, deep learning-based, suitable for production.</li>
<li><strong>TextBlob</strong>: Simple API for basic NLP tasks, not suitable for large-scale production.</li>
</ul>
</section>
<section id="normalization" class="level3" data-number="13.2.2">
<h3 data-number="13.2.2" class="anchored" data-anchor-id="normalization"><span class="header-section-number">13.2.2</span> Normalization</h3>
<ul>
<li>Standardizes text (lowercasing, removing punctuation/stopwords, extra spaces, special characters) to reduce noise.</li>
</ul>
</section>
<section id="morphological-normalization" class="level3" data-number="13.2.3">
<h3 data-number="13.2.3" class="anchored" data-anchor-id="morphological-normalization"><span class="header-section-number">13.2.3</span> Morphological Normalization</h3>
<ul>
<li><strong>Stemming</strong>: Reduces words to their root form by removing suffixes (fast, less accurate).</li>
<li><strong>Lemmatization</strong>: Maps words to their dictionary form using vocabulary and POS information (more accurate, slower).</li>
</ul>
</section>
<section id="tokenization" class="level3" data-number="13.2.4">
<h3 data-number="13.2.4" class="anchored" data-anchor-id="tokenization"><span class="header-section-number">13.2.4</span> Tokenization</h3>
<ul>
<li>Splits text into tokens (words, sentences) for further processing.</li>
<li>Tokens are used for stemming, lemmatization, and POS tagging.</li>
</ul>
</section>
<section id="regular-expressions" class="level3" data-number="13.2.5">
<h3 data-number="13.2.5" class="anchored" data-anchor-id="regular-expressions"><span class="header-section-number">13.2.5</span> Regular Expressions</h3>
<ul>
<li>Used for pattern matching and text manipulation (searching, extracting, replacing).</li>
<li>Python’s <code>re</code> module provides methods for regex operations.</li>
</ul>
</section>
<section id="pos-tagging" class="level3" data-number="13.2.6">
<h3 data-number="13.2.6" class="anchored" data-anchor-id="pos-tagging"><span class="header-section-number">13.2.6</span> POS Tagging</h3>
<ul>
<li>Assigns grammatical categories (noun, verb, adjective, etc.) to each word.</li>
<li>Methods: rule-based and statistical (machine learning).</li>
</ul>
</section>
<section id="named-entity-recognition-ner" class="level3" data-number="13.2.7">
<h3 data-number="13.2.7" class="anchored" data-anchor-id="named-entity-recognition-ner"><span class="header-section-number">13.2.7</span> Named Entity Recognition (NER)</h3>
<ul>
<li>Identifies and classifies named entities (persons, organizations, locations, dates, etc.) in text.</li>
<li>Crucial for extracting structured information from unstructured text.</li>
</ul>
<hr>
</section>
</section>
<section id="chapter-3-language-modeling-with-n-grams" class="level2" data-number="13.3">
<h2 data-number="13.3" class="anchored" data-anchor-id="chapter-3-language-modeling-with-n-grams"><span class="header-section-number">13.3</span> Chapter 3: Language Modeling with N-grams</h2>
<section id="n-gram-language-models" class="level3" data-number="13.3.1">
<h3 data-number="13.3.1" class="anchored" data-anchor-id="n-gram-language-models"><span class="header-section-number">13.3.1</span> N-Gram Language Models</h3>
<ul>
<li>Predict the probability of a word based on the previous n-1 words.</li>
<li>Types: unigram, bigram, trigram.</li>
</ul>
</section>
<section id="conditional-probability-and-chain-rule" class="level3" data-number="13.3.2">
<h3 data-number="13.3.2" class="anchored" data-anchor-id="conditional-probability-and-chain-rule"><span class="header-section-number">13.3.2</span> Conditional Probability and Chain Rule</h3>
<ul>
<li>Joint probability measures the likelihood of multiple events together; conditional probability measures one event given another.</li>
<li>The chain rule allows calculation of a word sequence’s probability by multiplying conditional probabilities.</li>
</ul>
</section>
<section id="markov-assumption" class="level3" data-number="13.3.3">
<h3 data-number="13.3.3" class="anchored" data-anchor-id="markov-assumption"><span class="header-section-number">13.3.3</span> Markov Assumption</h3>
<ul>
<li>Simplifies modeling by assuming a word depends only on the previous n-1 words.</li>
</ul>
</section>
<section id="n-gram-probability-calculation-mle" class="level3" data-number="13.3.4">
<h3 data-number="13.3.4" class="anchored" data-anchor-id="n-gram-probability-calculation-mle"><span class="header-section-number">13.3.4</span> N-Gram Probability Calculation &amp; MLE</h3>
<ul>
<li>Probability is estimated by counting n-gram occurrences and dividing by the count of the previous (n-1)-gram.</li>
<li>MLE can assign zero probability to unseen n-grams.</li>
</ul>
</section>
<section id="padding" class="level3" data-number="13.3.5">
<h3 data-number="13.3.5" class="anchored" data-anchor-id="padding"><span class="header-section-number">13.3.5</span> Padding</h3>
<ul>
<li>Uses special tokens at sentence boundaries to handle missing context.</li>
</ul>
</section>
<section id="underflow" class="level3" data-number="13.3.6">
<h3 data-number="13.3.6" class="anchored" data-anchor-id="underflow"><span class="header-section-number">13.3.6</span> Underflow</h3>
<ul>
<li>Multiplying many small probabilities can cause underflow; log probabilities convert multiplication into addition.</li>
</ul>
</section>
<section id="smoothing-techniques" class="level3" data-number="13.3.7">
<h3 data-number="13.3.7" class="anchored" data-anchor-id="smoothing-techniques"><span class="header-section-number">13.3.7</span> Smoothing Techniques</h3>
<ul>
<li>Assign small non-zero probabilities to unseen n-grams (Laplace, add-k, Good-Turing, backoff/interpolation, Kneser-Ney).</li>
</ul>
</section>
<section id="perplexity" class="level3" data-number="13.3.8">
<h3 data-number="13.3.8" class="anchored" data-anchor-id="perplexity"><span class="header-section-number">13.3.8</span> Perplexity</h3>
<ul>
<li>Measures how well a model predicts text; lower perplexity indicates a better model.</li>
</ul>
</section>
<section id="practical-implementation" class="level3" data-number="13.3.9">
<h3 data-number="13.3.9" class="anchored" data-anchor-id="practical-implementation"><span class="header-section-number">13.3.9</span> Practical Implementation</h3>
<ul>
<li>Steps: tokenize, pad, generate n-grams, count, calculate probabilities, evaluate with perplexity.</li>
</ul>
<hr>
</section>
</section>
<section id="chapter-4-text-classification" class="level2" data-number="13.4">
<h2 data-number="13.4" class="anchored" data-anchor-id="chapter-4-text-classification"><span class="header-section-number">13.4</span> Chapter 4: Text Classification</h2>
<section id="text-classification" class="level3" data-number="13.4.1">
<h3 data-number="13.4.1" class="anchored" data-anchor-id="text-classification"><span class="header-section-number">13.4.1</span> Text Classification</h3>
<ul>
<li>Assigns predefined categories to text using supervised, unsupervised, or deep learning methods.</li>
<li>Used in spam detection, sentiment analysis, and topic categorization.</li>
</ul>
</section>
<section id="types-of-text-classification-techniques" class="level3" data-number="13.4.2">
<h3 data-number="13.4.2" class="anchored" data-anchor-id="types-of-text-classification-techniques"><span class="header-section-number">13.4.2</span> Types of Text Classification Techniques</h3>
<ul>
<li><strong>Supervised Learning</strong>: Uses labeled data (e.g., Naive Bayes, Logistic Regression).</li>
<li><strong>Unsupervised Learning</strong>: Finds patterns in unlabeled data (e.g., LDA, K-means).</li>
<li><strong>Deep Learning</strong>: Uses neural networks (e.g., CNNs, RNNs, Transformers).</li>
</ul>
</section>
<section id="bayes-theorem" class="level3" data-number="13.4.3">
<h3 data-number="13.4.3" class="anchored" data-anchor-id="bayes-theorem"><span class="header-section-number">13.4.3</span> Bayes Theorem</h3>
<ul>
<li>Relates conditional probabilities and is used to update the probability of a hypothesis given new evidence.</li>
<li>Naive Bayes assumes features are conditionally independent given the class label.</li>
</ul>
</section>
<section id="types-of-naive-bayes-classifiers" class="level3" data-number="13.4.4">
<h3 data-number="13.4.4" class="anchored" data-anchor-id="types-of-naive-bayes-classifiers"><span class="header-section-number">13.4.4</span> Types of Naive Bayes Classifiers</h3>
<ul>
<li><strong>Gaussian</strong>: For continuous features.</li>
<li><strong>Multinomial</strong>: For discrete features like word counts.</li>
<li><strong>Bernoulli</strong>: For binary features.</li>
</ul>
</section>
<section id="implementing-naive-bayes" class="level3" data-number="13.4.5">
<h3 data-number="13.4.5" class="anchored" data-anchor-id="implementing-naive-bayes"><span class="header-section-number">13.4.5</span> Implementing Naive Bayes</h3>
<ul>
<li>Steps: load and preprocess data, split into train/test, extract features, train classifier, evaluate.</li>
</ul>
</section>
<section id="evaluation-metrics" class="level3" data-number="13.4.6">
<h3 data-number="13.4.6" class="anchored" data-anchor-id="evaluation-metrics"><span class="header-section-number">13.4.6</span> Evaluation Metrics</h3>
<ul>
<li><strong>Accuracy</strong>: Correct predictions / total predictions.</li>
<li><strong>Precision</strong>: True positives / predicted positives.</li>
<li><strong>Recall</strong>: True positives / actual positives.</li>
<li><strong>F1-Score</strong>: Harmonic mean of precision and recall.</li>
</ul>
</section>
<section id="disadvantages-of-naive-bayes" class="level3" data-number="13.4.7">
<h3 data-number="13.4.7" class="anchored" data-anchor-id="disadvantages-of-naive-bayes"><span class="header-section-number">13.4.7</span> Disadvantages of Naive Bayes</h3>
<ul>
<li>Independence assumption may not hold; zero probability for unseen features; struggles with correlated features.</li>
</ul>
</section>
<section id="handling-class-imbalance" class="level3" data-number="13.4.8">
<h3 data-number="13.4.8" class="anchored" data-anchor-id="handling-class-imbalance"><span class="header-section-number">13.4.8</span> Handling Class Imbalance</h3>
<ul>
<li><strong>Resampling</strong>: Oversample minority or undersample majority class.</li>
<li><strong>Class Weights</strong>: Penalize misclassification of minority class.</li>
<li><strong>Ensemble Methods</strong>: Improve robustness (e.g., Random Forest, Gradient Boosting).</li>
</ul>
<hr>
</section>
</section>
<section id="chapter-5-logistic-regression" class="level2" data-number="13.5">
<h2 data-number="13.5" class="anchored" data-anchor-id="chapter-5-logistic-regression"><span class="header-section-number">13.5</span> Chapter 5: Logistic Regression</h2>
<section id="logistic-regression" class="level3" data-number="13.5.1">
<h3 data-number="13.5.1" class="anchored" data-anchor-id="logistic-regression"><span class="header-section-number">13.5.1</span> Logistic Regression</h3>
<ul>
<li>Linear model for binary classification using the logistic (sigmoid) function.</li>
<li>Outputs probabilities between 0 and 1.</li>
</ul>
</section>
<section id="log-odds-logit" class="level3" data-number="13.5.2">
<h3 data-number="13.5.2" class="anchored" data-anchor-id="log-odds-logit"><span class="header-section-number">13.5.2</span> Log-Odds (Logit)</h3>
<ul>
<li>The log-odds is the logarithm of the odds ratio, modeled as a linear combination of input features.</li>
</ul>
</section>
<section id="training-logistic-regression" class="level3" data-number="13.5.3">
<h3 data-number="13.5.3" class="anchored" data-anchor-id="training-logistic-regression"><span class="header-section-number">13.5.3</span> Training Logistic Regression</h3>
<ul>
<li>Coefficients are estimated using Maximum Likelihood Estimation (MLE).</li>
<li>Regularization (L1, L2, Elastic Net) helps prevent overfitting.</li>
</ul>
</section>
<section id="bag-of-words-vs.-tf-idf" class="level3" data-number="13.5.4">
<h3 data-number="13.5.4" class="anchored" data-anchor-id="bag-of-words-vs.-tf-idf"><span class="header-section-number">13.5.4</span> Bag of Words vs.&nbsp;TF-IDF</h3>
<ul>
<li><strong>BoW</strong>: Represents text as word count vectors, ignoring word order and context.</li>
<li><strong>TF-IDF</strong>: Weighs words by their frequency in a document and rarity across the corpus.</li>
</ul>
</section>
<section id="lr-model-assumptions" class="level3" data-number="13.5.5">
<h3 data-number="13.5.5" class="anchored" data-anchor-id="lr-model-assumptions"><span class="header-section-number">13.5.5</span> LR Model Assumptions</h3>
<ul>
<li>Assumes linearity in log-odds, independence of observations, and no multicollinearity.</li>
</ul>
</section>
<section id="lr-limitations" class="level3" data-number="13.5.6">
<h3 data-number="13.5.6" class="anchored" data-anchor-id="lr-limitations"><span class="header-section-number">13.5.6</span> LR Limitations</h3>
<ul>
<li>Sensitive to multicollinearity and outliers; cannot capture complex, non-linear relationships.</li>
</ul>
<hr>
</section>
</section>
<section id="chapter-6-sentiment-analysis" class="level2" data-number="13.6">
<h2 data-number="13.6" class="anchored" data-anchor-id="chapter-6-sentiment-analysis"><span class="header-section-number">13.6</span> Chapter 6: Sentiment Analysis</h2>
<section id="sentiment-analysis-sa" class="level3" data-number="13.6.1">
<h3 data-number="13.6.1" class="anchored" data-anchor-id="sentiment-analysis-sa"><span class="header-section-number">13.6.1</span> Sentiment Analysis (SA)</h3>
<ul>
<li>Determines the sentiment (positive, negative, neutral) expressed in text.</li>
<li>Uses sentiment lexicons or machine learning models.</li>
</ul>
</section>
<section id="resources-for-sentiment-lexicons" class="level3" data-number="13.6.2">
<h3 data-number="13.6.2" class="anchored" data-anchor-id="resources-for-sentiment-lexicons"><span class="header-section-number">13.6.2</span> Resources for Sentiment Lexicons</h3>
<ul>
<li><strong>VADER</strong>: Social media, handles negations/intensifiers.</li>
<li><strong>AFINN</strong>: 3300+ English words, scores -5 to +5.</li>
<li><strong>SentiWordNet</strong>: Assigns positive, negative, objective scores to WordNet synsets.</li>
<li><strong>LIWC</strong>: Psychological/linguistic categories.</li>
</ul>
</section>
<section id="sentiment-analysis-approaches" class="level3" data-number="13.6.3">
<h3 data-number="13.6.3" class="anchored" data-anchor-id="sentiment-analysis-approaches"><span class="header-section-number">13.6.3</span> Sentiment Analysis Approaches</h3>
<ul>
<li><strong>Rule-based</strong>: Uses predefined rules and lexicons.</li>
<li><strong>Machine learning-based</strong>: Trains models on labeled data.</li>
<li><strong>Hybrid</strong>: Combines both for improved performance.</li>
</ul>
</section>
<section id="model-comparison" class="level3" data-number="13.6.4">
<h3 data-number="13.6.4" class="anchored" data-anchor-id="model-comparison"><span class="header-section-number">13.6.4</span> Model Comparison</h3>
<ul>
<li>Lexicon-based models are simple but struggle with negation/sarcasm.</li>
<li>Deep learning models (LSTM, BERT) handle complex sentiment and context.</li>
</ul>
</section>
<section id="python-libraries-and-tools-for-sa" class="level3" data-number="13.6.5">
<h3 data-number="13.6.5" class="anchored" data-anchor-id="python-libraries-and-tools-for-sa"><span class="header-section-number">13.6.5</span> Python Libraries and Tools for SA</h3>
<ul>
<li><strong>NLTK</strong>, <strong>TextBlob</strong>, <strong>TensorFlow</strong>, <strong>PyTorch</strong>, <strong>Hugging Face Transformers</strong>.</li>
</ul>
</section>
<section id="sa-challenges" class="level3" data-number="13.6.6">
<h3 data-number="13.6.6" class="anchored" data-anchor-id="sa-challenges"><span class="header-section-number">13.6.6</span> SA Challenges</h3>
<ul>
<li>Context, sarcasm, ambiguity, and domain-specific language make accurate classification difficult.</li>
</ul>
<hr>
</section>
</section>
<section id="chapter-7-topic-modeling" class="level2" data-number="13.7">
<h2 data-number="13.7" class="anchored" data-anchor-id="chapter-7-topic-modeling"><span class="header-section-number">13.7</span> Chapter 7: Topic Modeling</h2>
<section id="topic-modeling" class="level3" data-number="13.7.1">
<h3 data-number="13.7.1" class="anchored" data-anchor-id="topic-modeling"><span class="header-section-number">13.7.1</span> Topic Modeling</h3>
<ul>
<li>Unsupervised technique for discovering abstract topics in a collection of documents.</li>
</ul>
</section>
<section id="latent-dirichlet-allocation-lda" class="level3" data-number="13.7.2">
<h3 data-number="13.7.2" class="anchored" data-anchor-id="latent-dirichlet-allocation-lda"><span class="header-section-number">13.7.2</span> Latent Dirichlet Allocation (LDA)</h3>
<ul>
<li>Each document is a mixture of topics, each topic is a distribution over words.</li>
<li>Uses Bayes’ Theorem for parameter estimation; methods include EM, Variational Inference, Gibbs sampling.</li>
</ul>
</section>
<section id="lda-parameters-and-trade-offs" class="level3" data-number="13.7.3">
<h3 data-number="13.7.3" class="anchored" data-anchor-id="lda-parameters-and-trade-offs"><span class="header-section-number">13.7.3</span> LDA Parameters and Trade-offs</h3>
<ul>
<li>Key parameters: document density (α), topic word density (β), number of topics (K).</li>
</ul>
</section>
<section id="practical-considerations-for-lda" class="level3" data-number="13.7.4">
<h3 data-number="13.7.4" class="anchored" data-anchor-id="practical-considerations-for-lda"><span class="header-section-number">13.7.4</span> Practical Considerations for LDA</h3>
<ul>
<li>Requires specifying the number of topics; works best with longer, preprocessed documents.</li>
</ul>
</section>
<section id="top2vec" class="level3" data-number="13.7.5">
<h3 data-number="13.7.5" class="anchored" data-anchor-id="top2vec"><span class="header-section-number">13.7.5</span> Top2Vec</h3>
<ul>
<li>Embeds documents/words in vector space, clusters, finds topics automatically.</li>
</ul>
</section>
<section id="bertopic" class="level3" data-number="13.7.6">
<h3 data-number="13.7.6" class="anchored" data-anchor-id="bertopic"><span class="header-section-number">13.7.6</span> BERTopic</h3>
<ul>
<li>Uses transformer-based embeddings for better topic quality, especially in nuanced/short texts.</li>
</ul>
</section>
<section id="k-means-clustering-with-word2vec" class="level3" data-number="13.7.7">
<h3 data-number="13.7.7" class="anchored" data-anchor-id="k-means-clustering-with-word2vec"><span class="header-section-number">13.7.7</span> K-means Clustering with Word2Vec</h3>
<ul>
<li>Clusters Word2Vec embeddings, suitable for short texts.</li>
</ul>
<hr>
</section>
</section>
<section id="chapter-8-word-meanings-and-vector-semantics" class="level2" data-number="13.8">
<h2 data-number="13.8" class="anchored" data-anchor-id="chapter-8-word-meanings-and-vector-semantics"><span class="header-section-number">13.8</span> Chapter 8: Word Meanings and Vector Semantics</h2>
<section id="word-meanings" class="level3" data-number="13.8.1">
<h3 data-number="13.8.1" class="anchored" data-anchor-id="word-meanings"><span class="header-section-number">13.8.1</span> Word Meanings</h3>
<ul>
<li>Meanings are complex and context-dependent; words can have multiple unrelated meanings (homonymy) or related senses (polysemy).</li>
</ul>
</section>
<section id="word-relations" class="level3" data-number="13.8.2">
<h3 data-number="13.8.2" class="anchored" data-anchor-id="word-relations"><span class="header-section-number">13.8.2</span> Word Relations</h3>
<ul>
<li><strong>Synonymy</strong>: Similar meanings.</li>
<li><strong>Antonymy</strong>: Opposite meanings.</li>
<li><strong>Similarity/Relatedness</strong>: Words can be similar or just related.</li>
<li><strong>Connotation</strong>: Emotional/cultural associations.</li>
</ul>
</section>
<section id="vector-semantics" class="level3" data-number="13.8.3">
<h3 data-number="13.8.3" class="anchored" data-anchor-id="vector-semantics"><span class="header-section-number">13.8.3</span> Vector Semantics</h3>
<ul>
<li>Represents words/documents as vectors in multidimensional space.</li>
<li>Distributional hypothesis: words in similar contexts have similar meanings.</li>
</ul>
<section id="comparing-word-vectors" class="level4" data-number="13.8.3.1">
<h4 data-number="13.8.3.1" class="anchored" data-anchor-id="comparing-word-vectors"><span class="header-section-number">13.8.3.1</span> Comparing Word Vectors</h4>
<ul>
<li><strong>Cosine Similarity</strong>, <strong>Euclidean Distance</strong>, <strong>Dot Product</strong>: Metrics for quantifying semantic similarity.</li>
</ul>
</section>
</section>
<section id="word2vec" class="level3" data-number="13.8.4">
<h3 data-number="13.8.4" class="anchored" data-anchor-id="word2vec"><span class="header-section-number">13.8.4</span> Word2Vec</h3>
<ul>
<li>Learns dense word embeddings by distinguishing real context pairs from random pairs.</li>
<li>Embeddings capture abstract relationships and analogies.</li>
</ul>
<hr>
</section>
</section>
<section id="chapter-9-neural-networks-in-nlp" class="level2" data-number="13.9">
<h2 data-number="13.9" class="anchored" data-anchor-id="chapter-9-neural-networks-in-nlp"><span class="header-section-number">13.9</span> Chapter 9: Neural Networks in NLP</h2>
<section id="ambiguity-in-language" class="level3" data-number="13.9.1">
<h3 data-number="13.9.1" class="anchored" data-anchor-id="ambiguity-in-language"><span class="header-section-number">13.9.1</span> Ambiguity in Language</h3>
<ul>
<li>Ambiguity is a core challenge, requiring structural models (parse trees, dependency graphs) to clarify meaning.</li>
</ul>
</section>
<section id="simple-neural-networks" class="level3" data-number="13.9.2">
<h3 data-number="13.9.2" class="anchored" data-anchor-id="simple-neural-networks"><span class="header-section-number">13.9.2</span> Simple Neural Networks</h3>
<ul>
<li>Layers of units (neurons) compute weighted sums, add bias, and apply activation functions.</li>
</ul>
</section>
<section id="activation-functions" class="level3" data-number="13.9.3">
<h3 data-number="13.9.3" class="anchored" data-anchor-id="activation-functions"><span class="header-section-number">13.9.3</span> Activation Functions</h3>
<ul>
<li><strong>Sigmoid</strong>, <strong>Tanh</strong>, <strong>ReLU</strong>: Introduce non-linearity, enabling modeling of complex relationships.</li>
</ul>
</section>
<section id="multi-layer-networks-and-xor" class="level3" data-number="13.9.4">
<h3 data-number="13.9.4" class="anchored" data-anchor-id="multi-layer-networks-and-xor"><span class="header-section-number">13.9.4</span> Multi-Layer Networks and XOR</h3>
<ul>
<li>Multi-layer networks with non-linear activations can solve complex functions like XOR.</li>
</ul>
</section>
<section id="feedforward-neural-networks" class="level3" data-number="13.9.5">
<h3 data-number="13.9.5" class="anchored" data-anchor-id="feedforward-neural-networks"><span class="header-section-number">13.9.5</span> Feedforward Neural Networks</h3>
<ul>
<li>Information flows from input to output without cycles; hidden layers enable hierarchical representations.</li>
</ul>
</section>
<section id="applying-to-nlp" class="level3" data-number="13.9.6">
<h3 data-number="13.9.6" class="anchored" data-anchor-id="applying-to-nlp"><span class="header-section-number">13.9.6</span> Applying to NLP</h3>
<ul>
<li>Used for text classification, language modeling, and more; handle variable-length input via padding, truncation, or pooling.</li>
</ul>
</section>
<section id="training-neural-networks" class="level3" data-number="13.9.7">
<h3 data-number="13.9.7" class="anchored" data-anchor-id="training-neural-networks"><span class="header-section-number">13.9.7</span> Training Neural Networks</h3>
<ul>
<li>Uses backpropagation and gradient descent to minimize prediction error.</li>
</ul>
<hr>
</section>
</section>
<section id="chapter-10-transformers-and-large-language-models" class="level2" data-number="13.10">
<h2 data-number="13.10" class="anchored" data-anchor-id="chapter-10-transformers-and-large-language-models"><span class="header-section-number">13.10</span> Chapter 10: Transformers and Large Language Models</h2>
<section id="key-innovations" class="level3" data-number="13.10.1">
<h3 data-number="13.10.1" class="anchored" data-anchor-id="key-innovations"><span class="header-section-number">13.10.1</span> Key Innovations</h3>
<ul>
<li><strong>Bag of Words/N-grams</strong>: Early models missed deeper context.</li>
<li><strong>Word Embeddings</strong>: Capture semantic relationships.</li>
<li><strong>Transformers</strong>: Consider entire sentence context via attention.</li>
</ul>
</section>
<section id="transformers" class="level3" data-number="13.10.2">
<h3 data-number="13.10.2" class="anchored" data-anchor-id="transformers"><span class="header-section-number">13.10.2</span> Transformers</h3>
<ul>
<li>Use attention to create context-aware word representations.</li>
<li><strong>Position embeddings</strong> encode word order.</li>
</ul>
</section>
<section id="bert-and-decoder-only-models" class="level3" data-number="13.10.3">
<h3 data-number="13.10.3" class="anchored" data-anchor-id="bert-and-decoder-only-models"><span class="header-section-number">13.10.3</span> BERT and Decoder-Only Models</h3>
<ul>
<li><strong>BERT</strong>: Encoder-only, best for classification and embeddings.</li>
<li><strong>GPT</strong>: Decoder-only, best for text generation and chat.</li>
</ul>
</section>
<section id="large-language-models-llms" class="level3" data-number="13.10.4">
<h3 data-number="13.10.4" class="anchored" data-anchor-id="large-language-models-llms"><span class="header-section-number">13.10.4</span> Large Language Models (LLMs)</h3>
<ul>
<li>Trained to predict the next word, learning language, facts, and reasoning.</li>
<li>Decoding methods (random, top-k, top-p, temperature) balance quality and diversity.</li>
</ul>
</section>
<section id="pretraining-and-self-supervised-learning" class="level3" data-number="13.10.5">
<h3 data-number="13.10.5" class="anchored" data-anchor-id="pretraining-and-self-supervised-learning"><span class="header-section-number">13.10.5</span> Pretraining and Self-Supervised Learning</h3>
<ul>
<li>LLMs are pretrained on large text corpora using self-supervised learning and cross-entropy loss.</li>
</ul>
</section>
<section id="working-with-llms" class="level3" data-number="13.10.6">
<h3 data-number="13.10.6" class="anchored" data-anchor-id="working-with-llms"><span class="header-section-number">13.10.6</span> Working with LLMs</h3>
<ul>
<li>Prompt engineering (zero-shot, few-shot, chain-of-thought) is key for effective use.</li>
<li>Model types: base, instruct, chat.</li>
<li>Key settings: temperature, top-p, max length, penalties.</li>
</ul>
<hr>
</section>
</section>
<section id="chapter-11-question-answering-and-information-retrieval" class="level2" data-number="13.11">
<h2 data-number="13.11" class="anchored" data-anchor-id="chapter-11-question-answering-and-information-retrieval"><span class="header-section-number">13.11</span> Chapter 11: Question Answering and Information Retrieval</h2>
<section id="information-retrieval-ir" class="level3" data-number="13.11.1">
<h3 data-number="13.11.1" class="anchored" data-anchor-id="information-retrieval-ir"><span class="header-section-number">13.11.1</span> Information Retrieval (IR)</h3>
<ul>
<li>IR finds relevant documents from a large collection based on a user’s query.</li>
<li>Documents and queries are represented as vectors (e.g., BoW, TF-IDF), and similarity (often cosine similarity) is used to rank results.</li>
<li>Key metrics: precision (fraction of retrieved documents that are relevant) and recall (fraction of relevant documents that are retrieved).</li>
</ul>
</section>
<section id="vector-space-model-tf-idf" class="level3" data-number="13.11.2">
<h3 data-number="13.11.2" class="anchored" data-anchor-id="vector-space-model-tf-idf"><span class="header-section-number">13.11.2</span> Vector Space Model &amp; TF-IDF</h3>
<ul>
<li>Represents documents and queries as vectors in a high-dimensional space.</li>
<li>TF-IDF weighs terms by their importance: frequent in a document but rare in the collection.</li>
</ul>
</section>
<section id="dense-retrieval-with-neural-models" class="level3" data-number="13.11.3">
<h3 data-number="13.11.3" class="anchored" data-anchor-id="dense-retrieval-with-neural-models"><span class="header-section-number">13.11.3</span> Dense Retrieval with Neural Models</h3>
<ul>
<li>Uses neural networks (e.g., BERT) to encode queries and documents into dense embeddings.</li>
<li>Allows semantic matching beyond exact word overlap, addressing vocabulary mismatch.</li>
</ul>
</section>
<section id="question-answering-datasets" class="level3" data-number="13.11.4">
<h3 data-number="13.11.4" class="anchored" data-anchor-id="question-answering-datasets"><span class="header-section-number">13.11.4</span> Question Answering Datasets</h3>
<ul>
<li>Datasets like SQuAD provide passages and questions, with answers as text spans within the passage.</li>
<li>SQuAD 2.0 introduces unanswerable questions to test model robustness.</li>
</ul>
</section>
<section id="extractive-question-answering" class="level3" data-number="13.11.5">
<h3 data-number="13.11.5" class="anchored" data-anchor-id="extractive-question-answering"><span class="header-section-number">13.11.5</span> Extractive Question Answering</h3>
<ul>
<li>Involves selecting a span of text from a passage as the answer to a question.</li>
<li>Models (e.g., BERT) predict the start and end positions of the answer span.</li>
</ul>
</section>
<section id="entity-linking-wikification" class="level3" data-number="13.11.6">
<h3 data-number="13.11.6" class="anchored" data-anchor-id="entity-linking-wikification"><span class="header-section-number">13.11.6</span> Entity Linking (Wikification)</h3>
<ul>
<li>Maps mentions in text to specific entities in a knowledge base (e.g., Wikipedia pages).</li>
<li>Involves mention detection, candidate generation, and disambiguation based on context.</li>
</ul>
</section>
<section id="knowledge-based-question-answering-kbqa" class="level3" data-number="13.11.7">
<h3 data-number="13.11.7" class="anchored" data-anchor-id="knowledge-based-question-answering-kbqa"><span class="header-section-number">13.11.7</span> Knowledge-based Question Answering (KBQA)</h3>
<ul>
<li>Answers questions by mapping them to queries over structured knowledge bases (e.g., DBpedia, Wikidata).</li>
<li>Approaches: graph-based QA (traversing entity-relation graphs), semantic parsing (mapping questions to logical forms).</li>
<li>RDF triples (subject, predicate, object) are a common data structure.</li>
</ul>
</section>
<section id="supervision-for-semantic-parsing" class="level3" data-number="13.11.8">
<h3 data-number="13.11.8" class="anchored" data-anchor-id="supervision-for-semantic-parsing"><span class="header-section-number">13.11.8</span> Supervision for Semantic Parsing</h3>
<ul>
<li>Fully supervised: annotated logical forms for each question.</li>
<li>Weakly supervised: only the answer (denotation), logical form is latent.</li>
</ul>
</section>
<section id="retrieve-and-generate-rag" class="level3" data-number="13.11.9">
<h3 data-number="13.11.9" class="anchored" data-anchor-id="retrieve-and-generate-rag"><span class="header-section-number">13.11.9</span> Retrieve-and-Generate (RAG)</h3>
<ul>
<li>Combines retrieval of relevant documents or passages with generative LLMs to produce answers.</li>
<li>Retrieves relevant chunks, combines them with the question, and generates an answer using an LLM.</li>
</ul>
</section>
<section id="evaluation-of-rag-systems" class="level3" data-number="13.11.10">
<h3 data-number="13.11.10" class="anchored" data-anchor-id="evaluation-of-rag-systems"><span class="header-section-number">13.11.10</span> Evaluation of RAG Systems</h3>
<ul>
<li>Retrieval evaluation: checks if relevant chunks are retrieved (precision, recall).</li>
<li>Generation evaluation: assesses if the answer is correct, relevant, and faithful to the retrieved context.</li>
<li>Faithfulness and answer/context relevance are key; hallucinations should be avoided.</li>
</ul>
</section>
<section id="agents" class="level3" data-number="13.11.11">
<h3 data-number="13.11.11" class="anchored" data-anchor-id="agents"><span class="header-section-number">13.11.11</span> Agents</h3>
<ul>
<li>Agents are systems capable of forming intentions, making plans, and acting to achieve goals, often using internal representations.</li>
<li>LLM-based agents control application flow (e.g., LangChain agents) and can process text, audio, or visual input via modality fusion.</li>
</ul>
</section>
<section id="formal-vs.-functional-abilities-of-llms" class="level3" data-number="13.11.12">
<h3 data-number="13.11.12" class="anchored" data-anchor-id="formal-vs.-functional-abilities-of-llms"><span class="header-section-number">13.11.12</span> Formal vs.&nbsp;Functional Abilities of LLMs</h3>
<ul>
<li>Formal linguistic competence: knowledge of grammar and language rules.</li>
<li>Functional competence: ability to use language in real-world contexts.</li>
<li>LLMs excel at formal competence but may struggle with functional competence, such as real-world reasoning.</li>
</ul>
</section>
<section id="takeaways-1" class="level3" data-number="13.11.13">
<h3 data-number="13.11.13" class="anchored" data-anchor-id="takeaways-1"><span class="header-section-number">13.11.13</span> Takeaways</h3>
<ul>
<li>Question answering can be approached via IR, KBQA, or LLM-based methods (including RAG and agents).</li>
<li>Key datasets: SQuAD, SimpleQuestions, DBpedia.</li>
<li>LLMs are powerful for formal linguistic tasks, but functional, real-world understanding remains a challenge.</li>
</ul>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./session11.html" class="pagination-link" aria-label="Question Answering and Information Retrieval">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Question Answering and Information Retrieval</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./summary.html" class="pagination-link" aria-label="Concepts and Explanations Long">
        <span class="nav-page-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Concepts and Explanations Long</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>