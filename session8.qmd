---
title: "Lexical Semantics and Vector Embeddings"
author: "Tim Roessling"
date: "2025-06-12"
format: html
jupyter: myvenv
---

## Word Meanings

Understanding word meanings is fundamental in NLP. Here are some key points:

- **Words have meanings**: But meanings can be complex and context-dependent.
- **Some words are similar to others**: For example, "car" and "automobile".

### Challenges in Defining Word Meanings

- **Homonymy**: Words with multiple unrelated meanings (e.g., "bank" as a financial institution vs. "bank" of a river).
- **Polysemy**: Words with multiple related senses (e.g., "paper" as material vs. "paper" as an academic article).
- **Many-to-many mapping**: Concepts can be associated with multiple words, and words can represent multiple concepts.

### Word Relations

- **Synonymy**: Words with similar meanings (e.g., "big" and "large").
- **Antonymy**: Words with opposite meanings (e.g., "hot" and "cold").
- **Similarity**: Degree to which words are alike (e.g., "cup" and "mug").
- **Relatedness**: Words that are related but not necessarily similar (e.g., "doctor" and "hospital").
- **Connotation**: Emotional or cultural association (e.g., "childish" vs. "youthful").

---

## Vector Semantics

Vector semantics represents words and documents as points in a multidimensional space.

### Idea 1: Defining Meaning by Linguistic Distribution

> "You shall know a word by the company it keeps."  
> — J.R. Firth

Words that appear in similar contexts tend to have similar meanings.

### Idea 2: Meaning as a Point in Multidimensional Space

Each word or document is represented as a vector of numbers.

#### Example: Term-Document Matrix

|       | Doc1 | Doc2 | Doc3 |
|-------|------|------|------|
| apple |  2   |  0   |  1   |
| orange|  1   |  1   |  0   |
| bank  |  0   |  2   |  1   |

Each row is a word, each column is a document, and the numbers are word counts.

![Visualizing Document Vectors](images/chapter8_1.png)

#### Comparing Document Vectors

Suppose we have two document vectors:  
- Doc1: `[2, 1, 0]`  
- Doc2: `[0, 1, 2]`

We can measure their similarity using various metrics.

#### Example: Word Context Matrix

|       | context1 | context2 | context3 |
|-------|----------|----------|----------|
| bank  |    3     |    0     |    2     |
| river |    0     |    2     |    3     |
| money |    2     |    3     |    0     |

---

### Comparing Word Vectors

- **Cosine Similarity**: Measures the angle between two vectors (e.g., "king" and "queen" have high cosine similarity).

```{python}
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

vec_king = np.array([0.5, 0.8, 0.1])
vec_queen = np.array([0.51, 0.79, 0.12])

cos_sim = cosine_similarity([vec_king], [vec_queen])
print("Cosine Similarity:", cos_sim[0][0])
```

- **Euclidean Distance**: Measures the straight-line distance between vectors.

```{python}
from scipy.spatial.distance import euclidean
import numpy as np

vec_king = np.array([0.5, 0.8, 0.1])
vec_queen = np.array([0.51, 0.79, 0.12])

dist = euclidean(vec_king, vec_queen)
print("Euclidean Distance:", dist)
```    

- **Dot Product**: Measures similarity based on direction and magnitude.

```{python}
import numpy as np

vec_king = np.array([0.5, 0.8, 0.1])
vec_queen = np.array([0.51, 0.79, 0.12])

dot = np.dot(vec_king, vec_queen)
print("Dot Product:", dot)
```

These metrics help quantify how similar or related two words or documents are in vector space.


## Word2Vec

Word2Vec is a widely used algorithm for learning **word embeddings**—dense vector representations that capture word meanings and relationships.

### Dense vs. Sparse Vectors

- **Sparse Vectors**: High-dimensional, mostly zeros. Used in one-hot or bag-of-words models.
    - Example: One-hot for "apple" in a 10,000-word vocab: `[0, 0, ..., 1, ..., 0]`.
    - Example: Bag-of-words for a document: `[2, 0, 0, 1, 0, ..., 0]`.
- **Dense Vectors**: Low-dimensional, mostly nonzero. Used in embeddings like Word2Vec.
    - Example: Word2Vec for "apple": `[0.12, -0.08, 0.44, ..., 0.03]` (50–300 dimensions).

Dense vectors group similar words closer together in space, capturing semantic similarity.

---

### How Word2Vec Learns Embeddings

Word2Vec uses the **skip-gram** model:

1. **Skip-Gram Objective**: For each word (the **target**) in a sentence, predict its nearby words (the **context**).
    - Example sentence:  
      `"The bank of the river was flooded."`
    - If the target is `"bank"`, context words (window size 2) are: `"the"`, `"of"`, `"river"`, `"was"`.

2. **Training Data**: Create (target, context) pairs from text.
    - Example pairs:
        | Target | Context |
        |--------|---------|
        | bank   | the     |
        | bank   | of      |
        | bank   | river   |
        | bank   | was     |

3. **Positive & Negative Examples**:
    - **Positive**: Real (target, context) pairs from text.
    - **Negative**: Randomly pair the target with unrelated words (not in the context window).

4. **Learning Process**:
    - Use a simple neural network (logistic regression) to distinguish positive from negative pairs.
    - Adjust word vectors so that real pairs are similar, random pairs are dissimilar.

5. **Result**: The learned vectors (embeddings) represent each word.

> **Self-supervised**: The model learns from raw text, no human labels needed.

---

### Why Dense Vectors Matter

- Dense vectors capture subtle relationships:
    - `"king"` and `"queen"` have similar vectors.
    - Vector arithmetic:  
      `vec("king") - vec("man") + vec("woman") ≈ vec("queen")`
- These embeddings work better for most NLP tasks than sparse vectors.

---

### Summary Table: Word2Vec Relationships

| Concept                | Description                                                      |
|------------------------|------------------------------------------------------------------|
| **Embedding**          | Each word gets a dense vector                                    |
| **Skip-Gram**          | Predict context words from a target word                         |
| **Positive Example**   | (target, context) from real text                                 |
| **Negative Example**   | (target, random word) not in context                             |
| **Training**           | Make real pairs similar, random pairs dissimilar                 |
| **Result**             | Vectors encode word meaning and relationships                    |

---

#### Analogy Reasoning with Embeddings

Word2Vec embeddings capture relationships between words. For example, vector arithmetic can reveal analogies:


## Takeaways
### Key Takeaways

- **Vector Semantics**:  
    Summarizes word contexts as dense vectors, enabling mathematical operations on word meanings.

- **Word2Vec**:  
    - Provides powerful word representations for many NLP tasks (e.g., translation, sentiment analysis, question answering).
    - Uses the skip-gram word prediction method to learn embeddings.
    - Embeddings capture abstract relationships (e.g., male–female, capital–city, comparative–superlative).
    - Note: Word2Vec produces *static* embeddings—each word has a single vector, regardless of context.

#### Practical Steps

- Load pre-trained Word2Vec embeddings.
- Explore vector similarities to find related words.
- Solve analogies (e.g., "man" is to "king" as "woman" is to ?).
- Compute sentence embeddings (e.g., by averaging word vectors).
- Use embeddings for sentence classification and compare with Bag-of-Words (BoW) models.


*Add your notes and code examples below as you progress through the chapter.*