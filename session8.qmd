---
title: "Lexical Semantics and Vector Embeddings"
author: "Tim Roessling"
date: "2025-06-12"
format: html
jupyter: myvenv
---

## Word Meanings

Understanding word meanings is fundamental in NLP. Here are some key points:

- **Words have meanings**: But meanings can be complex and context-dependent.
- **Some words are similar to others**: For example, "car" and "automobile".

### Challenges in Defining Word Meanings

- **Homonymy**: Words with multiple unrelated meanings (e.g., "bank" as a financial institution vs. "bank" of a river).
- **Polysemy**: Words with multiple related senses (e.g., "paper" as material vs. "paper" as an academic article).
- **Many-to-many mapping**: Concepts can be associated with multiple words, and words can represent multiple concepts.

### Word Relations

- **Synonymy**: Words with similar meanings (e.g., "big" and "large").
- **Antonymy**: Words with opposite meanings (e.g., "hot" and "cold").
- **Similarity**: Degree to which words are alike (e.g., "cup" and "mug").
- **Relatedness**: Words that are related but not necessarily similar (e.g., "doctor" and "hospital").
- **Connotation**: Emotional or cultural association (e.g., "childish" vs. "youthful").

---

## Vector Semantics

Vector semantics represents words and documents as points in a multidimensional space.

### Idea 1: Defining Meaning by Linguistic Distribution

> "You shall know a word by the company it keeps."  
> — J.R. Firth

Words that appear in similar contexts tend to have similar meanings.

### Idea 2: Meaning as a Point in Multidimensional Space

Each word or document is represented as a vector of numbers.

#### Example: Term-Document Matrix

|       | Doc1 | Doc2 | Doc3 |
|-------|------|------|------|
| apple |  2   |  0   |  1   |
| orange|  1   |  1   |  0   |
| bank  |  0   |  2   |  1   |

Each row is a word, each column is a document, and the numbers are word counts.

![Visualizing Document Vectors](images\chapter8_1.png)

#### Comparing Document Vectors

Suppose we have two document vectors:  
- Doc1: `[2, 1, 0]`  
- Doc2: `[0, 1, 2]`

We can measure their similarity using various metrics.

#### Example: Word Context Matrix

|       | context1 | context2 | context3 |
|-------|----------|----------|----------|
| bank  |    3     |    0     |    2     |
| river |    0     |    2     |    3     |
| money |    2     |    3     |    0     |

---

### Comparing Word Vectors

- **Cosine Similarity**: Measures the angle between two vectors (e.g., "king" and "queen" have high cosine similarity).

```{python}
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

vec_king = np.array([0.5, 0.8, 0.1])
vec_queen = np.array([0.51, 0.79, 0.12])

cos_sim = cosine_similarity([vec_king], [vec_queen])
print("Cosine Similarity:", cos_sim[0][0])
```

- **Euclidean Distance**: Measures the straight-line distance between vectors.

```{python}
from scipy.spatial.distance import euclidean
import numpy as np

vec_king = np.array([0.5, 0.8, 0.1])
vec_queen = np.array([0.51, 0.79, 0.12])

dist = euclidean(vec_king, vec_queen)
print("Euclidean Distance:", dist)
```    

- **Dot Product**: Measures similarity based on direction and magnitude.

```{python}
import numpy as np

vec_king = np.array([0.5, 0.8, 0.1])
vec_queen = np.array([0.51, 0.79, 0.12])

dot = np.dot(vec_king, vec_queen)
print("Dot Product:", dot)
```

These metrics help quantify how similar or related two words or documents are in vector space.


## Word2Vec

Word2Vec is a popular algorithm for learning **word embeddings**—compact, dense vectors that capture the meanings and relationships of words.

### From Sparse to Dense Vectors

- **Sparse Vectors**: Traditional representations like one-hot or bag-of-words use high-dimensional vectors with mostly zeros.  
    - Example: One-hot for "apple" in a 10,000-word vocabulary: `[0, 0, ..., 1, ..., 0]`.
    - Example: Bag-of-words for a document: `[2, 0, 0, 1, 0, ..., 0]`.
- **Dense Vectors**: Word2Vec learns low-dimensional vectors (e.g., 50–300 dimensions) where most values are nonzero.  
    - Example: Word2Vec for "apple": `[0.12, -0.08, 0.44, ..., 0.03]`.

Dense vectors place similar words close together in space, making it easier to capture semantic similarity.

---

### How Word2Vec Works

Word2Vec can use the **skip-gram** approach to learn word relationships from text:

1. **Skip-Gram Objective**: For each word (the **target**) in a sentence, predict its nearby words (the **context**).
        - Example sentence:  
            `"The bank of the river was flooded."`
        - If the target is `"bank"`, and the window size is 2, the context words are: `"the"`, `"of"`, `"river"`, `"was"`.

2. **Creating Training Pairs**: The model creates (target, context) pairs from the text.
        - Example pairs:
                | Target | Context |
                |--------|---------|
                | bank   | the     |
                | bank   | of      |
                | bank   | river   |
                | bank   | was     |

3. **Positive and Negative Examples**:
        - **Positive pairs**: Real (target, context) pairs from the text.
        - **Negative pairs**: Randomly combine the target with words that do not appear in its context window.

4. **Learning Process**:
        - The model uses a simple neural network/logistic regression to distinguish positive pairs from negative ones.
        - The goal is to maximize the probability of positive pairs while minimizing the probability of negative pairs.
        - The model adjusts the word vectors so that words appearing in similar contexts have similar vectors.

5. **Result**: Each word is represented by a vector that reflects its meaning and relationships to other words.

> **Self-supervised**: Word2Vec learns directly from raw text, without any manual labeling.

---

### Why Dense Embeddings Are Powerful

Dense vectors capture subtle relationships between words:
- Words like `"king"` and `"queen"` have similar vectors.
- You can do vector arithmetic to solve analogies:  
    `vec("king") - vec("man") + vec("woman") ≈ vec("queen")`

These embeddings are much more effective for NLP tasks than sparse representations.

---

### Word2Vec at a Glance

| Concept                | Description                                                      |
|------------------------|------------------------------------------------------------------|
| **Embedding**          | Each word gets a dense vector                                    |
| **Skip-Gram**          | Predict context words from a target word                         |
| **Positive Example**   | (target, context) from real text                                 |
| **Negative Example**   | (target, random word) not in context                             |
| **Training**           | Make real pairs similar, random pairs dissimilar                 |
| **Result**             | Vectors encode word meaning and relationships                    |

---

#### Analogy Reasoning with Embeddings

Word2Vec embeddings can solve analogies using vector math. For example:


## Takeaways
### Key Takeaways

- **Vector Semantics**:  
    Summarizes word contexts as dense vectors, enabling mathematical operations on word meanings.

- **Word2Vec**:  
    - Provides powerful word representations for many NLP tasks (e.g., translation, sentiment analysis, question answering).
    - Uses the skip-gram word prediction method to learn embeddings.
    - Embeddings capture abstract relationships (e.g., male–female, capital–city, comparative–superlative).
    - Note: Word2Vec produces *static* embeddings—each word has a single vector, regardless of context.

#### Practical Steps

- Load pre-trained Word2Vec embeddings.
- Explore vector similarities to find related words.
- Solve analogies (e.g., "man" is to "king" as "woman" is to ?).
- Compute sentence embeddings (e.g., by averaging word vectors).
- Use embeddings for sentence classification and compare with Bag-of-Words (BoW) models.


*Add your notes and code examples below as you progress through the chapter.*